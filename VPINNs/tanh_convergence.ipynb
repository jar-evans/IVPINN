{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mesh import *\n",
    "\n",
    "from VPINN_tri_final import *\n",
    "\n",
    "from PROBDEF import PROBDEF\n",
    "\n",
    "import os; os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import logging; logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "\n",
    "import gmsh_worker as gw\n",
    "from MeshLib import MeshLib as ml\n",
    "\n",
    "# importing probdef \n",
    "pb = PROBDEF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quad pairs\n",
    "from numpy.polynomial import legendre\n",
    "\n",
    "def get_quad_rule(n: int):\n",
    "    # Degree of the Legendre polynomial (number of nodes - 1)\n",
    "\n",
    "    # Nodes and weights for Gauss-Legendre quadrature\n",
    "    nodes, weights = legendre.leggauss(n + 1)\n",
    "\n",
    "\n",
    "\n",
    "    nodes=(nodes+1.0)/2.0\n",
    "\n",
    "\n",
    "    # Nodes=np.array([nodes],dtype=np_type)\n",
    "\n",
    "    Weights=np.array([weights],dtype=np_type)\n",
    "    w = Weights.T @ Weights\n",
    "    w = np.reshape(w, (-1,1))\n",
    "\n",
    "\n",
    "    x, y =np.meshgrid(nodes,nodes)\n",
    "\n",
    "\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    xy=np.array([x,y]).T\n",
    "\n",
    "\n",
    "    return xy, w/4\n",
    "\n",
    "def L2_error(xy, w):\n",
    "    \n",
    "    u_NN = vp.NN_imposeBC(xy)\n",
    "\n",
    "    # Find the exact solution\n",
    "    u_ex = pb.u_exact_np(xy[:,0], xy[:,1])\n",
    "    u_ex = np.reshape(u_ex, (-1,1))\n",
    "\n",
    "    integrand = (u_ex - u_NN)**2\n",
    "\n",
    "\n",
    "    # Find the difference between exact and NN \n",
    "    # return (u_ex - u_NN)**2\n",
    "\n",
    "    return np.sqrt(np.sum(w*integrand))\n",
    "\n",
    "def semi_H1_error(xy, w):\n",
    "\n",
    "    # Find the gradient of the exact solution\n",
    "    grad_ex = np.array([pb.dudx(xy[:,0], xy[:,1]), pb.dudy(xy[:,0], xy[:,1])],dtype=np_type).T\n",
    "\n",
    "\n",
    "    # Find the gradient of the NN solution\n",
    "    grad_NN = vp.eval_grad_NN_BC(tf.constant(xy, dtype=tf_type))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Find the difference between exact and NN \n",
    "    pw_diff = grad_ex - grad_NN\n",
    "\n",
    "    pw_diff = tf.reduce_sum(tf.square(pw_diff),axis=1)\n",
    "\n",
    "    pw_diff=tf.reshape(pw_diff,(-1,1))\n",
    "\n",
    "\n",
    "    return np.sqrt(np.sum(pw_diff*w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restart_model():\n",
    "    model = tfk.models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(2,),dtype=tf.float64))\n",
    "    model.add(tfkl.Dense(50, activation='tanh',kernel_initializer=initializer,dtype=tf_type,bias_initializer=initializer))\n",
    "    model.add(tfkl.Dense(50, activation='tanh',kernel_initializer=initializer,dtype=tf_type,bias_initializer=initializer))\n",
    "    model.add(tfkl.Dense(50, activation='tanh',kernel_initializer=initializer,dtype=tf_type,bias_initializer=initializer))\n",
    "    model.add(tfkl.Dense(50, activation='tanh',kernel_initializer=initializer,dtype=tf_type,bias_initializer=initializer))\n",
    "    model.add(tfkl.Dense(50, activation='tanh',kernel_initializer=initializer,dtype=tf_type,bias_initializer=initializer))\n",
    "    model.add(tf.keras.layers.Dense(1,activation='linear',kernel_initializer=initializer,dtype=tf_type,use_bias=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-->mesh : \n",
      "     n_triangles :  4\n",
      "     n_vertices  :  5\n",
      "     n_edges     :  8\n",
      "     h_max           :  1.0\n",
      "     h_min           :  0.7071067811865476\n",
      "-->test_fun      : \n",
      "     order       :  1\n",
      "     dof         :  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 11:43:15.815820: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:43:15.821031: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:43:15.824981: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:43:15.828252: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:43:15.830192: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:43:15.833415: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:43:15.837739: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:43:15.840814: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:43:15.842685: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:43:15.844635: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:43:16.291247: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_43' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_43}}]]\n",
      "2023-12-28 11:43:16.291421: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_61' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_61}}]]\n",
      "2023-12-28 11:43:16.291485: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_75' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_75}}]]\n",
      "2023-12-28 11:43:16.291571: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_82' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_82}}]]\n",
      "2023-12-28 11:43:16.291665: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_87' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_87}}]]\n",
      "2023-12-28 11:43:16.291750: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_90' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_90}}]]\n",
      "2023-12-28 11:43:16.291840: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_93' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_93}}]]\n",
      "2023-12-28 11:43:16.291926: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_96' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_96}}]]\n",
      "2023-12-28 11:43:16.291986: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_99' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_99}}]]\n",
      "2023-12-28 11:43:16.292047: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_102' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_102}}]]\n",
      "2023-12-28 11:43:16.292107: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_105' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_105}}]]\n",
      "2023-12-28 11:43:16.292170: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_108' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_108}}]]\n",
      "2023-12-28 11:43:16.292287: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_111' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_111}}]]\n",
      "2023-12-28 11:43:16.292400: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_114' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_114}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 loss: 0.7905848660 time: 2.2271006107330322\n",
      "Iteration: 10 loss: 0.5956421734 time: 0.057094573974609375\n",
      "Iteration: 20 loss: 0.4302140082 time: 0.05663323402404785\n",
      "Iteration: 30 loss: 0.2966267255 time: 0.05653858184814453\n",
      "Iteration: 40 loss: 0.1942367903 time: 0.05179786682128906\n",
      "Iteration: 50 loss: 0.1200239432 time: 0.055130720138549805\n",
      "Iteration: 60 loss: 0.0694275524 time: 0.0527799129486084\n",
      "Iteration: 70 loss: 0.0372254270 time: 0.05182027816772461\n",
      "Iteration: 80 loss: 0.0182795839 time: 0.0525202751159668\n",
      "Iteration: 90 loss: 0.0080978251 time: 0.054486989974975586\n",
      "Iteration: 100 loss: 0.0031715366 time: 0.04995870590209961\n",
      "Iteration: 110 loss: 0.0010653537 time: 0.041959524154663086\n",
      "Iteration: 120 loss: 0.0002910327 time: 0.0395052433013916\n",
      "Iteration: 130 loss: 0.0000575956 time: 0.04500937461853027\n",
      "Iteration: 140 loss: 0.0000057115 time: 0.041318416595458984\n",
      "Iteration: 150 loss: 0.0000000046 time: 0.041175127029418945\n",
      "Iteration: 160 loss: 0.0000008784 time: 0.03386044502258301\n",
      "Iteration: 170 loss: 0.0000010156 time: 0.0379793643951416\n",
      "Iteration: 180 loss: 0.0000005843 time: 0.038811683654785156\n",
      "Iteration: 190 loss: 0.0000002184 time: 0.039511680603027344\n",
      "Iteration: 200 loss: 0.0000000525 time: 0.037343502044677734\n",
      "Iteration: 210 loss: 0.0000000060 time: 0.03730034828186035\n",
      "Iteration: 220 loss: 0.0000000000 time: 0.03180694580078125\n",
      "Iteration: 230 loss: 0.0000000007 time: 0.040044546127319336\n",
      "Iteration: 240 loss: 0.0000000008 time: 0.038102149963378906\n",
      "Iteration: 250 loss: 0.0000000004 time: 0.033660888671875\n",
      "Iteration: 260 loss: 0.0000000001 time: 0.03604412078857422\n",
      "Iteration: 270 loss: 0.0000000000 time: 0.03523063659667969\n",
      "Iteration: 280 loss: 0.0000000000 time: 0.03917860984802246\n",
      "Iteration: 290 loss: 0.0000000000 time: 0.03496527671813965\n",
      "Iteration: 300 loss: 0.0000000000 time: 0.03606820106506348\n",
      "Iteration: 310 loss: 0.0000000000 time: 0.03272223472595215\n",
      "Iteration: 320 loss: 0.0000000000 time: 0.03955650329589844\n",
      "Iteration: 330 loss: 0.0000000000 time: 0.03822469711303711\n",
      "Iteration: 340 loss: 0.0000000000 time: 0.03626704216003418\n",
      "Iteration: 350 loss: 0.0000000000 time: 0.0347440242767334\n",
      "Iteration: 360 loss: 0.0000000000 time: 0.031561851501464844\n",
      "Iteration: 370 loss: 0.0000000000 time: 0.03292417526245117\n",
      "Iteration: 380 loss: 0.0000000000 time: 0.041875600814819336\n",
      "Iteration: 390 loss: 0.0000000000 time: 0.037301063537597656\n",
      "Iteration: 400 loss: 0.0000000000 time: 0.037938594818115234\n",
      "Iteration: 410 loss: 0.0000000000 time: 0.03696894645690918\n",
      "Iteration: 420 loss: 0.0000000000 time: 0.03306460380554199\n",
      "Iteration: 430 loss: 0.0000000000 time: 0.032607078552246094\n",
      "Iteration: 440 loss: 0.0000000000 time: 0.03655433654785156\n",
      "Iteration: 450 loss: 0.0000000000 time: 0.03685903549194336\n",
      "Iteration: 460 loss: 0.0000000000 time: 0.03991103172302246\n",
      "Iteration: 470 loss: 0.0000000000 time: 0.040373802185058594\n",
      "Iteration: 480 loss: 0.0000000000 time: 0.03863644599914551\n",
      "Iteration: 490 loss: 0.0000000000 time: 0.044475555419921875\n",
      "Iteration: 500 loss: 0.0000000000 time: 0.03860878944396973\n",
      "Iteration: 510 loss: 0.0000000000 time: 0.033983469009399414\n",
      "Iteration: 520 loss: 0.0000000000 time: 0.03692126274108887\n",
      "Iteration: 530 loss: 0.0000000000 time: 0.03734469413757324\n",
      "Iteration: 540 loss: 0.0000000000 time: 0.03426337242126465\n",
      "Iteration: 550 loss: 0.0000000000 time: 0.03233957290649414\n",
      "Iteration: 560 loss: 0.0000000000 time: 0.03876376152038574\n",
      "Iteration: 570 loss: 0.0000000000 time: 0.03429079055786133\n",
      "Iteration: 580 loss: 0.0000000000 time: 0.0373532772064209\n",
      "Iteration: 590 loss: 0.0000000000 time: 0.03565168380737305\n",
      "Iteration: 600 loss: 0.0000000000 time: 0.038339853286743164\n",
      "Iteration: 610 loss: 0.0000000000 time: 0.03770589828491211\n",
      "Iteration: 620 loss: 0.0000000000 time: 0.03795027732849121\n",
      "Iteration: 630 loss: 0.0000000000 time: 0.03432178497314453\n",
      "Iteration: 640 loss: 0.0000000000 time: 0.03459906578063965\n",
      "Iteration: 650 loss: 0.0000000000 time: 0.0361635684967041\n",
      "Iteration: 660 loss: 0.0000000000 time: 0.03885340690612793\n",
      "Iteration: 670 loss: 0.0000000000 time: 0.04173564910888672\n",
      "Iteration: 680 loss: 0.0000000000 time: 0.04078984260559082\n",
      "Iteration: 690 loss: 0.0000000000 time: 0.03695344924926758\n",
      "Iteration: 700 loss: 0.0000000000 time: 0.03439927101135254\n",
      "Iteration: 710 loss: 0.0000000000 time: 0.03380775451660156\n",
      "Iteration: 720 loss: 0.0000000000 time: 0.03382372856140137\n",
      "Iteration: 730 loss: 0.0000000000 time: 0.04078078269958496\n",
      "Iteration: 740 loss: 0.0000000000 time: 0.03443264961242676\n",
      "Iteration: 750 loss: 0.0000000000 time: 0.0412290096282959\n",
      "Iteration: 760 loss: 0.0000000000 time: 0.035437583923339844\n",
      "Iteration: 770 loss: 0.0000000000 time: 0.03206467628479004\n",
      "Iteration: 780 loss: 0.0000000000 time: 0.03526163101196289\n",
      "Iteration: 790 loss: 0.0000000000 time: 0.03571057319641113\n",
      "Iteration: 800 loss: 0.0000000000 time: 0.040531158447265625\n",
      "Iteration: 810 loss: 0.0000000000 time: 0.03763556480407715\n",
      "Iteration: 820 loss: 0.0000000000 time: 0.03937840461730957\n",
      "Iteration: 830 loss: 0.0000000000 time: 0.03451204299926758\n",
      "Iteration: 840 loss: 0.0000000000 time: 0.03819775581359863\n",
      "Iteration: 850 loss: 0.0000000000 time: 0.038152456283569336\n",
      "Iteration: 860 loss: 0.0000000000 time: 0.03992438316345215\n",
      "Iteration: 870 loss: 0.0000000000 time: 0.03323245048522949\n",
      "Iteration: 880 loss: 0.0000000000 time: 0.037944793701171875\n",
      "Iteration: 890 loss: 0.0000000000 time: 0.032091379165649414\n",
      "Iteration: 900 loss: 0.0000000000 time: 0.03545236587524414\n",
      "Iteration: 910 loss: 0.0000000000 time: 0.036948442459106445\n",
      "Iteration: 920 loss: 0.0000000000 time: 0.034832000732421875\n",
      "Iteration: 930 loss: 0.0000000000 time: 0.03932547569274902\n",
      "Iteration: 940 loss: 0.0000000000 time: 0.03643798828125\n",
      "Iteration: 950 loss: 0.0000000000 time: 0.03476357460021973\n",
      "Iteration: 960 loss: 0.0000000000 time: 0.035719871520996094\n",
      "Iteration: 970 loss: 0.0000000000 time: 0.03471517562866211\n",
      "Iteration: 980 loss: 0.0000000000 time: 0.0397191047668457\n",
      "Iteration: 990 loss: 0.0000000000 time: 0.03199911117553711\n",
      "Iteration: 1000 loss: 0.0000000000 time: 0.03522324562072754\n",
      "Iteration: 1010 loss: 0.0000000000 time: 0.036151885986328125\n",
      "Iteration: 1020 loss: 0.0000000000 time: 0.03645014762878418\n",
      "Iteration: 1030 loss: 0.0000000000 time: 0.036145687103271484\n",
      "Iteration: 1040 loss: 0.0000000000 time: 0.038291215896606445\n",
      "Iteration: 1050 loss: 0.0000000000 time: 0.04044961929321289\n",
      "Iteration: 1060 loss: 0.0000000000 time: 0.0377352237701416\n",
      "Iteration: 1070 loss: 0.0000000000 time: 0.04154801368713379\n",
      "Iteration: 1080 loss: 0.0000000000 time: 0.03973841667175293\n",
      "Iteration: 1090 loss: 0.0000000000 time: 0.04388713836669922\n",
      "Iteration: 1100 loss: 0.0000000000 time: 0.03990435600280762\n",
      "Iteration: 1110 loss: 0.0000000000 time: 0.04324960708618164\n",
      "Iteration: 1120 loss: 0.0000000000 time: 0.03526639938354492\n",
      "Iteration: 1130 loss: 0.0000000000 time: 0.0337834358215332\n",
      "Iteration: 1140 loss: 0.0000000000 time: 0.03550100326538086\n",
      "Iteration: 1150 loss: 0.0000000000 time: 0.0371251106262207\n",
      "Iteration: 1160 loss: 0.0000000000 time: 0.036974430084228516\n",
      "Iteration: 1170 loss: 0.0000000000 time: 0.03575944900512695\n",
      "Iteration: 1180 loss: 0.0000000000 time: 0.037725210189819336\n",
      "Iteration: 1190 loss: 0.0000000000 time: 0.03411126136779785\n",
      "Iteration: 1200 loss: 0.0000000000 time: 0.03221726417541504\n",
      "Iteration: 1210 loss: 0.0000000000 time: 0.03589773178100586\n",
      "Iteration: 1220 loss: 0.0000000000 time: 0.03705716133117676\n",
      "Iteration: 1230 loss: 0.0000000000 time: 0.038050174713134766\n",
      "Iteration: 1240 loss: 0.0000000000 time: 0.03385162353515625\n",
      "Iteration: 1250 loss: 0.0000000000 time: 0.03903031349182129\n",
      "Iteration: 1260 loss: 0.0000000000 time: 0.04062342643737793\n",
      "Iteration: 1270 loss: 0.0000000000 time: 0.03702044486999512\n",
      "Iteration: 1280 loss: 0.0000000000 time: 0.03620004653930664\n",
      "Iteration: 1290 loss: 0.0000000000 time: 0.036940574645996094\n",
      "Iteration: 1300 loss: 0.0000000000 time: 0.03432416915893555\n",
      "Iteration: 1310 loss: 0.0000000000 time: 0.04142117500305176\n",
      "Iteration: 1320 loss: 0.0000000000 time: 0.0366666316986084\n",
      "Iteration: 1330 loss: 0.0000000000 time: 0.03879809379577637\n",
      "Iteration: 1340 loss: 0.0000000000 time: 0.0457758903503418\n",
      "Iteration: 1350 loss: 0.0000000000 time: 0.041443824768066406\n",
      "Iteration: 1360 loss: 0.0000000000 time: 0.039611101150512695\n",
      "Iteration: 1370 loss: 0.0000000000 time: 0.03855180740356445\n",
      "Iteration: 1380 loss: 0.0000000000 time: 0.03922462463378906\n",
      "Iteration: 1390 loss: 0.0000000000 time: 0.043093204498291016\n",
      "Iteration: 1400 loss: 0.0000000000 time: 0.04113006591796875\n",
      "Iteration: 1410 loss: 0.0000000000 time: 0.041948556900024414\n",
      "Iteration: 1420 loss: 0.0000000000 time: 0.04170584678649902\n",
      "Iteration: 1430 loss: 0.0000000000 time: 0.03953671455383301\n",
      "Iteration: 1440 loss: 0.0000000000 time: 0.03875732421875\n",
      "Iteration: 1450 loss: 0.0000000000 time: 0.0316617488861084\n",
      "Iteration: 1460 loss: 0.0000000000 time: 0.028308391571044922\n",
      "Iteration: 1470 loss: 0.0000000000 time: 0.030069828033447266\n",
      "Iteration: 1480 loss: 0.0000000000 time: 0.03513526916503906\n",
      "Iteration: 1490 loss: 0.0000000000 time: 0.035660505294799805\n",
      "Iteration: 1500 loss: 0.0000000000 time: 0.037261962890625\n",
      "Iteration: 1510 loss: 0.0000000000 time: 0.037216901779174805\n",
      "Iteration: 1520 loss: 0.0000000000 time: 0.03662514686584473\n",
      "Iteration: 1530 loss: 0.0000000000 time: 0.03586530685424805\n",
      "Iteration: 1540 loss: 0.0000000000 time: 0.03733420372009277\n",
      "Iteration: 1550 loss: 0.0000000000 time: 0.03829026222229004\n",
      "Iteration: 1560 loss: 0.0000000000 time: 0.0362246036529541\n",
      "Iteration: 1570 loss: 0.0000000000 time: 0.03827404975891113\n",
      "Iteration: 1580 loss: 0.0000000000 time: 0.035131216049194336\n",
      "Iteration: 1590 loss: 0.0000000000 time: 0.034465789794921875\n",
      "Iteration: 1600 loss: 0.0000000000 time: 0.034338951110839844\n",
      "Iteration: 1610 loss: 0.0000000000 time: 0.035481929779052734\n",
      "Iteration: 1620 loss: 0.0000000000 time: 0.03498435020446777\n",
      "Iteration: 1630 loss: 0.0000000000 time: 0.03823661804199219\n",
      "Iteration: 1640 loss: 0.0000000000 time: 0.03688979148864746\n",
      "Iteration: 1650 loss: 0.0000000000 time: 0.03350520133972168\n",
      "Iteration: 1660 loss: 0.0000000000 time: 0.034772396087646484\n",
      "Iteration: 1670 loss: 0.0000000000 time: 0.035265445709228516\n",
      "Iteration: 1680 loss: 0.0000000000 time: 0.0389714241027832\n",
      "Iteration: 1690 loss: 0.0000000000 time: 0.033521413803100586\n",
      "Iteration: 1700 loss: 0.0000000000 time: 0.029584646224975586\n",
      "Iteration: 1710 loss: 0.0000000000 time: 0.03928947448730469\n",
      "Iteration: 1720 loss: 0.0000000000 time: 0.031769752502441406\n",
      "Iteration: 1730 loss: 0.0000000000 time: 0.04116630554199219\n",
      "Iteration: 1740 loss: 0.0000000000 time: 0.03282785415649414\n",
      "Iteration: 1750 loss: 0.0000000000 time: 0.032328128814697266\n",
      "Iteration: 1760 loss: 0.0000000000 time: 0.0317685604095459\n",
      "Iteration: 1770 loss: 0.0000000000 time: 0.030593156814575195\n",
      "Iteration: 1780 loss: 0.0000000000 time: 0.0361628532409668\n",
      "Iteration: 1790 loss: 0.0000000000 time: 0.03420376777648926\n",
      "Iteration: 1800 loss: 0.0000000000 time: 0.03537893295288086\n",
      "Iteration: 1810 loss: 0.0000000000 time: 0.03519916534423828\n",
      "Iteration: 1820 loss: 0.0000000000 time: 0.034834861755371094\n",
      "Iteration: 1830 loss: 0.0000000000 time: 0.02945685386657715\n",
      "Iteration: 1840 loss: 0.0000000000 time: 0.031114578247070312\n",
      "Iteration: 1850 loss: 0.0000000000 time: 0.03601813316345215\n",
      "Iteration: 1860 loss: 0.0000000000 time: 0.03639721870422363\n",
      "Iteration: 1870 loss: 0.0000000000 time: 0.03686809539794922\n",
      "Iteration: 1880 loss: 0.0000000000 time: 0.03268861770629883\n",
      "Iteration: 1890 loss: 0.0000000000 time: 0.030143022537231445\n",
      "Iteration: 1900 loss: 0.0000000000 time: 0.03241419792175293\n",
      "Iteration: 1910 loss: 0.0000000000 time: 0.031873226165771484\n",
      "Iteration: 1920 loss: 0.0000000000 time: 0.035489559173583984\n",
      "Iteration: 1930 loss: 0.0000000000 time: 0.03134584426879883\n",
      "Iteration: 1940 loss: 0.0000000000 time: 0.03575253486633301\n",
      "Iteration: 1950 loss: 0.0000000000 time: 0.03347182273864746\n",
      "Iteration: 1960 loss: 0.0000000000 time: 0.03210020065307617\n",
      "Iteration: 1970 loss: 0.0000000000 time: 0.032488107681274414\n",
      "Iteration: 1980 loss: 0.0000000000 time: 0.031531572341918945\n",
      "Iteration: 1990 loss: 0.0000000000 time: 0.032807350158691406\n",
      "Iteration: 2000 loss: 0.0000000000 time: 0.030591964721679688\n",
      "Iteration: 2010 loss: 0.0000000000 time: 0.033278465270996094\n",
      "Iteration: 2020 loss: 0.0000000000 time: 0.03541731834411621\n",
      "Iteration: 2030 loss: 0.0000000000 time: 0.03516077995300293\n",
      "Iteration: 2040 loss: 0.0000000000 time: 0.03441333770751953\n",
      "Iteration: 2050 loss: 0.0000000000 time: 0.03461623191833496\n",
      "Iteration: 2060 loss: 0.0000000000 time: 0.0312044620513916\n",
      "Iteration: 2070 loss: 0.0000000000 time: 0.0353550910949707\n",
      "Iteration: 2080 loss: 0.0000000000 time: 0.03965044021606445\n",
      "Iteration: 2090 loss: 0.0000000000 time: 0.03622555732727051\n",
      "Iteration: 2100 loss: 0.0000000000 time: 0.035703182220458984\n",
      "Iteration: 2110 loss: 0.0000000000 time: 0.0314030647277832\n",
      "Iteration: 2120 loss: 0.0000000000 time: 0.03202009201049805\n",
      "Iteration: 2130 loss: 0.0000000000 time: 0.0376734733581543\n",
      "Iteration: 2140 loss: 0.0000000000 time: 0.03653287887573242\n",
      "Iteration: 2150 loss: 0.0000000000 time: 0.03787660598754883\n",
      "Iteration: 2160 loss: 0.0000000000 time: 0.032402753829956055\n",
      "Iteration: 2170 loss: 0.0000000000 time: 0.03587985038757324\n",
      "Iteration: 2180 loss: 0.0000000000 time: 0.03172898292541504\n",
      "Iteration: 2190 loss: 0.0000000000 time: 0.04077482223510742\n",
      "Iteration: 2200 loss: 0.0000000000 time: 0.036179304122924805\n",
      "Iteration: 2210 loss: 0.0000000000 time: 0.03075885772705078\n",
      "Iteration: 2220 loss: 0.0000000000 time: 0.034136056900024414\n",
      "Iteration: 2230 loss: 0.0000000000 time: 0.033640384674072266\n",
      "Iteration: 2240 loss: 0.0000000000 time: 0.03414034843444824\n",
      "Iteration: 2250 loss: 0.0000000000 time: 0.033957481384277344\n",
      "Iteration: 2260 loss: 0.0000000000 time: 0.03734111785888672\n",
      "Iteration: 2270 loss: 0.0000000000 time: 0.0361475944519043\n",
      "Iteration: 2280 loss: 0.0000000000 time: 0.03621363639831543\n",
      "Iteration: 2290 loss: 0.0000000000 time: 0.037561893463134766\n",
      "Iteration: 2300 loss: 0.0000000000 time: 0.04136061668395996\n",
      "Iteration: 2310 loss: 0.0000000000 time: 0.03880786895751953\n",
      "Iteration: 2320 loss: 0.0000000000 time: 0.038280487060546875\n",
      "Iteration: 2330 loss: 0.0000000000 time: 0.033316850662231445\n",
      "Iteration: 2340 loss: 0.0000000000 time: 0.036107540130615234\n",
      "Iteration: 2350 loss: 0.0000000000 time: 0.0369112491607666\n",
      "Iteration: 2360 loss: 0.0000000000 time: 0.032746315002441406\n",
      "Iteration: 2370 loss: 0.0000000000 time: 0.03476381301879883\n",
      "Iteration: 2380 loss: 0.0000000000 time: 0.03749990463256836\n",
      "Iteration: 2390 loss: 0.0000000000 time: 0.04111361503601074\n",
      "Iteration: 2400 loss: 0.0000000000 time: 0.03759884834289551\n",
      "Iteration: 2410 loss: 0.0000000000 time: 0.038857460021972656\n",
      "Iteration: 2420 loss: 0.0000000000 time: 0.037024736404418945\n",
      "Iteration: 2430 loss: 0.0000000000 time: 0.03998732566833496\n",
      "Iteration: 2440 loss: 0.0000000000 time: 0.03934216499328613\n",
      "Iteration: 2450 loss: 0.0000000000 time: 0.03748822212219238\n",
      "Iteration: 2460 loss: 0.0000000000 time: 0.04540371894836426\n",
      "Iteration: 2470 loss: 0.0000000000 time: 0.04393792152404785\n",
      "Iteration: 2480 loss: 0.0000000000 time: 0.03859877586364746\n",
      "Iteration: 2490 loss: 0.0000000000 time: 0.04319477081298828\n",
      "Iteration: 2500 loss: 0.0000000000 time: 0.036104440689086914\n",
      "Iteration: 2510 loss: 0.0000000000 time: 0.036455392837524414\n",
      "Iteration: 2520 loss: 0.0000000000 time: 0.03684353828430176\n",
      "Iteration: 2530 loss: 0.0000000000 time: 0.03584647178649902\n",
      "Iteration: 2540 loss: 0.0000000000 time: 0.036870479583740234\n",
      "Iteration: 2550 loss: 0.0000000000 time: 0.03953194618225098\n",
      "Iteration: 2560 loss: 0.0000000000 time: 0.035823822021484375\n",
      "Iteration: 2570 loss: 0.0000000000 time: 0.035889625549316406\n",
      "Iteration: 2580 loss: 0.0000000000 time: 0.03545689582824707\n",
      "Iteration: 2590 loss: 0.0000000000 time: 0.03283262252807617\n",
      "Iteration: 2600 loss: 0.0000000000 time: 0.03350353240966797\n",
      "Iteration: 2610 loss: 0.0000000000 time: 0.03947114944458008\n",
      "Iteration: 2620 loss: 0.0000000000 time: 0.03564643859863281\n",
      "Iteration: 2630 loss: 0.0000000000 time: 0.03611350059509277\n",
      "Iteration: 2640 loss: 0.0000000000 time: 0.039897918701171875\n",
      "Iteration: 2650 loss: 0.0000000000 time: 0.035695791244506836\n",
      "Iteration: 2660 loss: 0.0000000000 time: 0.03975629806518555\n",
      "Iteration: 2670 loss: 0.0000000000 time: 0.03853583335876465\n",
      "Iteration: 2680 loss: 0.0000000000 time: 0.03568696975708008\n",
      "Iteration: 2690 loss: 0.0000000000 time: 0.03685712814331055\n",
      "Iteration: 2700 loss: 0.0000000000 time: 0.03926587104797363\n",
      "Iteration: 2710 loss: 0.0000000000 time: 0.0347750186920166\n",
      "Iteration: 2720 loss: 0.0000000000 time: 0.033050537109375\n",
      "Iteration: 2730 loss: 0.0000000000 time: 0.0406951904296875\n",
      "Iteration: 2740 loss: 0.0000000000 time: 0.03766345977783203\n",
      "Iteration: 2750 loss: 0.0000000000 time: 0.038741111755371094\n",
      "Iteration: 2760 loss: 0.0000000000 time: 0.03877615928649902\n",
      "Iteration: 2770 loss: 0.0000000000 time: 0.03892946243286133\n",
      "Iteration: 2780 loss: 0.0000000000 time: 0.041489601135253906\n",
      "Iteration: 2790 loss: 0.0000000000 time: 0.040331125259399414\n",
      "Iteration: 2800 loss: 0.0000000000 time: 0.03714704513549805\n",
      "Iteration: 2810 loss: 0.0000000000 time: 0.039217472076416016\n",
      "Iteration: 2820 loss: 0.0000000000 time: 0.03882646560668945\n",
      "Iteration: 2830 loss: 0.0000000000 time: 0.0372166633605957\n",
      "Iteration: 2840 loss: 0.0000000000 time: 0.042078256607055664\n",
      "Iteration: 2850 loss: 0.0000000000 time: 0.03811287879943848\n",
      "Iteration: 2860 loss: 0.0000000000 time: 0.038791656494140625\n",
      "Iteration: 2870 loss: 0.0000000000 time: 0.03908276557922363\n",
      "Iteration: 2880 loss: 0.0000000000 time: 0.0449221134185791\n",
      "Iteration: 2890 loss: 0.0000000000 time: 0.038571834564208984\n",
      "Iteration: 2900 loss: 0.0000000000 time: 0.039261579513549805\n",
      "Iteration: 2910 loss: 0.0000000000 time: 0.03791618347167969\n",
      "Iteration: 2920 loss: 0.0000000000 time: 0.04004096984863281\n",
      "Iteration: 2930 loss: 0.0000000000 time: 0.03108525276184082\n",
      "Iteration: 2940 loss: 0.0000000000 time: 0.03081989288330078\n",
      "Iteration: 2950 loss: 0.0000000000 time: 0.034035444259643555\n",
      "Iteration: 2960 loss: 0.0000000000 time: 0.034964799880981445\n",
      "Iteration: 2970 loss: 0.0000000000 time: 0.038956642150878906\n",
      "Iteration: 2980 loss: 0.0000000000 time: 0.0351254940032959\n",
      "Iteration: 2990 loss: 0.0000000000 time: 0.03872418403625488\n",
      "Iteration: 3000 loss: 0.0000000000 time: 0.03057575225830078\n",
      "Iteration: 3010 loss: 0.0000000000 time: 0.030501842498779297\n",
      "Iteration: 3020 loss: 0.0000000000 time: 0.0356135368347168\n",
      "Iteration: 3030 loss: 0.0000000000 time: 0.04125547409057617\n",
      "Iteration: 3040 loss: 0.0000000000 time: 0.037331342697143555\n",
      "Iteration: 3050 loss: 0.0000000000 time: 0.03162789344787598\n",
      "Iteration: 3060 loss: 0.0000000000 time: 0.03131294250488281\n",
      "Iteration: 3070 loss: 0.0000000000 time: 0.03149819374084473\n",
      "Iteration: 3080 loss: 0.0000000000 time: 0.041550397872924805\n",
      "Iteration: 3090 loss: 0.0000000000 time: 0.0389707088470459\n",
      "Iteration: 3100 loss: 0.0000000000 time: 0.033178091049194336\n",
      "Iteration: 3110 loss: 0.0000000000 time: 0.036290645599365234\n",
      "Iteration: 3120 loss: 0.0000000000 time: 0.03548383712768555\n",
      "Iteration: 3130 loss: 0.0000000000 time: 0.03773164749145508\n",
      "Iteration: 3140 loss: 0.0000000000 time: 0.036948442459106445\n",
      "Iteration: 3150 loss: 0.0000000000 time: 0.038572072982788086\n",
      "Iteration: 3160 loss: 0.0000000000 time: 0.03340339660644531\n",
      "Iteration: 3170 loss: 0.0000000000 time: 0.032819271087646484\n",
      "Iteration: 3180 loss: 0.0000000000 time: 0.03245902061462402\n",
      "Iteration: 3190 loss: 0.0000000000 time: 0.032199859619140625\n",
      "Iteration: 3200 loss: 0.0000000000 time: 0.03381156921386719\n",
      "Iteration: 3210 loss: 0.0000000000 time: 0.03478097915649414\n",
      "Iteration: 3220 loss: 0.0000000000 time: 0.03276801109313965\n",
      "Iteration: 3230 loss: 0.0000000000 time: 0.0380396842956543\n",
      "Iteration: 3240 loss: 0.0000000000 time: 0.03415870666503906\n",
      "Iteration: 3250 loss: 0.0000000000 time: 0.033521413803100586\n",
      "Iteration: 3260 loss: 0.0000000000 time: 0.03713083267211914\n",
      "Iteration: 3270 loss: 0.0000000000 time: 0.03389334678649902\n",
      "Iteration: 3280 loss: 0.0000000000 time: 0.02911376953125\n",
      "Iteration: 3290 loss: 0.0000000000 time: 0.03881406784057617\n",
      "Iteration: 3300 loss: 0.0000000000 time: 0.03615593910217285\n",
      "Iteration: 3310 loss: 0.0000000000 time: 0.034211158752441406\n",
      "Iteration: 3320 loss: 0.0000000000 time: 0.03412914276123047\n",
      "Iteration: 3330 loss: 0.0000000000 time: 0.03747749328613281\n",
      "Iteration: 3340 loss: 0.0000000000 time: 0.034628868103027344\n",
      "Iteration: 3350 loss: 0.0000000000 time: 0.03351640701293945\n",
      "Iteration: 3360 loss: 0.0000000000 time: 0.03270888328552246\n",
      "Iteration: 3370 loss: 0.0000000000 time: 0.03144383430480957\n",
      "Iteration: 3380 loss: 0.0000000000 time: 0.029738903045654297\n",
      "Iteration: 3390 loss: 0.0000000000 time: 0.03682374954223633\n",
      "Iteration: 3400 loss: 0.0000000000 time: 0.03658938407897949\n",
      "Iteration: 3410 loss: 0.0000000000 time: 0.030400514602661133\n",
      "Iteration: 3420 loss: 0.0000000000 time: 0.030896902084350586\n",
      "Iteration: 3430 loss: 0.0000000000 time: 0.037988901138305664\n",
      "Iteration: 3440 loss: 0.0000000000 time: 0.03133344650268555\n",
      "Iteration: 3450 loss: 0.0000000000 time: 0.029667377471923828\n",
      "Iteration: 3460 loss: 0.0000000000 time: 0.0388181209564209\n",
      "Iteration: 3470 loss: 0.0000000000 time: 0.033670902252197266\n",
      "Iteration: 3480 loss: 0.0000000000 time: 0.034063100814819336\n",
      "Iteration: 3490 loss: 0.0000000000 time: 0.04219460487365723\n",
      "Iteration: 3500 loss: 0.0000000000 time: 0.039755821228027344\n",
      "Iteration: 3510 loss: 0.0000000000 time: 0.03201484680175781\n",
      "Iteration: 3520 loss: 0.0000000000 time: 0.03279232978820801\n",
      "Iteration: 3530 loss: 0.0000000000 time: 0.03338479995727539\n",
      "Iteration: 3540 loss: 0.0000000000 time: 0.03077840805053711\n",
      "Iteration: 3550 loss: 0.0000000000 time: 0.03755068778991699\n",
      "Iteration: 3560 loss: 0.0000000000 time: 0.031671762466430664\n",
      "Iteration: 3570 loss: 0.0000000000 time: 0.0313878059387207\n",
      "Iteration: 3580 loss: 0.0000000000 time: 0.030939579010009766\n",
      "Iteration: 3590 loss: 0.0000000000 time: 0.03214454650878906\n",
      "Iteration: 3600 loss: 0.0000000000 time: 0.03241896629333496\n",
      "Iteration: 3610 loss: 0.0000000000 time: 0.03092360496520996\n",
      "Iteration: 3620 loss: 0.0000000000 time: 0.0336461067199707\n",
      "Iteration: 3630 loss: 0.0000000000 time: 0.03944683074951172\n",
      "Iteration: 3640 loss: 0.0000000000 time: 0.037723541259765625\n",
      "Iteration: 3650 loss: 0.0000000000 time: 0.037695884704589844\n",
      "Iteration: 3660 loss: 0.0000000000 time: 0.0410466194152832\n",
      "Iteration: 3670 loss: 0.0000000000 time: 0.038277626037597656\n",
      "Iteration: 3680 loss: 0.0000000000 time: 0.03658437728881836\n",
      "Iteration: 3690 loss: 0.0000000000 time: 0.03886103630065918\n",
      "Iteration: 3700 loss: 0.0000000000 time: 0.03780961036682129\n",
      "Iteration: 3710 loss: 0.0000000000 time: 0.03985261917114258\n",
      "Iteration: 3720 loss: 0.0000000000 time: 0.039925336837768555\n",
      "Iteration: 3730 loss: 0.0000000000 time: 0.03928685188293457\n",
      "Iteration: 3740 loss: 0.0000000000 time: 0.03815174102783203\n",
      "Iteration: 3750 loss: 0.0000000000 time: 0.03759646415710449\n",
      "Iteration: 3760 loss: 0.0000000000 time: 0.035874366760253906\n",
      "Iteration: 3770 loss: 0.0000000000 time: 0.03374934196472168\n",
      "Iteration: 3780 loss: 0.0000000000 time: 0.03563237190246582\n",
      "Iteration: 3790 loss: 0.0000000000 time: 0.033601999282836914\n",
      "Iteration: 3800 loss: 0.0000000000 time: 0.03384590148925781\n",
      "Iteration: 3810 loss: 0.0000000000 time: 0.034348487854003906\n",
      "Iteration: 3820 loss: 0.0000000000 time: 0.03261613845825195\n",
      "Iteration: 3830 loss: 0.0000000000 time: 0.03925013542175293\n",
      "Iteration: 3840 loss: 0.0000000000 time: 0.03714585304260254\n",
      "Iteration: 3850 loss: 0.0000000000 time: 0.03738284111022949\n",
      "Iteration: 3860 loss: 0.0000000000 time: 0.03503823280334473\n",
      "Iteration: 3870 loss: 0.0000000000 time: 0.03597307205200195\n",
      "Iteration: 3880 loss: 0.0000000000 time: 0.03536057472229004\n",
      "Iteration: 3890 loss: 0.0000000000 time: 0.03821134567260742\n",
      "Iteration: 3900 loss: 0.0000000000 time: 0.03883814811706543\n",
      "Iteration: 3910 loss: 0.0000000000 time: 0.03766632080078125\n",
      "Iteration: 3920 loss: 0.0000000000 time: 0.030373334884643555\n",
      "Iteration: 3930 loss: 0.0000000000 time: 0.03348398208618164\n",
      "Iteration: 3940 loss: 0.0000000000 time: 0.03753232955932617\n",
      "Iteration: 3950 loss: 0.0000000000 time: 0.03351020812988281\n",
      "Iteration: 3960 loss: 0.0000000000 time: 0.03580307960510254\n",
      "Iteration: 3970 loss: 0.0000000000 time: 0.029554128646850586\n",
      "Iteration: 3980 loss: 0.0000000000 time: 0.03106999397277832\n",
      "Iteration: 3990 loss: 0.0000000000 time: 0.032755136489868164\n",
      "Iteration: 4000 loss: 0.0000000000 time: 0.03340482711791992\n",
      "Iteration: 4010 loss: 0.0000000000 time: 0.038214683532714844\n",
      "Iteration: 4020 loss: 0.0000000000 time: 0.03652167320251465\n",
      "Iteration: 4030 loss: 0.0000000000 time: 0.03263282775878906\n",
      "Iteration: 4040 loss: 0.0000000000 time: 0.03558850288391113\n",
      "Iteration: 4050 loss: 0.0000000000 time: 0.031549930572509766\n",
      "Iteration: 4060 loss: 0.0000000000 time: 0.03172707557678223\n",
      "Iteration: 4070 loss: 0.0000000000 time: 0.033379554748535156\n",
      "Iteration: 4080 loss: 0.0000000000 time: 0.03873300552368164\n",
      "Iteration: 4090 loss: 0.0000000000 time: 0.03753781318664551\n",
      "Iteration: 4100 loss: 0.0000000000 time: 0.036878347396850586\n",
      "Iteration: 4110 loss: 0.0000000000 time: 0.03979921340942383\n",
      "Iteration: 4120 loss: 0.0000000000 time: 0.03876495361328125\n",
      "Iteration: 4130 loss: 0.0000000000 time: 0.0333409309387207\n",
      "Iteration: 4140 loss: 0.0000000000 time: 0.03605961799621582\n",
      "Iteration: 4150 loss: 0.0000000000 time: 0.0345761775970459\n",
      "Iteration: 4160 loss: 0.0000000000 time: 0.03403067588806152\n",
      "Iteration: 4170 loss: 0.0000000000 time: 0.03264451026916504\n",
      "Iteration: 4180 loss: 0.0000000000 time: 0.032769203186035156\n",
      "Iteration: 4190 loss: 0.0000000000 time: 0.03705406188964844\n",
      "Iteration: 4200 loss: 0.0000000000 time: 0.03646993637084961\n",
      "Iteration: 4210 loss: 0.0000000000 time: 0.03949761390686035\n",
      "Iteration: 4220 loss: 0.0000000000 time: 0.03566408157348633\n",
      "Iteration: 4230 loss: 0.0000000000 time: 0.0370638370513916\n",
      "Iteration: 4240 loss: 0.0000000000 time: 0.03727602958679199\n",
      "Iteration: 4250 loss: 0.0000000000 time: 0.03555870056152344\n",
      "Iteration: 4260 loss: 0.0000000000 time: 0.03795289993286133\n",
      "Iteration: 4270 loss: 0.0000000000 time: 0.0406033992767334\n",
      "Iteration: 4280 loss: 0.0000000000 time: 0.03595280647277832\n",
      "Iteration: 4290 loss: 0.0000000000 time: 0.03473329544067383\n",
      "Iteration: 4300 loss: 0.0000000000 time: 0.03714728355407715\n",
      "Iteration: 4310 loss: 0.0000000000 time: 0.03545689582824707\n",
      "Iteration: 4320 loss: 0.0000000000 time: 0.03744983673095703\n",
      "Iteration: 4330 loss: 0.0000000000 time: 0.038225412368774414\n",
      "Iteration: 4340 loss: 0.0000000000 time: 0.03599071502685547\n",
      "Iteration: 4350 loss: 0.0000000000 time: 0.03577780723571777\n",
      "Iteration: 4360 loss: 0.0000000000 time: 0.038317203521728516\n",
      "Iteration: 4370 loss: 0.0000000000 time: 0.035973310470581055\n",
      "Iteration: 4380 loss: 0.0000000000 time: 0.03346657752990723\n",
      "Iteration: 4390 loss: 0.0000000000 time: 0.03434467315673828\n",
      "Iteration: 4400 loss: 0.0000000000 time: 0.03114151954650879\n",
      "Iteration: 4410 loss: 0.0000000000 time: 0.03160524368286133\n",
      "Iteration: 4420 loss: 0.0000000000 time: 0.03547096252441406\n",
      "Iteration: 4430 loss: 0.0000000000 time: 0.02939319610595703\n",
      "Iteration: 4440 loss: 0.0000000000 time: 0.027837514877319336\n",
      "Iteration: 4450 loss: 0.0000000000 time: 0.036313533782958984\n",
      "Iteration: 4460 loss: 0.0000000000 time: 0.03882598876953125\n",
      "Iteration: 4470 loss: 0.0000000000 time: 0.03750729560852051\n",
      "Iteration: 4480 loss: 0.0000000000 time: 0.03976726531982422\n",
      "Iteration: 4490 loss: 0.0000000000 time: 0.03970503807067871\n",
      "Iteration: 4500 loss: 0.0000000000 time: 0.03719592094421387\n",
      "Iteration: 4510 loss: 0.0000000000 time: 0.03974461555480957\n",
      "Iteration: 4520 loss: 0.0000000000 time: 0.03575873374938965\n",
      "Iteration: 4530 loss: 0.0000000000 time: 0.035144805908203125\n",
      "Iteration: 4540 loss: 0.0000000000 time: 0.03416323661804199\n",
      "Iteration: 4550 loss: 0.0000000000 time: 0.03591752052307129\n",
      "Iteration: 4560 loss: 0.0000000000 time: 0.03415632247924805\n",
      "Iteration: 4570 loss: 0.0000000000 time: 0.03624296188354492\n",
      "Iteration: 4580 loss: 0.0000000000 time: 0.037020206451416016\n",
      "Iteration: 4590 loss: 0.0000000000 time: 0.034230709075927734\n",
      "Iteration: 4600 loss: 0.0000000000 time: 0.036971330642700195\n",
      "Iteration: 4610 loss: 0.0000000000 time: 0.03796982765197754\n",
      "Iteration: 4620 loss: 0.0000000000 time: 0.038855791091918945\n",
      "Iteration: 4630 loss: 0.0000000000 time: 0.03960990905761719\n",
      "Iteration: 4640 loss: 0.0000000000 time: 0.04086136817932129\n",
      "Iteration: 4650 loss: 0.0000000000 time: 0.03946661949157715\n",
      "Iteration: 4660 loss: 0.0000000000 time: 0.039632558822631836\n",
      "Iteration: 4670 loss: 0.0000000000 time: 0.037888288497924805\n",
      "Iteration: 4680 loss: 0.0000000000 time: 0.03803443908691406\n",
      "Iteration: 4690 loss: 0.0000000000 time: 0.03461027145385742\n",
      "Iteration: 4700 loss: 0.0000000000 time: 0.04193401336669922\n",
      "Iteration: 4710 loss: 0.0000000000 time: 0.03590083122253418\n",
      "Iteration: 4720 loss: 0.0000000000 time: 0.03547859191894531\n",
      "Iteration: 4730 loss: 0.0000000000 time: 0.030428647994995117\n",
      "Iteration: 4740 loss: 0.0000000000 time: 0.031140565872192383\n",
      "Iteration: 4750 loss: 0.0000000000 time: 0.03625226020812988\n",
      "Iteration: 4760 loss: 0.0000000000 time: 0.03701138496398926\n",
      "Iteration: 4770 loss: 0.0000000000 time: 0.03602766990661621\n",
      "Iteration: 4780 loss: 0.0000000000 time: 0.030980348587036133\n",
      "Iteration: 4790 loss: 0.0000000000 time: 0.02961134910583496\n",
      "Iteration: 4800 loss: 0.0000000000 time: 0.029790639877319336\n",
      "Iteration: 4810 loss: 0.0000000000 time: 0.03143620491027832\n",
      "Iteration: 4820 loss: 0.0000000000 time: 0.0365300178527832\n",
      "Iteration: 4830 loss: 0.0000000000 time: 0.0368502140045166\n",
      "Iteration: 4840 loss: 0.0000000000 time: 0.037481069564819336\n",
      "Iteration: 4850 loss: 0.0000000000 time: 0.03629636764526367\n",
      "Iteration: 4860 loss: 0.0000000000 time: 0.03578925132751465\n",
      "Iteration: 4870 loss: 0.0000000000 time: 0.038454294204711914\n",
      "Iteration: 4880 loss: 0.0000000000 time: 0.03968954086303711\n",
      "Iteration: 4890 loss: 0.0000000000 time: 0.038071393966674805\n",
      "Iteration: 4900 loss: 0.0000000000 time: 0.039650678634643555\n",
      "Iteration: 4910 loss: 0.0000000000 time: 0.03689885139465332\n",
      "Iteration: 4920 loss: 0.0000000000 time: 0.03706932067871094\n",
      "Iteration: 4930 loss: 0.0000000000 time: 0.03640389442443848\n",
      "Iteration: 4940 loss: 0.0000000000 time: 0.03697562217712402\n",
      "Iteration: 4950 loss: 0.0000000000 time: 0.036020755767822266\n",
      "Iteration: 4960 loss: 0.0000000000 time: 0.035920143127441406\n",
      "Iteration: 4970 loss: 0.0000000000 time: 0.03454875946044922\n",
      "Iteration: 4980 loss: 0.0000000000 time: 0.03278470039367676\n",
      "Iteration: 4990 loss: 0.0000000000 time: 0.030118227005004883\n",
      "Iteration: 5000 loss: 0.0000000000 time: 0.039726972579956055\n",
      "Iteration: 5010 loss: 0.0000000000 time: 0.037215232849121094\n",
      "Iteration: 5020 loss: 0.0000000000 time: 0.035805463790893555\n",
      "Iteration: 5030 loss: 0.0000000000 time: 0.03576254844665527\n",
      "Iteration: 5040 loss: 0.0000000000 time: 0.034621477127075195\n",
      "Iteration: 5050 loss: 0.0000000000 time: 0.04103827476501465\n",
      "Iteration: 5060 loss: 0.0000000000 time: 0.0312647819519043\n",
      "Iteration: 5070 loss: 0.0000000000 time: 0.03825974464416504\n",
      "Iteration: 5080 loss: 0.0000000000 time: 0.034781455993652344\n",
      "Iteration: 5090 loss: 0.0000000000 time: 0.03818202018737793\n",
      "Iteration: 5100 loss: 0.0000000000 time: 0.03309917449951172\n",
      "Iteration: 5110 loss: 0.0000000000 time: 0.031560420989990234\n",
      "Iteration: 5120 loss: 0.0000000000 time: 0.03922629356384277\n",
      "Iteration: 5130 loss: 0.0000000000 time: 0.03389787673950195\n",
      "Iteration: 5140 loss: 0.0000000000 time: 0.03169059753417969\n",
      "Iteration: 5150 loss: 0.0000000000 time: 0.03160905838012695\n",
      "Iteration: 5160 loss: 0.0000000000 time: 0.03304243087768555\n",
      "Iteration: 5170 loss: 0.0000000000 time: 0.031670570373535156\n",
      "Iteration: 5180 loss: 0.0000000000 time: 0.028839826583862305\n",
      "Iteration: 5190 loss: 0.0000000000 time: 0.029641151428222656\n",
      "Iteration: 5200 loss: 0.0000000000 time: 0.03919696807861328\n",
      "Iteration: 5210 loss: 0.0000000000 time: 0.03804922103881836\n",
      "Iteration: 5220 loss: 0.0000000000 time: 0.034168243408203125\n",
      "Iteration: 5230 loss: 0.0000000000 time: 0.04070878028869629\n",
      "Iteration: 5240 loss: 0.0000000000 time: 0.03363180160522461\n",
      "Iteration: 5250 loss: 0.0000000000 time: 0.03211188316345215\n",
      "Iteration: 5260 loss: 0.0000000000 time: 0.0347590446472168\n",
      "Iteration: 5270 loss: 0.0000000000 time: 0.02954268455505371\n",
      "Iteration: 5280 loss: 0.0000000000 time: 0.029400110244750977\n",
      "Iteration: 5290 loss: 0.0000000000 time: 0.03558659553527832\n",
      "Iteration: 5300 loss: 0.0000000000 time: 0.039881229400634766\n",
      "Iteration: 5310 loss: 0.0000000000 time: 0.03742671012878418\n",
      "Iteration: 5320 loss: 0.0000000000 time: 0.03436422348022461\n",
      "Iteration: 5330 loss: 0.0000000000 time: 0.0356292724609375\n",
      "Iteration: 5340 loss: 0.0000000000 time: 0.03283214569091797\n",
      "Iteration: 5350 loss: 0.0000000000 time: 0.037309885025024414\n",
      "Iteration: 5360 loss: 0.0000000000 time: 0.04012489318847656\n",
      "Iteration: 5370 loss: 0.0000000000 time: 0.03864288330078125\n",
      "Iteration: 5380 loss: 0.0000000000 time: 0.04235577583312988\n",
      "Iteration: 5390 loss: 0.0000000000 time: 0.03827166557312012\n",
      "Iteration: 5400 loss: 0.0000000000 time: 0.03666996955871582\n",
      "Iteration: 5410 loss: 0.0000000000 time: 0.036858320236206055\n",
      "Iteration: 5420 loss: 0.0000000000 time: 0.037630558013916016\n",
      "Iteration: 5430 loss: 0.0000000000 time: 0.038816213607788086\n",
      "Iteration: 5440 loss: 0.0000000000 time: 0.038039207458496094\n",
      "Iteration: 5450 loss: 0.0000000000 time: 0.03670215606689453\n",
      "Iteration: 5460 loss: 0.0000000000 time: 0.0341954231262207\n",
      "Iteration: 5470 loss: 0.0000000000 time: 0.031812429428100586\n",
      "Iteration: 5480 loss: 0.0000000000 time: 0.03857994079589844\n",
      "Iteration: 5490 loss: 0.0000000000 time: 0.03404045104980469\n",
      "Iteration: 5500 loss: 0.0000000000 time: 0.035832881927490234\n",
      "Iteration: 5510 loss: 0.0000000000 time: 0.035849809646606445\n",
      "Iteration: 5520 loss: 0.0000000000 time: 0.03334999084472656\n",
      "Iteration: 5530 loss: 0.0000000000 time: 0.03717494010925293\n",
      "Iteration: 5540 loss: 0.0000000000 time: 0.03198647499084473\n",
      "Iteration: 5550 loss: 0.0000000000 time: 0.030985593795776367\n",
      "Iteration: 5560 loss: 0.0000000000 time: 0.03527069091796875\n",
      "Iteration: 5570 loss: 0.0000000000 time: 0.03219175338745117\n",
      "Iteration: 5580 loss: 0.0000000000 time: 0.03461909294128418\n",
      "Iteration: 5590 loss: 0.0000000000 time: 0.035616397857666016\n",
      "Iteration: 5600 loss: 0.0000000000 time: 0.034082889556884766\n",
      "Iteration: 5610 loss: 0.0000000000 time: 0.033592939376831055\n",
      "Iteration: 5620 loss: 0.0000000000 time: 0.03288745880126953\n",
      "Iteration: 5630 loss: 0.0000000000 time: 0.035455942153930664\n",
      "Iteration: 5640 loss: 0.0000000000 time: 0.03804302215576172\n",
      "Iteration: 5650 loss: 0.0000000000 time: 0.035019636154174805\n",
      "Iteration: 5660 loss: 0.0000000000 time: 0.03263974189758301\n",
      "Iteration: 5670 loss: 0.0000000000 time: 0.037505388259887695\n",
      "Iteration: 5680 loss: 0.0000000000 time: 0.03881716728210449\n",
      "Iteration: 5690 loss: 0.0000000000 time: 0.04125547409057617\n",
      "Iteration: 5700 loss: 0.0000000000 time: 0.03769516944885254\n",
      "Iteration: 5710 loss: 0.0000000000 time: 0.036554813385009766\n",
      "Iteration: 5720 loss: 0.0000000000 time: 0.03658914566040039\n",
      "Iteration: 5730 loss: 0.0000000000 time: 0.03656172752380371\n",
      "Iteration: 5740 loss: 0.0000000000 time: 0.03442859649658203\n",
      "Iteration: 5750 loss: 0.0000000000 time: 0.030535459518432617\n",
      "Iteration: 5760 loss: 0.0000000000 time: 0.03284168243408203\n",
      "Iteration: 5770 loss: 0.0000000000 time: 0.032799482345581055\n",
      "Iteration: 5780 loss: 0.0000000000 time: 0.032752275466918945\n",
      "Iteration: 5790 loss: 0.0000000000 time: 0.037171363830566406\n",
      "Iteration: 5800 loss: 0.0000000000 time: 0.03607439994812012\n",
      "Iteration: 5810 loss: 0.0000000000 time: 0.03167915344238281\n",
      "Iteration: 5820 loss: 0.0000000000 time: 0.03978323936462402\n",
      "Iteration: 5830 loss: 0.0000000000 time: 0.03451108932495117\n",
      "Iteration: 5840 loss: 0.0000000000 time: 0.03730630874633789\n",
      "Iteration: 5850 loss: 0.0000000000 time: 0.0341649055480957\n",
      "Iteration: 5860 loss: 0.0000000000 time: 0.036206960678100586\n",
      "Iteration: 5870 loss: 0.0000000000 time: 0.03835773468017578\n",
      "Iteration: 5880 loss: 0.0000000000 time: 0.037320613861083984\n",
      "Iteration: 5890 loss: 0.0000000000 time: 0.032460927963256836\n",
      "Iteration: 5900 loss: 0.0000000000 time: 0.03366565704345703\n",
      "Iteration: 5910 loss: 0.0000000000 time: 0.035549163818359375\n",
      "Iteration: 5920 loss: 0.0000000000 time: 0.039234161376953125\n",
      "Iteration: 5930 loss: 0.0000000000 time: 0.04052019119262695\n",
      "Iteration: 5940 loss: 0.0000000000 time: 0.03243708610534668\n",
      "Iteration: 5950 loss: 0.0000000000 time: 0.0330653190612793\n",
      "Iteration: 5960 loss: 0.0000000000 time: 0.03617286682128906\n",
      "Iteration: 5970 loss: 0.0000000000 time: 0.03756260871887207\n",
      "Iteration: 5980 loss: 0.0000000000 time: 0.035552024841308594\n",
      "Iteration: 5990 loss: 0.0000000000 time: 0.044701576232910156\n",
      "Iteration: 6000 loss: 0.0000000000 time: 0.03171968460083008\n",
      "Iteration: 6010 loss: 0.0000000000 time: 0.030423641204833984\n",
      "Iteration: 6020 loss: 0.0000000000 time: 0.031080245971679688\n",
      "Iteration: 6030 loss: 0.0000000000 time: 0.03365492820739746\n",
      "Iteration: 6040 loss: 0.0000000000 time: 0.03380632400512695\n",
      "Iteration: 6050 loss: 0.0000000000 time: 0.032758235931396484\n",
      "Iteration: 6060 loss: 0.0000000000 time: 0.03618311882019043\n",
      "Iteration: 6070 loss: 0.0000000000 time: 0.03355264663696289\n",
      "Iteration: 6080 loss: 0.0000000000 time: 0.03348088264465332\n",
      "Iteration: 6090 loss: 0.0000000000 time: 0.036748647689819336\n",
      "Iteration: 6100 loss: 0.0000000000 time: 0.03468799591064453\n",
      "Iteration: 6110 loss: 0.0000000000 time: 0.03723025321960449\n",
      "Iteration: 6120 loss: 0.0000000000 time: 0.03592419624328613\n",
      "Iteration: 6130 loss: 0.0000000000 time: 0.035223960876464844\n",
      "Iteration: 6140 loss: 0.0000000000 time: 0.03578448295593262\n",
      "Iteration: 6150 loss: 0.0000000000 time: 0.03575587272644043\n",
      "Iteration: 6160 loss: 0.0000000000 time: 0.031683921813964844\n",
      "Iteration: 6170 loss: 0.0000000000 time: 0.03589582443237305\n",
      "Iteration: 6180 loss: 0.0000000000 time: 0.03959083557128906\n",
      "Iteration: 6190 loss: 0.0000000000 time: 0.033623456954956055\n",
      "Iteration: 6200 loss: 0.0000000000 time: 0.03288388252258301\n",
      "Iteration: 6210 loss: 0.0000000000 time: 0.03317117691040039\n",
      "Iteration: 6220 loss: 0.0000000000 time: 0.04239487648010254\n",
      "Iteration: 6230 loss: 0.0000000000 time: 0.030983448028564453\n",
      "Iteration: 6240 loss: 0.0000000000 time: 0.035210371017456055\n",
      "Iteration: 6250 loss: 0.0000000000 time: 0.03866124153137207\n",
      "Iteration: 6260 loss: 0.0000000000 time: 0.03592085838317871\n",
      "Iteration: 6270 loss: 0.0000000000 time: 0.03763389587402344\n",
      "Iteration: 6280 loss: 0.0000000000 time: 0.03505444526672363\n",
      "Iteration: 6290 loss: 0.0000000000 time: 0.03647136688232422\n",
      "Iteration: 6300 loss: 0.0000000000 time: 0.03502535820007324\n",
      "Iteration: 6310 loss: 0.0000000000 time: 0.037085533142089844\n",
      "Iteration: 6320 loss: 0.0000000000 time: 0.03384280204772949\n",
      "Iteration: 6330 loss: 0.0000000000 time: 0.031977176666259766\n",
      "Iteration: 6340 loss: 0.0000000000 time: 0.03642630577087402\n",
      "Iteration: 6350 loss: 0.0000000000 time: 0.03438377380371094\n",
      "Iteration: 6360 loss: 0.0000000000 time: 0.034445762634277344\n",
      "Iteration: 6370 loss: 0.0000000000 time: 0.03733348846435547\n",
      "Iteration: 6380 loss: 0.0000000000 time: 0.03716301918029785\n",
      "Iteration: 6390 loss: 0.0000000000 time: 0.03777337074279785\n",
      "Iteration: 6400 loss: 0.0000000000 time: 0.03863811492919922\n",
      "Iteration: 6410 loss: 0.0000000000 time: 0.04033493995666504\n",
      "Iteration: 6420 loss: 0.0000000000 time: 0.038819074630737305\n",
      "Iteration: 6430 loss: 0.0000000000 time: 0.037787437438964844\n",
      "Iteration: 6440 loss: 0.0000000000 time: 0.03842473030090332\n",
      "Iteration: 6450 loss: 0.0000000000 time: 0.03929543495178223\n",
      "Iteration: 6460 loss: 0.0000000000 time: 0.038674116134643555\n",
      "Iteration: 6470 loss: 0.0000000000 time: 0.03875398635864258\n",
      "Iteration: 6480 loss: 0.0000000000 time: 0.03748297691345215\n",
      "Iteration: 6490 loss: 0.0000000000 time: 0.03770136833190918\n",
      "Iteration: 6500 loss: 0.0000000000 time: 0.039003849029541016\n",
      "Iteration: 6510 loss: 0.0000000000 time: 0.03843045234680176\n",
      "Iteration: 6520 loss: 0.0000000000 time: 0.03675222396850586\n",
      "Iteration: 6530 loss: 0.0000000000 time: 0.04152679443359375\n",
      "Iteration: 6540 loss: 0.0000000000 time: 0.034403085708618164\n",
      "Iteration: 6550 loss: 0.0000000000 time: 0.03471016883850098\n",
      "Iteration: 6560 loss: 0.0000000000 time: 0.03377413749694824\n",
      "Iteration: 6570 loss: 0.0000000000 time: 0.03017425537109375\n",
      "Iteration: 6580 loss: 0.0000000000 time: 0.03043532371520996\n",
      "Iteration: 6590 loss: 0.0000000000 time: 0.03320050239562988\n",
      "Iteration: 6600 loss: 0.0000000000 time: 0.04098033905029297\n",
      "Iteration: 6610 loss: 0.0000000000 time: 0.03974318504333496\n",
      "Iteration: 6620 loss: 0.0000000000 time: 0.04418158531188965\n",
      "Iteration: 6630 loss: 0.0000000000 time: 0.03758835792541504\n",
      "Iteration: 6640 loss: 0.0000000000 time: 0.04296517372131348\n",
      "Iteration: 6650 loss: 0.0000000000 time: 0.03965568542480469\n",
      "Iteration: 6660 loss: 0.0000000000 time: 0.03770756721496582\n",
      "Iteration: 6670 loss: 0.0000000000 time: 0.038163185119628906\n",
      "Iteration: 6680 loss: 0.0000000000 time: 0.03611493110656738\n",
      "Iteration: 6690 loss: 0.0000000000 time: 0.03454732894897461\n",
      "Iteration: 6700 loss: 0.0000000000 time: 0.04110288619995117\n",
      "Iteration: 6710 loss: 0.0000000000 time: 0.041422128677368164\n",
      "Iteration: 6720 loss: 0.0000000000 time: 0.042189598083496094\n",
      "Iteration: 6730 loss: 0.0000000000 time: 0.04128003120422363\n",
      "Iteration: 6740 loss: 0.0000000000 time: 0.0399932861328125\n",
      "Iteration: 6750 loss: 0.0000000000 time: 0.033797502517700195\n",
      "Iteration: 6760 loss: 0.0000000000 time: 0.03734636306762695\n",
      "Iteration: 6770 loss: 0.0000000000 time: 0.04203915596008301\n",
      "Iteration: 6780 loss: 0.0000000000 time: 0.04075169563293457\n",
      "Iteration: 6790 loss: 0.0000000000 time: 0.04100656509399414\n",
      "Iteration: 6800 loss: 0.0000000000 time: 0.042581796646118164\n",
      "Iteration: 6810 loss: 0.0000000000 time: 0.036895036697387695\n",
      "Iteration: 6820 loss: 0.0000000000 time: 0.043152809143066406\n",
      "Iteration: 6830 loss: 0.0000000000 time: 0.04188275337219238\n",
      "Iteration: 6840 loss: 0.0000000000 time: 0.042734384536743164\n",
      "Iteration: 6850 loss: 0.0000000000 time: 0.037076711654663086\n",
      "Iteration: 6860 loss: 0.0000000000 time: 0.043201446533203125\n",
      "Iteration: 6870 loss: 0.0000000000 time: 0.048668861389160156\n",
      "Iteration: 6880 loss: 0.0000000000 time: 0.03993844985961914\n",
      "Iteration: 6890 loss: 0.0000000000 time: 0.04082298278808594\n",
      "Iteration: 6900 loss: 0.0000000000 time: 0.037787675857543945\n",
      "Iteration: 6910 loss: 0.0000000000 time: 0.044408321380615234\n",
      "Iteration: 6920 loss: 0.0000000000 time: 0.04318404197692871\n",
      "Iteration: 6930 loss: 0.0000000000 time: 0.04165816307067871\n",
      "Iteration: 6940 loss: 0.0000000000 time: 0.03728199005126953\n",
      "Iteration: 6950 loss: 0.0000000000 time: 0.0376584529876709\n",
      "Iteration: 6960 loss: 0.0000000000 time: 0.037851572036743164\n",
      "Iteration: 6970 loss: 0.0000000000 time: 0.04005789756774902\n",
      "Iteration: 6980 loss: 0.0000000000 time: 0.039000511169433594\n",
      "Iteration: 6990 loss: 0.0000000000 time: 0.03871941566467285\n",
      "Iteration: 7000 loss: 0.0000000000 time: 0.04111218452453613\n",
      "Iteration: 7010 loss: 0.0000000000 time: 0.04010486602783203\n",
      "Iteration: 7020 loss: 0.0000000000 time: 0.041051387786865234\n",
      "Iteration: 7030 loss: 0.0000000000 time: 0.04383540153503418\n",
      "Iteration: 7040 loss: 0.0000000000 time: 0.04237508773803711\n",
      "Iteration: 7050 loss: 0.0000000000 time: 0.04065704345703125\n",
      "Iteration: 7060 loss: 0.0000000000 time: 0.04303121566772461\n",
      "Iteration: 7070 loss: 0.0000000000 time: 0.049417734146118164\n",
      "Iteration: 7080 loss: 0.0000000000 time: 0.0523066520690918\n",
      "Iteration: 7090 loss: 0.0000000000 time: 0.045772552490234375\n",
      "Iteration: 7100 loss: 0.0000000000 time: 0.03836870193481445\n",
      "Iteration: 7110 loss: 0.0000000000 time: 0.035674333572387695\n",
      "Iteration: 7120 loss: 0.0000000000 time: 0.03812837600708008\n",
      "Iteration: 7130 loss: 0.0000000000 time: 0.03371167182922363\n",
      "Iteration: 7140 loss: 0.0000000000 time: 0.04046010971069336\n",
      "Iteration: 7150 loss: 0.0000000000 time: 0.04650473594665527\n",
      "Iteration: 7160 loss: 0.0000000000 time: 0.04408001899719238\n",
      "Iteration: 7170 loss: 0.0000000000 time: 0.043565988540649414\n",
      "Iteration: 7180 loss: 0.0000000000 time: 0.04644489288330078\n",
      "Iteration: 7190 loss: 0.0000000000 time: 0.04414558410644531\n",
      "Iteration: 7200 loss: 0.0000000000 time: 0.05356574058532715\n",
      "Iteration: 7210 loss: 0.0000000000 time: 0.03977394104003906\n",
      "Iteration: 7220 loss: 0.0000000000 time: 0.04441046714782715\n",
      "Iteration: 7230 loss: 0.0000000000 time: 0.042224884033203125\n",
      "Iteration: 7240 loss: 0.0000000000 time: 0.042055606842041016\n",
      "Iteration: 7250 loss: 0.0000000000 time: 0.04062485694885254\n",
      "Iteration: 7260 loss: 0.0000000000 time: 0.034722089767456055\n",
      "Iteration: 7270 loss: 0.0000000000 time: 0.043923139572143555\n",
      "Iteration: 7280 loss: 0.0000000000 time: 0.043044090270996094\n",
      "Iteration: 7290 loss: 0.0000000000 time: 0.0365297794342041\n",
      "Iteration: 7300 loss: 0.0000000000 time: 0.03639554977416992\n",
      "Iteration: 7310 loss: 0.0000000000 time: 0.033481597900390625\n",
      "Iteration: 7320 loss: 0.0000000000 time: 0.04119062423706055\n",
      "Iteration: 7330 loss: 0.0000000000 time: 0.044734954833984375\n",
      "Iteration: 7340 loss: 0.0000000000 time: 0.03979015350341797\n",
      "Iteration: 7350 loss: 0.0000000000 time: 0.03931617736816406\n",
      "Iteration: 7360 loss: 0.0000000000 time: 0.04104042053222656\n",
      "Iteration: 7370 loss: 0.0000000000 time: 0.042310476303100586\n",
      "Iteration: 7380 loss: 0.0000000000 time: 0.04369378089904785\n",
      "Iteration: 7390 loss: 0.0000000000 time: 0.03865194320678711\n",
      "Iteration: 7400 loss: 0.0000000000 time: 0.03856825828552246\n",
      "Iteration: 7410 loss: 0.0000000000 time: 0.036132097244262695\n",
      "Iteration: 7420 loss: 0.0000000000 time: 0.040955543518066406\n",
      "Iteration: 7430 loss: 0.0000000000 time: 0.0372624397277832\n",
      "Iteration: 7440 loss: 0.0000000000 time: 0.03623509407043457\n",
      "Iteration: 7450 loss: 0.0000000000 time: 0.03668999671936035\n",
      "Iteration: 7460 loss: 0.0000000000 time: 0.036318302154541016\n",
      "Iteration: 7470 loss: 0.0000000000 time: 0.035997867584228516\n",
      "Iteration: 7480 loss: 0.0000000000 time: 0.03669261932373047\n",
      "Iteration: 7490 loss: 0.0000000000 time: 0.038553476333618164\n",
      "Iteration: 7500 loss: 0.0000000000 time: 0.03758525848388672\n",
      "Iteration: 7510 loss: 0.0000000000 time: 0.03653597831726074\n",
      "Iteration: 7520 loss: 0.0000000000 time: 0.03042912483215332\n",
      "Iteration: 7530 loss: 0.0000000000 time: 0.03585195541381836\n",
      "Iteration: 7540 loss: 0.0000000000 time: 0.03277301788330078\n",
      "Iteration: 7550 loss: 0.0000000000 time: 0.0399017333984375\n",
      "Iteration: 7560 loss: 0.0000000000 time: 0.036115407943725586\n",
      "Iteration: 7570 loss: 0.0000000000 time: 0.03569769859313965\n",
      "Iteration: 7580 loss: 0.0000000000 time: 0.03637504577636719\n",
      "Iteration: 7590 loss: 0.0000000000 time: 0.035010576248168945\n",
      "Iteration: 7600 loss: 0.0000000000 time: 0.038796424865722656\n",
      "Iteration: 7610 loss: 0.0000000000 time: 0.03756976127624512\n",
      "Iteration: 7620 loss: 0.0000000000 time: 0.034655094146728516\n",
      "Iteration: 7630 loss: 0.0000000000 time: 0.040290117263793945\n",
      "Iteration: 7640 loss: 0.0000000000 time: 0.03944253921508789\n",
      "Iteration: 7650 loss: 0.0000000000 time: 0.03274416923522949\n",
      "Iteration: 7660 loss: 0.0000000000 time: 0.0362553596496582\n",
      "Iteration: 7670 loss: 0.0000000000 time: 0.035056114196777344\n",
      "Iteration: 7680 loss: 0.0000000000 time: 0.035863399505615234\n",
      "Iteration: 7690 loss: 0.0000000000 time: 0.03963160514831543\n",
      "Iteration: 7700 loss: 0.0000000000 time: 0.037307024002075195\n",
      "Iteration: 7710 loss: 0.0000000000 time: 0.03466200828552246\n",
      "Iteration: 7720 loss: 0.0000000000 time: 0.03764462471008301\n",
      "Iteration: 7730 loss: 0.0000000000 time: 0.03748512268066406\n",
      "Iteration: 7740 loss: 0.0000000000 time: 0.033582210540771484\n",
      "Iteration: 7750 loss: 0.0000000000 time: 0.031015872955322266\n",
      "Iteration: 7760 loss: 0.0000000000 time: 0.032980918884277344\n",
      "Iteration: 7770 loss: 0.0000000000 time: 0.03554415702819824\n",
      "Iteration: 7780 loss: 0.0000000000 time: 0.035954952239990234\n",
      "Iteration: 7790 loss: 0.0000000000 time: 0.039342403411865234\n",
      "Iteration: 7800 loss: 0.0000000000 time: 0.03643226623535156\n",
      "Iteration: 7810 loss: 0.0000000000 time: 0.03836846351623535\n",
      "Iteration: 7820 loss: 0.0000000000 time: 0.0310211181640625\n",
      "Iteration: 7830 loss: 0.0000000000 time: 0.03158712387084961\n",
      "Iteration: 7840 loss: 0.0000000000 time: 0.03733325004577637\n",
      "Iteration: 7850 loss: 0.0000000000 time: 0.03790855407714844\n",
      "Iteration: 7860 loss: 0.0000000000 time: 0.04395866394042969\n",
      "Iteration: 7870 loss: 0.0000000000 time: 0.03715682029724121\n",
      "Iteration: 7880 loss: 0.0000000000 time: 0.03646206855773926\n",
      "Iteration: 7890 loss: 0.0000000000 time: 0.039078712463378906\n",
      "Iteration: 7900 loss: 0.0000000000 time: 0.044708967208862305\n",
      "Iteration: 7910 loss: 0.0000000000 time: 0.03610706329345703\n",
      "Iteration: 7920 loss: 0.0000000000 time: 0.03621077537536621\n",
      "Iteration: 7930 loss: 0.0000000000 time: 0.03550577163696289\n",
      "Iteration: 7940 loss: 0.0000000000 time: 0.0375058650970459\n",
      "Iteration: 7950 loss: 0.0000000000 time: 0.03790020942687988\n",
      "Iteration: 7960 loss: 0.0000000000 time: 0.0418851375579834\n",
      "Iteration: 7970 loss: 0.0000000000 time: 0.03936123847961426\n",
      "Iteration: 7980 loss: 0.0000000000 time: 0.036995649337768555\n",
      "Iteration: 7990 loss: 0.0000000000 time: 0.03735160827636719\n",
      "Iteration: 8000 loss: 0.0000000000 time: 0.037923336029052734\n",
      "Iteration: 8010 loss: 0.0000000000 time: 0.03930950164794922\n",
      "Iteration: 8020 loss: 0.0000000000 time: 0.03951597213745117\n",
      "Iteration: 8030 loss: 0.0000000000 time: 0.039772987365722656\n",
      "Iteration: 8040 loss: 0.0000000000 time: 0.03680825233459473\n",
      "Iteration: 8050 loss: 0.0000000000 time: 0.03799152374267578\n",
      "Iteration: 8060 loss: 0.0000000000 time: 0.03802156448364258\n",
      "Iteration: 8070 loss: 0.0000000000 time: 0.03775811195373535\n",
      "Iteration: 8080 loss: 0.0000000000 time: 0.039839982986450195\n",
      "Iteration: 8090 loss: 0.0000000000 time: 0.03794503211975098\n",
      "Iteration: 8100 loss: 0.0000000000 time: 0.03815293312072754\n",
      "Iteration: 8110 loss: 0.0000000000 time: 0.03711724281311035\n",
      "Iteration: 8120 loss: 0.0000000000 time: 0.03523659706115723\n",
      "Iteration: 8130 loss: 0.0000000000 time: 0.03527235984802246\n",
      "Iteration: 8140 loss: 0.0000000000 time: 0.03832840919494629\n",
      "Iteration: 8150 loss: 0.0000000000 time: 0.03937959671020508\n",
      "Iteration: 8160 loss: 0.0000000000 time: 0.04370617866516113\n",
      "Iteration: 8170 loss: 0.0000000000 time: 0.04098343849182129\n",
      "Iteration: 8180 loss: 0.0000000000 time: 0.04800677299499512\n",
      "Iteration: 8190 loss: 0.0000000000 time: 0.04493212699890137\n",
      "Iteration: 8200 loss: 0.0000000000 time: 0.04565739631652832\n",
      "Iteration: 8210 loss: 0.0000000000 time: 0.04175567626953125\n",
      "Iteration: 8220 loss: 0.0000000000 time: 0.04308128356933594\n",
      "Iteration: 8230 loss: 0.0000000000 time: 0.03221774101257324\n",
      "Iteration: 8240 loss: 0.0000000000 time: 0.03441905975341797\n",
      "Iteration: 8250 loss: 0.0000000000 time: 0.03402996063232422\n",
      "Iteration: 8260 loss: 0.0000000000 time: 0.032054901123046875\n",
      "Iteration: 8270 loss: 0.0000000000 time: 0.03821301460266113\n",
      "Iteration: 8280 loss: 0.0000000000 time: 0.0399022102355957\n",
      "Iteration: 8290 loss: 0.0000000000 time: 0.039714813232421875\n",
      "Iteration: 8300 loss: 0.0000000000 time: 0.03971600532531738\n",
      "Iteration: 8310 loss: 0.0000000000 time: 0.042328834533691406\n",
      "Iteration: 8320 loss: 0.0000000000 time: 0.04032707214355469\n",
      "Iteration: 8330 loss: 0.0000000000 time: 0.03934144973754883\n",
      "Iteration: 8340 loss: 0.0000000000 time: 0.039865732192993164\n",
      "Iteration: 8350 loss: 0.0000000000 time: 0.039904117584228516\n",
      "Iteration: 8360 loss: 0.0000000000 time: 0.03968405723571777\n",
      "Iteration: 8370 loss: 0.0000000000 time: 0.04279613494873047\n",
      "Iteration: 8380 loss: 0.0000000000 time: 0.0403745174407959\n",
      "Iteration: 8390 loss: 0.0000000000 time: 0.0398256778717041\n",
      "Iteration: 8400 loss: 0.0000000000 time: 0.0380864143371582\n",
      "Iteration: 8410 loss: 0.0000000000 time: 0.038756370544433594\n",
      "Iteration: 8420 loss: 0.0000000000 time: 0.04119539260864258\n",
      "Iteration: 8430 loss: 0.0000000000 time: 0.03846478462219238\n",
      "Iteration: 8440 loss: 0.0000000000 time: 0.033998727798461914\n",
      "Iteration: 8450 loss: 0.0000000000 time: 0.03290247917175293\n",
      "Iteration: 8460 loss: 0.0000000000 time: 0.03563666343688965\n",
      "Iteration: 8470 loss: 0.0000000000 time: 0.03339719772338867\n",
      "Iteration: 8480 loss: 0.0000000000 time: 0.03274655342102051\n",
      "Iteration: 8490 loss: 0.0000000000 time: 0.03322005271911621\n",
      "Iteration: 8500 loss: 0.0000000000 time: 0.0380401611328125\n",
      "Iteration: 8510 loss: 0.0000000000 time: 0.034417152404785156\n",
      "Iteration: 8520 loss: 0.0000000000 time: 0.03520941734313965\n",
      "Iteration: 8530 loss: 0.0000000000 time: 0.038503408432006836\n",
      "Iteration: 8540 loss: 0.0000000000 time: 0.03777480125427246\n",
      "Iteration: 8550 loss: 0.0000000000 time: 0.03916144371032715\n",
      "Iteration: 8560 loss: 0.0000000000 time: 0.0388951301574707\n",
      "Iteration: 8570 loss: 0.0000000000 time: 0.03822636604309082\n",
      "Iteration: 8580 loss: 0.0000000000 time: 0.038170814514160156\n",
      "Iteration: 8590 loss: 0.0000000000 time: 0.038346290588378906\n",
      "Iteration: 8600 loss: 0.0000000000 time: 0.0380396842956543\n",
      "Iteration: 8610 loss: 0.0000000000 time: 0.04170727729797363\n",
      "Iteration: 8620 loss: 0.0000000000 time: 0.0348358154296875\n",
      "Iteration: 8630 loss: 0.0000000000 time: 0.03416323661804199\n",
      "Iteration: 8640 loss: 0.0000000000 time: 0.03925061225891113\n",
      "Iteration: 8650 loss: 0.0000000000 time: 0.03872537612915039\n",
      "Iteration: 8660 loss: 0.0000000000 time: 0.040287017822265625\n",
      "Iteration: 8670 loss: 0.0000000000 time: 0.04232478141784668\n",
      "Iteration: 8680 loss: 0.0000000000 time: 0.03728437423706055\n",
      "Iteration: 8690 loss: 0.0000000000 time: 0.03554415702819824\n",
      "Iteration: 8700 loss: 0.0000000000 time: 0.03578495979309082\n",
      "Iteration: 8710 loss: 0.0000000000 time: 0.03496241569519043\n",
      "Iteration: 8720 loss: 0.0000000000 time: 0.03661513328552246\n",
      "Iteration: 8730 loss: 0.0000000000 time: 0.0365605354309082\n",
      "Iteration: 8740 loss: 0.0000000000 time: 0.03918957710266113\n",
      "Iteration: 8750 loss: 0.0000000000 time: 0.03682112693786621\n",
      "Iteration: 8760 loss: 0.0000000000 time: 0.03827524185180664\n",
      "Iteration: 8770 loss: 0.0000000000 time: 0.03746652603149414\n",
      "Iteration: 8780 loss: 0.0000000000 time: 0.03832697868347168\n",
      "Iteration: 8790 loss: 0.0000000000 time: 0.04112505912780762\n",
      "Iteration: 8800 loss: 0.0000000000 time: 0.037661075592041016\n",
      "Iteration: 8810 loss: 0.0000000000 time: 0.030577421188354492\n",
      "Iteration: 8820 loss: 0.0000000000 time: 0.03188776969909668\n",
      "Iteration: 8830 loss: 0.0000000000 time: 0.03997302055358887\n",
      "Iteration: 8840 loss: 0.0000000000 time: 0.031670570373535156\n",
      "Iteration: 8850 loss: 0.0000000000 time: 0.03874802589416504\n",
      "Iteration: 8860 loss: 0.0000000000 time: 0.03632950782775879\n",
      "Iteration: 8870 loss: 0.0000000000 time: 0.03575921058654785\n",
      "Iteration: 8880 loss: 0.0000000000 time: 0.035421133041381836\n",
      "Iteration: 8890 loss: 0.0000000000 time: 0.0303499698638916\n",
      "Iteration: 8900 loss: 0.0000000000 time: 0.029534339904785156\n",
      "Iteration: 8910 loss: 0.0000000000 time: 0.03451061248779297\n",
      "Iteration: 8920 loss: 0.0000000000 time: 0.03903651237487793\n",
      "Iteration: 8930 loss: 0.0000000000 time: 0.030870437622070312\n",
      "Iteration: 8940 loss: 0.0000000000 time: 0.03528308868408203\n",
      "Iteration: 8950 loss: 0.0000000000 time: 0.03762245178222656\n",
      "Iteration: 8960 loss: 0.0000000000 time: 0.03655576705932617\n",
      "Iteration: 8970 loss: 0.0000000000 time: 0.041464805603027344\n",
      "Iteration: 8980 loss: 0.0000000000 time: 0.03681683540344238\n",
      "Iteration: 8990 loss: 0.0000000000 time: 0.03335905075073242\n",
      "Iteration: 9000 loss: 0.0000000000 time: 0.03325295448303223\n",
      "Iteration: 9010 loss: 0.0000000000 time: 0.03320622444152832\n",
      "Iteration: 9020 loss: 0.0000000000 time: 0.035123348236083984\n",
      "Iteration: 9030 loss: 0.0000000000 time: 0.03766369819641113\n",
      "Iteration: 9040 loss: 0.0000000000 time: 0.0448307991027832\n",
      "Iteration: 9050 loss: 0.0000000000 time: 0.04088783264160156\n",
      "Iteration: 9060 loss: 0.0000000000 time: 0.04531431198120117\n",
      "Iteration: 9070 loss: 0.0000000000 time: 0.03730463981628418\n",
      "Iteration: 9080 loss: 0.0000000000 time: 0.04066181182861328\n",
      "Iteration: 9090 loss: 0.0000000000 time: 0.044806718826293945\n",
      "Iteration: 9100 loss: 0.0000000000 time: 0.04565763473510742\n",
      "Iteration: 9110 loss: 0.0000000000 time: 0.04002118110656738\n",
      "Iteration: 9120 loss: 0.0000000000 time: 0.043639421463012695\n",
      "Iteration: 9130 loss: 0.0000000000 time: 0.03971433639526367\n",
      "Iteration: 9140 loss: 0.0000000000 time: 0.040247440338134766\n",
      "Iteration: 9150 loss: 0.0000000000 time: 0.031760215759277344\n",
      "Iteration: 9160 loss: 0.0000000000 time: 0.037844181060791016\n",
      "Iteration: 9170 loss: 0.0000000000 time: 0.03617739677429199\n",
      "Iteration: 9180 loss: 0.0000000000 time: 0.038023948669433594\n",
      "Iteration: 9190 loss: 0.0000000000 time: 0.037061452865600586\n",
      "Iteration: 9200 loss: 0.0000000000 time: 0.037474870681762695\n",
      "Iteration: 9210 loss: 0.0000000000 time: 0.0360414981842041\n",
      "Iteration: 9220 loss: 0.0000000000 time: 0.03104400634765625\n",
      "Iteration: 9230 loss: 0.0000000000 time: 0.03584933280944824\n",
      "Iteration: 9240 loss: 0.0000000000 time: 0.03243255615234375\n",
      "Iteration: 9250 loss: 0.0000000000 time: 0.02953171730041504\n",
      "Iteration: 9260 loss: 0.0000000000 time: 0.032326459884643555\n",
      "Iteration: 9270 loss: 0.0000000000 time: 0.035188913345336914\n",
      "Iteration: 9280 loss: 0.0000000000 time: 0.03339028358459473\n",
      "Iteration: 9290 loss: 0.0000000000 time: 0.03334498405456543\n",
      "Iteration: 9300 loss: 0.0000000000 time: 0.03361654281616211\n",
      "Iteration: 9310 loss: 0.0000000000 time: 0.03465580940246582\n",
      "Iteration: 9320 loss: 0.0000000000 time: 0.04154801368713379\n",
      "Iteration: 9330 loss: 0.0000000000 time: 0.035059213638305664\n",
      "Iteration: 9340 loss: 0.0000000000 time: 0.03388381004333496\n",
      "Iteration: 9350 loss: 0.0000000000 time: 0.03402590751647949\n",
      "Iteration: 9360 loss: 0.0000000000 time: 0.037650346755981445\n",
      "Iteration: 9370 loss: 0.0000000000 time: 0.0358273983001709\n",
      "Iteration: 9380 loss: 0.0000000000 time: 0.03967881202697754\n",
      "Iteration: 9390 loss: 0.0000000000 time: 0.036626338958740234\n",
      "Iteration: 9400 loss: 0.0000000000 time: 0.03565263748168945\n",
      "Iteration: 9410 loss: 0.0000000000 time: 0.03022909164428711\n",
      "Iteration: 9420 loss: 0.0000000000 time: 0.03759455680847168\n",
      "Iteration: 9430 loss: 0.0000000000 time: 0.03586983680725098\n",
      "Iteration: 9440 loss: 0.0000000000 time: 0.041198015213012695\n",
      "Iteration: 9450 loss: 0.0000000000 time: 0.03743577003479004\n",
      "Iteration: 9460 loss: 0.0000000000 time: 0.0383450984954834\n",
      "Iteration: 9470 loss: 0.0000000000 time: 0.03435945510864258\n",
      "Iteration: 9480 loss: 0.0000000000 time: 0.03334784507751465\n",
      "Iteration: 9490 loss: 0.0000000000 time: 0.03618431091308594\n",
      "Iteration: 9500 loss: 0.0000000000 time: 0.0359344482421875\n",
      "Iteration: 9510 loss: 0.0000000000 time: 0.035303354263305664\n",
      "Iteration: 9520 loss: 0.0000000000 time: 0.03421211242675781\n",
      "Iteration: 9530 loss: 0.0000000000 time: 0.03296160697937012\n",
      "Iteration: 9540 loss: 0.0000000000 time: 0.03441262245178223\n",
      "Iteration: 9550 loss: 0.0000000000 time: 0.033936500549316406\n",
      "Iteration: 9560 loss: 0.0000000000 time: 0.03754615783691406\n",
      "Iteration: 9570 loss: 0.0000000000 time: 0.033052682876586914\n",
      "Iteration: 9580 loss: 0.0000000000 time: 0.030392885208129883\n",
      "Iteration: 9590 loss: 0.0000000000 time: 0.03235435485839844\n",
      "Iteration: 9600 loss: 0.0000000000 time: 0.029656410217285156\n",
      "Iteration: 9610 loss: 0.0000000000 time: 0.029781579971313477\n",
      "Iteration: 9620 loss: 0.0000000000 time: 0.03539752960205078\n",
      "Iteration: 9630 loss: 0.0000000000 time: 0.03691577911376953\n",
      "Iteration: 9640 loss: 0.0000000000 time: 0.02988719940185547\n",
      "Iteration: 9650 loss: 0.0000000000 time: 0.030283451080322266\n",
      "Iteration: 9660 loss: 0.0000000000 time: 0.0330660343170166\n",
      "Iteration: 9670 loss: 0.0000000000 time: 0.03217267990112305\n",
      "Iteration: 9680 loss: 0.0000000000 time: 0.03325223922729492\n",
      "Iteration: 9690 loss: 0.0000000000 time: 0.035230398178100586\n",
      "Iteration: 9700 loss: 0.0000000000 time: 0.033425092697143555\n",
      "Iteration: 9710 loss: 0.0000000000 time: 0.0322873592376709\n",
      "Iteration: 9720 loss: 0.0000000000 time: 0.0319366455078125\n",
      "Iteration: 9730 loss: 0.0000000000 time: 0.031025409698486328\n",
      "Iteration: 9740 loss: 0.0000000000 time: 0.03408360481262207\n",
      "Iteration: 9750 loss: 0.0000000000 time: 0.03356361389160156\n",
      "Iteration: 9760 loss: 0.0000000000 time: 0.033470869064331055\n",
      "Iteration: 9770 loss: 0.0000000000 time: 0.04373311996459961\n",
      "Iteration: 9780 loss: 0.0000000000 time: 0.04169487953186035\n",
      "Iteration: 9790 loss: 0.0000000000 time: 0.030605316162109375\n",
      "Iteration: 9800 loss: 0.0000000000 time: 0.031249046325683594\n",
      "Iteration: 9810 loss: 0.0000000000 time: 0.031062602996826172\n",
      "Iteration: 9820 loss: 0.0000000000 time: 0.029544830322265625\n",
      "Iteration: 9830 loss: 0.0000000000 time: 0.03261899948120117\n",
      "Iteration: 9840 loss: 0.0000000000 time: 0.039777517318725586\n",
      "Iteration: 9850 loss: 0.0000000000 time: 0.03378653526306152\n",
      "Iteration: 9860 loss: 0.0000000000 time: 0.03403520584106445\n",
      "Iteration: 9870 loss: 0.0000000000 time: 0.03305172920227051\n",
      "Iteration: 9880 loss: 0.0000000000 time: 0.03414654731750488\n",
      "Iteration: 9890 loss: 0.0000000000 time: 0.037077903747558594\n",
      "Iteration: 9900 loss: 0.0000000000 time: 0.0359499454498291\n",
      "Iteration: 9910 loss: 0.0000000000 time: 0.029907703399658203\n",
      "Iteration: 9920 loss: 0.0000000000 time: 0.030070066452026367\n",
      "Iteration: 9930 loss: 0.0000000000 time: 0.03535604476928711\n",
      "Iteration: 9940 loss: 0.0000000000 time: 0.03768444061279297\n",
      "Iteration: 9950 loss: 0.0000000000 time: 0.04292607307434082\n",
      "Iteration: 9960 loss: 0.0000000000 time: 0.03466939926147461\n",
      "Iteration: 9970 loss: 0.0000000000 time: 0.03479790687561035\n",
      "Iteration: 9980 loss: 0.0000000000 time: 0.0368497371673584\n",
      "Iteration: 9990 loss: 0.0000000000 time: 0.03763079643249512\n",
      "Iteration: 10000 loss: 0.0000000000 time: 0.03910541534423828\n",
      "Iteration: 10010 loss: 0.0000000000 time: 0.03806018829345703\n",
      "Iteration: 10020 loss: 0.0000000000 time: 0.0381016731262207\n",
      "Iteration: 10030 loss: 0.0000000000 time: 0.04124951362609863\n",
      "Iteration: 10040 loss: 0.0000000000 time: 0.03731536865234375\n",
      "Iteration: 10050 loss: 0.0000000000 time: 0.036211252212524414\n",
      "Iteration: 10060 loss: 0.0000000000 time: 0.03779172897338867\n",
      "Iteration: 10070 loss: 0.0000000000 time: 0.03787970542907715\n",
      "Iteration: 10080 loss: 0.0000000000 time: 0.03532266616821289\n",
      "Iteration: 10090 loss: 0.0000000000 time: 0.041875600814819336\n",
      "Iteration: 10100 loss: 0.0000000000 time: 0.031342506408691406\n",
      "Iteration: 10110 loss: 0.0000000000 time: 0.03357553482055664\n",
      "Iteration: 10120 loss: 0.0000000000 time: 0.03799319267272949\n",
      "Iteration: 10130 loss: 0.0000000000 time: 0.03936314582824707\n",
      "Iteration: 10140 loss: 0.0000000000 time: 0.036273956298828125\n",
      "Iteration: 10150 loss: 0.0000000000 time: 0.0364682674407959\n",
      "Iteration: 10160 loss: 0.0000000000 time: 0.03673744201660156\n",
      "Iteration: 10170 loss: 0.0000000000 time: 0.03517794609069824\n",
      "Iteration: 10180 loss: 0.0000000000 time: 0.039008378982543945\n",
      "Iteration: 10190 loss: 0.0000000000 time: 0.039424896240234375\n",
      "Iteration: 10200 loss: 0.0000000000 time: 0.03809833526611328\n",
      "Iteration: 10210 loss: 0.0000000000 time: 0.03630709648132324\n",
      "Iteration: 10220 loss: 0.0000000000 time: 0.03777956962585449\n",
      "Iteration: 10230 loss: 0.0000000000 time: 0.034812211990356445\n",
      "Iteration: 10240 loss: 0.0000000000 time: 0.03761601448059082\n",
      "Iteration: 10250 loss: 0.0000000000 time: 0.03641200065612793\n",
      "Iteration: 10260 loss: 0.0000000000 time: 0.030170917510986328\n",
      "Iteration: 10270 loss: 0.0000000000 time: 0.03299689292907715\n",
      "Iteration: 10280 loss: 0.0000000000 time: 0.030883073806762695\n",
      "Iteration: 10290 loss: 0.0000000000 time: 0.030269145965576172\n",
      "Iteration: 10300 loss: 0.0000000000 time: 0.03492403030395508\n",
      "Iteration: 10310 loss: 0.0000000000 time: 0.031937599182128906\n",
      "Iteration: 10320 loss: 0.0000000000 time: 0.03415060043334961\n",
      "Iteration: 10330 loss: 0.0000000000 time: 0.03022909164428711\n",
      "Iteration: 10340 loss: 0.0000000000 time: 0.03313422203063965\n",
      "Iteration: 10350 loss: 0.0000000000 time: 0.03176307678222656\n",
      "Iteration: 10360 loss: 0.0000000000 time: 0.031044960021972656\n",
      "Iteration: 10370 loss: 0.0000000000 time: 0.037108421325683594\n",
      "Iteration: 10380 loss: 0.0000000000 time: 0.03368377685546875\n",
      "Iteration: 10390 loss: 0.0000000000 time: 0.03543400764465332\n",
      "Iteration: 10400 loss: 0.0000000000 time: 0.029386043548583984\n",
      "Iteration: 10410 loss: 0.0000000000 time: 0.035218000411987305\n",
      "Iteration: 10420 loss: 0.0000000000 time: 0.03449726104736328\n",
      "Iteration: 10430 loss: 0.0000000000 time: 0.035531044006347656\n",
      "Iteration: 10440 loss: 0.0000000000 time: 0.03565263748168945\n",
      "Iteration: 10450 loss: 0.0000000000 time: 0.03811192512512207\n",
      "Iteration: 10460 loss: 0.0000000000 time: 0.03617668151855469\n",
      "Iteration: 10470 loss: 0.0000000000 time: 0.029968738555908203\n",
      "Iteration: 10480 loss: 0.0000000000 time: 0.030405044555664062\n",
      "Iteration: 10490 loss: 0.0000000000 time: 0.03358936309814453\n",
      "Iteration: 10500 loss: 0.0000000000 time: 0.032713890075683594\n",
      "Iteration: 10510 loss: 0.0000000000 time: 0.03578519821166992\n",
      "Iteration: 10520 loss: 0.0000000000 time: 0.033772945404052734\n",
      "Iteration: 10530 loss: 0.0000000000 time: 0.0353548526763916\n",
      "Iteration: 10540 loss: 0.0000000000 time: 0.029672622680664062\n",
      "Iteration: 10550 loss: 0.0000000000 time: 0.02929091453552246\n",
      "Iteration: 10560 loss: 0.0000000000 time: 0.036118268966674805\n",
      "Iteration: 10570 loss: 0.0000000000 time: 0.03881478309631348\n",
      "Iteration: 10580 loss: 0.0000000000 time: 0.03456473350524902\n",
      "Iteration: 10590 loss: 0.0000000000 time: 0.030150651931762695\n",
      "Iteration: 10600 loss: 0.0000000000 time: 0.030353307723999023\n",
      "Iteration: 10610 loss: 0.0000000000 time: 0.03502058982849121\n",
      "Iteration: 10620 loss: 0.0000000000 time: 0.03344416618347168\n",
      "Iteration: 10630 loss: 0.0000000000 time: 0.03678107261657715\n",
      "Iteration: 10640 loss: 0.0000000000 time: 0.03972315788269043\n",
      "Iteration: 10650 loss: 0.0000000000 time: 0.036423444747924805\n",
      "Iteration: 10660 loss: 0.0000000000 time: 0.03061389923095703\n",
      "Iteration: 10670 loss: 0.0000000000 time: 0.029630422592163086\n",
      "Iteration: 10680 loss: 0.0000000000 time: 0.0351715087890625\n",
      "Iteration: 10690 loss: 0.0000000000 time: 0.036739349365234375\n",
      "Iteration: 10700 loss: 0.0000000000 time: 0.03577899932861328\n",
      "Iteration: 10710 loss: 0.0000000000 time: 0.037937164306640625\n",
      "Iteration: 10720 loss: 0.0000000000 time: 0.03823661804199219\n",
      "Iteration: 10730 loss: 0.0000000000 time: 0.03892040252685547\n",
      "Iteration: 10740 loss: 0.0000000000 time: 0.03791165351867676\n",
      "Iteration: 10750 loss: 0.0000000000 time: 0.029878854751586914\n",
      "Iteration: 10760 loss: 0.0000000000 time: 0.03035116195678711\n",
      "Iteration: 10770 loss: 0.0000000000 time: 0.03590536117553711\n",
      "Iteration: 10780 loss: 0.0000000000 time: 0.034261226654052734\n",
      "Iteration: 10790 loss: 0.0000000000 time: 0.03374290466308594\n",
      "Iteration: 10800 loss: 0.0000000000 time: 0.03326702117919922\n",
      "Iteration: 10810 loss: 0.0000000000 time: 0.03449583053588867\n",
      "Iteration: 10820 loss: 0.0000000000 time: 0.030582189559936523\n",
      "Iteration: 10830 loss: 0.0000000000 time: 0.03269791603088379\n",
      "Iteration: 10840 loss: 0.0000000000 time: 0.030745267868041992\n",
      "Iteration: 10850 loss: 0.0000000000 time: 0.029851198196411133\n",
      "Iteration: 10860 loss: 0.0000000000 time: 0.03622794151306152\n",
      "Iteration: 10870 loss: 0.0000000000 time: 0.03185677528381348\n",
      "Iteration: 10880 loss: 0.0000000000 time: 0.0329585075378418\n",
      "Iteration: 10890 loss: 0.0000000000 time: 0.03848624229431152\n",
      "Iteration: 10900 loss: 0.0000000000 time: 0.03801417350769043\n",
      "Iteration: 10910 loss: 0.0000000000 time: 0.034729719161987305\n",
      "Iteration: 10920 loss: 0.0000000000 time: 0.03580760955810547\n",
      "Iteration: 10930 loss: 0.0000000000 time: 0.03744101524353027\n",
      "Iteration: 10940 loss: 0.0000000000 time: 0.031281471252441406\n",
      "Iteration: 10950 loss: 0.0000000000 time: 0.03176426887512207\n",
      "Iteration: 10960 loss: 0.0000000000 time: 0.033826589584350586\n",
      "Iteration: 10970 loss: 0.0000000000 time: 0.0316622257232666\n",
      "Iteration: 10980 loss: 0.0000000000 time: 0.0307466983795166\n",
      "Iteration: 10990 loss: 0.0000000000 time: 0.03152894973754883\n",
      "Iteration: 11000 loss: 0.0000000000 time: 0.03609013557434082\n",
      "Iteration: 11010 loss: 0.0000000000 time: 0.03688669204711914\n",
      "Iteration: 11020 loss: 0.0000000000 time: 0.037806034088134766\n",
      "Iteration: 11030 loss: 0.0000000000 time: 0.03044605255126953\n",
      "Iteration: 11040 loss: 0.0000000000 time: 0.03122544288635254\n",
      "Iteration: 11050 loss: 0.0000000000 time: 0.03475022315979004\n",
      "Iteration: 11060 loss: 0.0000000000 time: 0.04057884216308594\n",
      "Iteration: 11070 loss: 0.0000000000 time: 0.03415083885192871\n",
      "Iteration: 11080 loss: 0.0000000000 time: 0.029416561126708984\n",
      "Iteration: 11090 loss: 0.0000000000 time: 0.03582620620727539\n",
      "Iteration: 11100 loss: 0.0000000000 time: 0.03474068641662598\n",
      "Iteration: 11110 loss: 0.0000000000 time: 0.03414106369018555\n",
      "Iteration: 11120 loss: 0.0000000000 time: 0.029996633529663086\n",
      "Iteration: 11130 loss: 0.0000000000 time: 0.029994964599609375\n",
      "Iteration: 11140 loss: 0.0000000000 time: 0.03033161163330078\n",
      "Iteration: 11150 loss: 0.0000000000 time: 0.03223872184753418\n",
      "Iteration: 11160 loss: 0.0000000000 time: 0.0340723991394043\n",
      "Iteration: 11170 loss: 0.0000000000 time: 0.03185462951660156\n",
      "Iteration: 11180 loss: 0.0000000000 time: 0.0343017578125\n",
      "Iteration: 11190 loss: 0.0000000000 time: 0.037554025650024414\n",
      "Iteration: 11200 loss: 0.0000000000 time: 0.03880953788757324\n",
      "Iteration: 11210 loss: 0.0000000000 time: 0.03593873977661133\n",
      "Iteration: 11220 loss: 0.0000000000 time: 0.036121368408203125\n",
      "Iteration: 11230 loss: 0.0000000000 time: 0.04062080383300781\n",
      "Iteration: 11240 loss: 0.0000000000 time: 0.03926730155944824\n",
      "Iteration: 11250 loss: 0.0000000000 time: 0.03770136833190918\n",
      "Iteration: 11260 loss: 0.0000000000 time: 0.03226637840270996\n",
      "Iteration: 11270 loss: 0.0000000000 time: 0.037249088287353516\n",
      "Iteration: 11280 loss: 0.0000000000 time: 0.03677988052368164\n",
      "Iteration: 11290 loss: 0.0000000000 time: 0.031447410583496094\n",
      "Iteration: 11300 loss: 0.0000000000 time: 0.02974843978881836\n",
      "Iteration: 11310 loss: 0.0000000000 time: 0.030211210250854492\n",
      "Iteration: 11320 loss: 0.0000000000 time: 0.02914261817932129\n",
      "Iteration: 11330 loss: 0.0000000000 time: 0.030123233795166016\n",
      "Iteration: 11340 loss: 0.0000000000 time: 0.0291750431060791\n",
      "Iteration: 11350 loss: 0.0000000000 time: 0.029653072357177734\n",
      "Iteration: 11360 loss: 0.0000000000 time: 0.03164482116699219\n",
      "Iteration: 11370 loss: 0.0000000000 time: 0.03913307189941406\n",
      "Iteration: 11380 loss: 0.0000000000 time: 0.03963327407836914\n",
      "Iteration: 11390 loss: 0.0000000000 time: 0.039160966873168945\n",
      "Iteration: 11400 loss: 0.0000000000 time: 0.038500308990478516\n",
      "Iteration: 11410 loss: 0.0000000000 time: 0.03645181655883789\n",
      "Iteration: 11420 loss: 0.0000000000 time: 0.03362846374511719\n",
      "Iteration: 11430 loss: 0.0000000000 time: 0.032975196838378906\n",
      "Iteration: 11440 loss: 0.0000000000 time: 0.03231048583984375\n",
      "Iteration: 11450 loss: 0.0000000000 time: 0.03551673889160156\n",
      "Iteration: 11460 loss: 0.0000000000 time: 0.037332773208618164\n",
      "Iteration: 11470 loss: 0.0000000000 time: 0.03664398193359375\n",
      "Iteration: 11480 loss: 0.0000000000 time: 0.036511898040771484\n",
      "Iteration: 11490 loss: 0.0000000000 time: 0.032063961029052734\n",
      "Iteration: 11500 loss: 0.0000000000 time: 0.030826568603515625\n",
      "Iteration: 11510 loss: 0.0000000000 time: 0.03674602508544922\n",
      "Iteration: 11520 loss: 0.0000000000 time: 0.032202720642089844\n",
      "Iteration: 11530 loss: 0.0000000000 time: 0.034926652908325195\n",
      "Iteration: 11540 loss: 0.0000000000 time: 0.03472638130187988\n",
      "Iteration: 11550 loss: 0.0000000000 time: 0.03655600547790527\n",
      "Iteration: 11560 loss: 0.0000000000 time: 0.035066843032836914\n",
      "Iteration: 11570 loss: 0.0000000000 time: 0.0357358455657959\n",
      "Iteration: 11580 loss: 0.0000000000 time: 0.03655648231506348\n",
      "Iteration: 11590 loss: 0.0000000000 time: 0.030717134475708008\n",
      "Iteration: 11600 loss: 0.0000000000 time: 0.03480219841003418\n",
      "Iteration: 11610 loss: 0.0000000000 time: 0.030977487564086914\n",
      "Iteration: 11620 loss: 0.0000000000 time: 0.03270602226257324\n",
      "Iteration: 11630 loss: 0.0000000000 time: 0.03180336952209473\n",
      "Iteration: 11640 loss: 0.0000000000 time: 0.0322117805480957\n",
      "Iteration: 11650 loss: 0.0000000000 time: 0.03332042694091797\n",
      "Iteration: 11660 loss: 0.0000000000 time: 0.03289198875427246\n",
      "Iteration: 11670 loss: 0.0000000000 time: 0.034493446350097656\n",
      "Iteration: 11680 loss: 0.0000000000 time: 0.029926538467407227\n",
      "Iteration: 11690 loss: 0.0000000000 time: 0.03214097023010254\n",
      "Iteration: 11700 loss: 0.0000000000 time: 0.03305244445800781\n",
      "Iteration: 11710 loss: 0.0000000000 time: 0.028760194778442383\n",
      "Iteration: 11720 loss: 0.0000000000 time: 0.032038211822509766\n",
      "Iteration: 11730 loss: 0.0000000000 time: 0.03234720230102539\n",
      "Iteration: 11740 loss: 0.0000000000 time: 0.03516221046447754\n",
      "Iteration: 11750 loss: 0.0000000000 time: 0.03359389305114746\n",
      "Iteration: 11760 loss: 0.0000000000 time: 0.03191852569580078\n",
      "Iteration: 11770 loss: 0.0000000000 time: 0.030279874801635742\n",
      "Iteration: 11780 loss: 0.0000000000 time: 0.03143811225891113\n",
      "Iteration: 11790 loss: 0.0000000000 time: 0.03274869918823242\n",
      "Iteration: 11800 loss: 0.0000000000 time: 0.03616619110107422\n",
      "Iteration: 11810 loss: 0.0000000000 time: 0.0314786434173584\n",
      "Iteration: 11820 loss: 0.0000000000 time: 0.029920101165771484\n",
      "Iteration: 11830 loss: 0.0000000000 time: 0.02950882911682129\n",
      "Iteration: 11840 loss: 0.0000000000 time: 0.0289304256439209\n",
      "Iteration: 11850 loss: 0.0000000000 time: 0.03374624252319336\n",
      "Iteration: 11860 loss: 0.0000000000 time: 0.03612995147705078\n",
      "Iteration: 11870 loss: 0.0000000000 time: 0.03652334213256836\n",
      "Iteration: 11880 loss: 0.0000000000 time: 0.03904366493225098\n",
      "Iteration: 11890 loss: 0.0000000000 time: 0.03979158401489258\n",
      "Iteration: 11900 loss: 0.0000000000 time: 0.03635263442993164\n",
      "Iteration: 11910 loss: 0.0000000000 time: 0.03558063507080078\n",
      "Iteration: 11920 loss: 0.0000000000 time: 0.04128694534301758\n",
      "Iteration: 11930 loss: 0.0000000000 time: 0.03191995620727539\n",
      "Iteration: 11940 loss: 0.0000000000 time: 0.034903764724731445\n",
      "Iteration: 11950 loss: 0.0000000000 time: 0.03661942481994629\n",
      "Iteration: 11960 loss: 0.0000000000 time: 0.02973008155822754\n",
      "Iteration: 11970 loss: 0.0000000000 time: 0.033562660217285156\n",
      "Iteration: 11980 loss: 0.0000000000 time: 0.03035283088684082\n",
      "Iteration: 11990 loss: 0.0000000000 time: 0.032392263412475586\n",
      "Iteration: 12000 loss: 0.0000000000 time: 0.03726387023925781\n",
      "Iteration: 12010 loss: 0.0000000000 time: 0.035121917724609375\n",
      "Iteration: 12020 loss: 0.0000000000 time: 0.035668134689331055\n",
      "Iteration: 12030 loss: 0.0000000000 time: 0.03902554512023926\n",
      "Iteration: 12040 loss: 0.0000000000 time: 0.033214569091796875\n",
      "Iteration: 12050 loss: 0.0000000000 time: 0.03292083740234375\n",
      "Iteration: 12060 loss: 0.0000000000 time: 0.03727579116821289\n",
      "Iteration: 12070 loss: 0.0000000000 time: 0.033406972885131836\n",
      "Iteration: 12080 loss: 0.0000000000 time: 0.030690908432006836\n",
      "Iteration: 12090 loss: 0.0000000000 time: 0.03628969192504883\n",
      "Iteration: 12100 loss: 0.0000000000 time: 0.035105228424072266\n",
      "Iteration: 12110 loss: 0.0000000000 time: 0.03438401222229004\n",
      "Iteration: 12120 loss: 0.0000000000 time: 0.03602743148803711\n",
      "Iteration: 12130 loss: 0.0000000000 time: 0.036789655685424805\n",
      "Iteration: 12140 loss: 0.0000000000 time: 0.02989959716796875\n",
      "Iteration: 12150 loss: 0.0000000000 time: 0.029746055603027344\n",
      "Iteration: 12160 loss: 0.0000000000 time: 0.03381037712097168\n",
      "Iteration: 12170 loss: 0.0000000000 time: 0.032196044921875\n",
      "Iteration: 12180 loss: 0.0000000000 time: 0.03110361099243164\n",
      "Iteration: 12190 loss: 0.0000000000 time: 0.043287038803100586\n",
      "Iteration: 12200 loss: 0.0000000000 time: 0.03477120399475098\n",
      "Iteration: 12210 loss: 0.0000000000 time: 0.03423643112182617\n",
      "Iteration: 12220 loss: 0.0000000000 time: 0.034258365631103516\n",
      "Iteration: 12230 loss: 0.0000000000 time: 0.03485703468322754\n",
      "Iteration: 12240 loss: 0.0000000000 time: 0.03431081771850586\n",
      "Iteration: 12250 loss: 0.0000000000 time: 0.03536653518676758\n",
      "Iteration: 12260 loss: 0.0000000000 time: 0.03998279571533203\n",
      "Iteration: 12270 loss: 0.0000000000 time: 0.034815073013305664\n",
      "Iteration: 12280 loss: 0.0000000000 time: 0.03574490547180176\n",
      "Iteration: 12290 loss: 0.0000000000 time: 0.03411436080932617\n",
      "Iteration: 12300 loss: 0.0000000000 time: 0.03481340408325195\n",
      "Iteration: 12310 loss: 0.0000000000 time: 0.034780025482177734\n",
      "Iteration: 12320 loss: 0.0000000000 time: 0.03795814514160156\n",
      "Iteration: 12330 loss: 0.0000000000 time: 0.035378217697143555\n",
      "Iteration: 12340 loss: 0.0000000000 time: 0.03867030143737793\n",
      "Iteration: 12350 loss: 0.0000000000 time: 0.03371024131774902\n",
      "Iteration: 12360 loss: 0.0000000000 time: 0.03407096862792969\n",
      "Iteration: 12370 loss: 0.0000000000 time: 0.036499977111816406\n",
      "Iteration: 12380 loss: 0.0000000000 time: 0.037067413330078125\n",
      "Iteration: 12390 loss: 0.0000000000 time: 0.03465771675109863\n",
      "Iteration: 12400 loss: 0.0000000000 time: 0.03422212600708008\n",
      "Iteration: 12410 loss: 0.0000000000 time: 0.034369707107543945\n",
      "Iteration: 12420 loss: 0.0000000000 time: 0.030930519104003906\n",
      "Iteration: 12430 loss: 0.0000000000 time: 0.030008554458618164\n",
      "Iteration: 12440 loss: 0.0000000000 time: 0.03769683837890625\n",
      "Iteration: 12450 loss: 0.0000000000 time: 0.03407788276672363\n",
      "Iteration: 12460 loss: 0.0000000000 time: 0.033698320388793945\n",
      "Iteration: 12470 loss: 0.0000000000 time: 0.034436941146850586\n",
      "Iteration: 12480 loss: 0.0000000000 time: 0.03413653373718262\n",
      "Iteration: 12490 loss: 0.0000000000 time: 0.03407120704650879\n",
      "Iteration: 12500 loss: 0.0000000000 time: 0.04010581970214844\n",
      "Iteration: 12510 loss: 0.0000000000 time: 0.03888821601867676\n",
      "Iteration: 12520 loss: 0.0000000000 time: 0.03544759750366211\n",
      "Iteration: 12530 loss: 0.0000000000 time: 0.03629612922668457\n",
      "Iteration: 12540 loss: 0.0000000000 time: 0.04067564010620117\n",
      "Iteration: 12550 loss: 0.0000000000 time: 0.03525996208190918\n",
      "Iteration: 12560 loss: 0.0000000000 time: 0.0325162410736084\n",
      "Iteration: 12570 loss: 0.0000000000 time: 0.038693904876708984\n",
      "Iteration: 12580 loss: 0.0000000000 time: 0.03445291519165039\n",
      "Iteration: 12590 loss: 0.0000000000 time: 0.03641700744628906\n",
      "Iteration: 12600 loss: 0.0000000000 time: 0.03406262397766113\n",
      "Iteration: 12610 loss: 0.0000000000 time: 0.0330963134765625\n",
      "Iteration: 12620 loss: 0.0000000000 time: 0.03370213508605957\n",
      "Iteration: 12630 loss: 0.0000537096 time: 0.03672647476196289\n",
      "Iteration: 12640 loss: 0.0000291646 time: 0.038450002670288086\n",
      "Iteration: 12650 loss: 0.0000010756 time: 0.040654897689819336\n",
      "Iteration: 12660 loss: 0.0000008758 time: 0.03248333930969238\n",
      "Iteration: 12670 loss: 0.0000011361 time: 0.03106856346130371\n",
      "Iteration: 12680 loss: 0.0000003143 time: 0.03208780288696289\n",
      "Iteration: 12690 loss: 0.0000000071 time: 0.04552912712097168\n",
      "Iteration: 12700 loss: 0.0000000181 time: 0.03822898864746094\n",
      "Iteration: 12710 loss: 0.0000000182 time: 0.03782153129577637\n",
      "Iteration: 12720 loss: 0.0000000026 time: 0.038422346115112305\n",
      "Iteration: 12730 loss: 0.0000000002 time: 0.038317203521728516\n",
      "Iteration: 12740 loss: 0.0000000007 time: 0.0319666862487793\n",
      "Iteration: 12750 loss: 0.0000000001 time: 0.03216075897216797\n",
      "Iteration: 12760 loss: 0.0000000000 time: 0.03396129608154297\n",
      "Iteration: 12770 loss: 0.0000000000 time: 0.03591156005859375\n",
      "Iteration: 12780 loss: 0.0000000000 time: 0.04294109344482422\n",
      "Iteration: 12790 loss: 0.0000000000 time: 0.037917137145996094\n",
      "Iteration: 12800 loss: 0.0000000000 time: 0.045783042907714844\n",
      "Iteration: 12810 loss: 0.0000000000 time: 0.03808021545410156\n",
      "Iteration: 12820 loss: 0.0000000000 time: 0.03706526756286621\n",
      "Iteration: 12830 loss: 0.0000000000 time: 0.03541088104248047\n",
      "Iteration: 12840 loss: 0.0000000000 time: 0.036889076232910156\n",
      "Iteration: 12850 loss: 0.0000000000 time: 0.03579878807067871\n",
      "Iteration: 12860 loss: 0.0000000000 time: 0.038159847259521484\n",
      "Iteration: 12870 loss: 0.0000000000 time: 0.03782963752746582\n",
      "Iteration: 12880 loss: 0.0000000000 time: 0.03588747978210449\n",
      "Iteration: 12890 loss: 0.0000000000 time: 0.0355532169342041\n",
      "Iteration: 12900 loss: 0.0000000000 time: 0.03744959831237793\n",
      "Iteration: 12910 loss: 0.0000000000 time: 0.03942990303039551\n",
      "Iteration: 12920 loss: 0.0000000000 time: 0.04037880897521973\n",
      "Iteration: 12930 loss: 0.0000000000 time: 0.04006385803222656\n",
      "Iteration: 12940 loss: 0.0000000000 time: 0.03998517990112305\n",
      "Iteration: 12950 loss: 0.0000000000 time: 0.037653446197509766\n",
      "Iteration: 12960 loss: 0.0000000000 time: 0.03841567039489746\n",
      "Iteration: 12970 loss: 0.0000000000 time: 0.037137746810913086\n",
      "Iteration: 12980 loss: 0.0000000000 time: 0.0403292179107666\n",
      "Iteration: 12990 loss: 0.0000000000 time: 0.040753841400146484\n",
      "Iteration: 13000 loss: 0.0000000000 time: 0.03917837142944336\n",
      "Iteration: 13010 loss: 0.0000000000 time: 0.037748098373413086\n",
      "Iteration: 13020 loss: 0.0000000000 time: 0.03968000411987305\n",
      "Iteration: 13030 loss: 0.0000000000 time: 0.03806900978088379\n",
      "Iteration: 13040 loss: 0.0000000000 time: 0.035994768142700195\n",
      "Iteration: 13050 loss: 0.0000000000 time: 0.03284287452697754\n",
      "Iteration: 13060 loss: 0.0000000000 time: 0.030411958694458008\n",
      "Iteration: 13070 loss: 0.0000000000 time: 0.030382156372070312\n",
      "Iteration: 13080 loss: 0.0000000000 time: 0.03614687919616699\n",
      "Iteration: 13090 loss: 0.0000000000 time: 0.0359952449798584\n",
      "Iteration: 13100 loss: 0.0000000000 time: 0.03657078742980957\n",
      "Iteration: 13110 loss: 0.0000000000 time: 0.03542923927307129\n",
      "Iteration: 13120 loss: 0.0000000000 time: 0.03482222557067871\n",
      "Iteration: 13130 loss: 0.0000000000 time: 0.0385441780090332\n",
      "Iteration: 13140 loss: 0.0000000000 time: 0.03528714179992676\n",
      "Iteration: 13150 loss: 0.0000000000 time: 0.0354914665222168\n",
      "Iteration: 13160 loss: 0.0000000000 time: 0.03409838676452637\n",
      "Iteration: 13170 loss: 0.0000000000 time: 0.03794550895690918\n",
      "Iteration: 13180 loss: 0.0000000000 time: 0.03476405143737793\n",
      "Iteration: 13190 loss: 0.0000000000 time: 0.03428936004638672\n",
      "Iteration: 13200 loss: 0.0000000000 time: 0.03555440902709961\n",
      "Iteration: 13210 loss: 0.0000000000 time: 0.03711700439453125\n",
      "Iteration: 13220 loss: 0.0000000000 time: 0.038810014724731445\n",
      "Iteration: 13230 loss: 0.0000000000 time: 0.0407257080078125\n",
      "Iteration: 13240 loss: 0.0000000000 time: 0.04140186309814453\n",
      "Iteration: 13250 loss: 0.0000000000 time: 0.03453779220581055\n",
      "Iteration: 13260 loss: 0.0000000000 time: 0.034110069274902344\n",
      "Iteration: 13270 loss: 0.0000000000 time: 0.0328061580657959\n",
      "Iteration: 13280 loss: 0.0000000000 time: 0.03377819061279297\n",
      "Iteration: 13290 loss: 0.0000000000 time: 0.03578519821166992\n",
      "Iteration: 13300 loss: 0.0000000000 time: 0.03418922424316406\n",
      "Iteration: 13310 loss: 0.0000000000 time: 0.0352330207824707\n",
      "Iteration: 13320 loss: 0.0000000000 time: 0.03442692756652832\n",
      "Iteration: 13330 loss: 0.0000000000 time: 0.036362648010253906\n",
      "Iteration: 13340 loss: 0.0000000000 time: 0.03127574920654297\n",
      "Iteration: 13350 loss: 0.0000000000 time: 0.03462934494018555\n",
      "Iteration: 13360 loss: 0.0000000000 time: 0.037183284759521484\n",
      "Iteration: 13370 loss: 0.0000000000 time: 0.0302584171295166\n",
      "Iteration: 13380 loss: 0.0000000000 time: 0.030622005462646484\n",
      "Iteration: 13390 loss: 0.0000000000 time: 0.03368806838989258\n",
      "Iteration: 13400 loss: 0.0000000000 time: 0.03687763214111328\n",
      "Iteration: 13410 loss: 0.0000000000 time: 0.03970789909362793\n",
      "Iteration: 13420 loss: 0.0000000000 time: 0.039258480072021484\n",
      "Iteration: 13430 loss: 0.0000000000 time: 0.03651785850524902\n",
      "Iteration: 13440 loss: 0.0000000000 time: 0.03590536117553711\n",
      "Iteration: 13450 loss: 0.0000000000 time: 0.03738975524902344\n",
      "Iteration: 13460 loss: 0.0000000000 time: 0.03701639175415039\n",
      "Iteration: 13470 loss: 0.0000000000 time: 0.034372806549072266\n",
      "Iteration: 13480 loss: 0.0000000000 time: 0.02953028678894043\n",
      "Iteration: 13490 loss: 0.0000000000 time: 0.03506135940551758\n",
      "Iteration: 13500 loss: 0.0000000000 time: 0.03338146209716797\n",
      "Iteration: 13510 loss: 0.0000000000 time: 0.031207561492919922\n",
      "Iteration: 13520 loss: 0.0000000000 time: 0.03842759132385254\n",
      "Iteration: 13530 loss: 0.0000000000 time: 0.03417849540710449\n",
      "Iteration: 13540 loss: 0.0000000000 time: 0.03845643997192383\n",
      "Iteration: 13550 loss: 0.0000000000 time: 0.037895917892456055\n",
      "Iteration: 13560 loss: 0.0000000000 time: 0.02924656867980957\n",
      "Iteration: 13570 loss: 0.0000000000 time: 0.02997446060180664\n",
      "Iteration: 13580 loss: 0.0000000000 time: 0.03356504440307617\n",
      "Iteration: 13590 loss: 0.0000000000 time: 0.04177546501159668\n",
      "Iteration: 13600 loss: 0.0000000000 time: 0.0354764461517334\n",
      "Iteration: 13610 loss: 0.0000000000 time: 0.039034128189086914\n",
      "Iteration: 13620 loss: 0.0000000000 time: 0.036041975021362305\n",
      "Iteration: 13630 loss: 0.0000000000 time: 0.03810286521911621\n",
      "Iteration: 13640 loss: 0.0000000000 time: 0.039621591567993164\n",
      "Iteration: 13650 loss: 0.0000000000 time: 0.03946375846862793\n",
      "Iteration: 13660 loss: 0.0000000000 time: 0.03797197341918945\n",
      "Iteration: 13670 loss: 0.0000000000 time: 0.039641380310058594\n",
      "Iteration: 13680 loss: 0.0000000000 time: 0.038097381591796875\n",
      "Iteration: 13690 loss: 0.0000000000 time: 0.03902721405029297\n",
      "Iteration: 13700 loss: 0.0000000000 time: 0.038375139236450195\n",
      "Iteration: 13710 loss: 0.0000000000 time: 0.03869175910949707\n",
      "Iteration: 13720 loss: 0.0000000000 time: 0.038739919662475586\n",
      "Iteration: 13730 loss: 0.0000000000 time: 0.037301063537597656\n",
      "Iteration: 13740 loss: 0.0000000000 time: 0.04193592071533203\n",
      "Iteration: 13750 loss: 0.0000000000 time: 0.03752946853637695\n",
      "Iteration: 13760 loss: 0.0000000000 time: 0.03873252868652344\n",
      "Iteration: 13770 loss: 0.0000000000 time: 0.03765058517456055\n",
      "Iteration: 13780 loss: 0.0000000000 time: 0.03777813911437988\n",
      "Iteration: 13790 loss: 0.0000000000 time: 0.03597092628479004\n",
      "Iteration: 13800 loss: 0.0000000000 time: 0.03596949577331543\n",
      "Iteration: 13810 loss: 0.0000000000 time: 0.0375063419342041\n",
      "Iteration: 13820 loss: 0.0000000000 time: 0.035973310470581055\n",
      "Iteration: 13830 loss: 0.0000000000 time: 0.03706097602844238\n",
      "Iteration: 13840 loss: 0.0000000000 time: 0.038056135177612305\n",
      "Iteration: 13850 loss: 0.0000000000 time: 0.03512883186340332\n",
      "Iteration: 13860 loss: 0.0000000000 time: 0.03589177131652832\n",
      "Iteration: 13870 loss: 0.0000000000 time: 0.035466909408569336\n",
      "Iteration: 13880 loss: 0.0000000000 time: 0.035012006759643555\n",
      "Iteration: 13890 loss: 0.0000000000 time: 0.037911415100097656\n",
      "Iteration: 13900 loss: 0.0000000000 time: 0.03644418716430664\n",
      "Iteration: 13910 loss: 0.0000000000 time: 0.03464770317077637\n",
      "Iteration: 13920 loss: 0.0000000000 time: 0.035199642181396484\n",
      "Iteration: 13930 loss: 0.0000000000 time: 0.036164283752441406\n",
      "Iteration: 13940 loss: 0.0000000000 time: 0.03549981117248535\n",
      "Iteration: 13950 loss: 0.0000000000 time: 0.03350996971130371\n",
      "Iteration: 13960 loss: 0.0000000000 time: 0.039122819900512695\n",
      "Iteration: 13970 loss: 0.0000000000 time: 0.0400083065032959\n",
      "Iteration: 13980 loss: 0.0000000000 time: 0.03732705116271973\n",
      "Iteration: 13990 loss: 0.0000000000 time: 0.03863215446472168\n",
      "Iteration: 14000 loss: 0.0000000000 time: 0.032587528228759766\n",
      "Iteration: 14010 loss: 0.0000000000 time: 0.033344268798828125\n",
      "Iteration: 14020 loss: 0.0000000000 time: 0.031987905502319336\n",
      "Iteration: 14030 loss: 0.0000000000 time: 0.03278994560241699\n",
      "Iteration: 14040 loss: 0.0000000000 time: 0.030681848526000977\n",
      "Iteration: 14050 loss: 0.0000000000 time: 0.032341718673706055\n",
      "Iteration: 14060 loss: 0.0000000000 time: 0.030303001403808594\n",
      "Iteration: 14070 loss: 0.0000000000 time: 0.033263206481933594\n",
      "Iteration: 14080 loss: 0.0000000000 time: 0.03559994697570801\n",
      "Iteration: 14090 loss: 0.0000000000 time: 0.03131818771362305\n",
      "Iteration: 14100 loss: 0.0000000000 time: 0.03940939903259277\n",
      "Iteration: 14110 loss: 0.0000000000 time: 0.030089139938354492\n",
      "Iteration: 14120 loss: 0.0000000000 time: 0.029338359832763672\n",
      "Iteration: 14130 loss: 0.0000000000 time: 0.030977964401245117\n",
      "Iteration: 14140 loss: 0.0000000000 time: 0.03364062309265137\n",
      "Iteration: 14150 loss: 0.0000000000 time: 0.03601861000061035\n",
      "Iteration: 14160 loss: 0.0000000000 time: 0.03592085838317871\n",
      "Iteration: 14170 loss: 0.0000000000 time: 0.0345609188079834\n",
      "Iteration: 14180 loss: 0.0000000000 time: 0.031903743743896484\n",
      "Iteration: 14190 loss: 0.0000000000 time: 0.0316314697265625\n",
      "Iteration: 14200 loss: 0.0000000000 time: 0.03669095039367676\n",
      "Iteration: 14210 loss: 0.0000000000 time: 0.03684568405151367\n",
      "Iteration: 14220 loss: 0.0000000000 time: 0.03464937210083008\n",
      "Iteration: 14230 loss: 0.0000000000 time: 0.03398919105529785\n",
      "Iteration: 14240 loss: 0.0000000000 time: 0.03384828567504883\n",
      "Iteration: 14250 loss: 0.0000000000 time: 0.03323006629943848\n",
      "Iteration: 14260 loss: 0.0000000000 time: 0.03337717056274414\n",
      "Iteration: 14270 loss: 0.0000000000 time: 0.03809857368469238\n",
      "Iteration: 14280 loss: 0.0000000000 time: 0.032663583755493164\n",
      "Iteration: 14290 loss: 0.0000000000 time: 0.034894704818725586\n",
      "Iteration: 14300 loss: 0.0000000000 time: 0.03149223327636719\n",
      "Iteration: 14310 loss: 0.0000000000 time: 0.03118443489074707\n",
      "Iteration: 14320 loss: 0.0000000000 time: 0.035480499267578125\n",
      "Iteration: 14330 loss: 0.0000000000 time: 0.03388047218322754\n",
      "Iteration: 14340 loss: 0.0000000000 time: 0.033704280853271484\n",
      "Iteration: 14350 loss: 0.0000000000 time: 0.041913509368896484\n",
      "Iteration: 14360 loss: 0.0000000000 time: 0.03426766395568848\n",
      "Iteration: 14370 loss: 0.0000000000 time: 0.03036975860595703\n",
      "Iteration: 14380 loss: 0.0000000000 time: 0.02989029884338379\n",
      "Iteration: 14390 loss: 0.0000000000 time: 0.03853583335876465\n",
      "Iteration: 14400 loss: 0.0000000000 time: 0.031034231185913086\n",
      "Iteration: 14410 loss: 0.0000000000 time: 0.030729055404663086\n",
      "Iteration: 14420 loss: 0.0000000000 time: 0.03278684616088867\n",
      "Iteration: 14430 loss: 0.0000000000 time: 0.03515625\n",
      "Iteration: 14440 loss: 0.0000000000 time: 0.03818035125732422\n",
      "Iteration: 14450 loss: 0.0000000000 time: 0.04145050048828125\n",
      "Iteration: 14460 loss: 0.0000000000 time: 0.03905892372131348\n",
      "Iteration: 14470 loss: 0.0000000000 time: 0.03662872314453125\n",
      "Iteration: 14480 loss: 0.0000000000 time: 0.03720211982727051\n",
      "Iteration: 14490 loss: 0.0000000000 time: 0.03658771514892578\n",
      "Iteration: 14500 loss: 0.0000000000 time: 0.038588762283325195\n",
      "Iteration: 14510 loss: 0.0000000000 time: 0.0377349853515625\n",
      "Iteration: 14520 loss: 0.0000000000 time: 0.036573171615600586\n",
      "Iteration: 14530 loss: 0.0000000000 time: 0.030382871627807617\n",
      "Iteration: 14540 loss: 0.0000000000 time: 0.02959609031677246\n",
      "Iteration: 14550 loss: 0.0000000000 time: 0.03692746162414551\n",
      "Iteration: 14560 loss: 0.0000000000 time: 0.03683280944824219\n",
      "Iteration: 14570 loss: 0.0000000000 time: 0.03810238838195801\n",
      "Iteration: 14580 loss: 0.0000000000 time: 0.04032588005065918\n",
      "Iteration: 14590 loss: 0.0000000000 time: 0.037326812744140625\n",
      "Iteration: 14600 loss: 0.0000000000 time: 0.03566169738769531\n",
      "Iteration: 14610 loss: 0.0000000000 time: 0.032253265380859375\n",
      "Iteration: 14620 loss: 0.0000000000 time: 0.03415083885192871\n",
      "Iteration: 14630 loss: 0.0000000000 time: 0.03288149833679199\n",
      "Iteration: 14640 loss: 0.0000000000 time: 0.03620266914367676\n",
      "Iteration: 14650 loss: 0.0000000000 time: 0.03505730628967285\n",
      "Iteration: 14660 loss: 0.0000000000 time: 0.03738212585449219\n",
      "Iteration: 14670 loss: 0.0000000000 time: 0.03580617904663086\n",
      "Iteration: 14680 loss: 0.0000000000 time: 0.03588104248046875\n",
      "Iteration: 14690 loss: 0.0000000000 time: 0.03695988655090332\n",
      "Iteration: 14700 loss: 0.0000000497 time: 0.0347294807434082\n",
      "Iteration: 14710 loss: 0.0000021270 time: 0.03253531455993652\n",
      "Iteration: 14720 loss: 0.0000028744 time: 0.03626585006713867\n",
      "Iteration: 14730 loss: 0.0000008755 time: 0.03603720664978027\n",
      "Iteration: 14740 loss: 0.0000003099 time: 0.034811973571777344\n",
      "Iteration: 14750 loss: 0.0000001113 time: 0.0364840030670166\n",
      "Iteration: 14760 loss: 0.0000000386 time: 0.03359174728393555\n",
      "Iteration: 14770 loss: 0.0000000124 time: 0.036229610443115234\n",
      "Iteration: 14780 loss: 0.0000000034 time: 0.03807187080383301\n",
      "Iteration: 14790 loss: 0.0000000006 time: 0.03558659553527832\n",
      "Iteration: 14800 loss: 0.0000000000 time: 0.037251949310302734\n",
      "Iteration: 14810 loss: 0.0000000000 time: 0.037627458572387695\n",
      "Iteration: 14820 loss: 0.0000000001 time: 0.038170814514160156\n",
      "Iteration: 14830 loss: 0.0000000000 time: 0.03397679328918457\n",
      "Iteration: 14840 loss: 0.0000000000 time: 0.034339189529418945\n",
      "Iteration: 14850 loss: 0.0000000000 time: 0.034162282943725586\n",
      "Iteration: 14860 loss: 0.0000000000 time: 0.03349566459655762\n",
      "Iteration: 14870 loss: 0.0000000000 time: 0.03611493110656738\n",
      "Iteration: 14880 loss: 0.0000000000 time: 0.03500008583068848\n",
      "Iteration: 14890 loss: 0.0000000000 time: 0.036684274673461914\n",
      "Iteration: 14900 loss: 0.0000000000 time: 0.03502225875854492\n",
      "Iteration: 14910 loss: 0.0000000000 time: 0.03260350227355957\n",
      "Iteration: 14920 loss: 0.0000000000 time: 0.033344268798828125\n",
      "Iteration: 14930 loss: 0.0000000000 time: 0.03187060356140137\n",
      "Iteration: 14940 loss: 0.0000000000 time: 0.036379098892211914\n",
      "Iteration: 14950 loss: 0.0000000000 time: 0.03605842590332031\n",
      "Iteration: 14960 loss: 0.0000000000 time: 0.038246870040893555\n",
      "Iteration: 14970 loss: 0.0000000000 time: 0.03365159034729004\n",
      "Iteration: 14980 loss: 0.0000000000 time: 0.032560110092163086\n",
      "Iteration: 14990 loss: 0.0000000000 time: 0.03568887710571289\n",
      "Iteration: 15000 loss: 0.0000000000 time: 0.039099693298339844\n",
      "-->mesh : \n",
      "     n_triangles :  16\n",
      "     n_vertices  :  13\n",
      "     n_edges     :  28\n",
      "     h_max           :  0.5000000000013305\n",
      "     h_min           :  0.353553390592333\n",
      "-->test_fun      : \n",
      "     order       :  1\n",
      "     dof         :  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 11:44:15.153765: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:44:15.157930: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:44:15.160891: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:44:15.162961: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:44:15.166138: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:44:15.168487: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:44:15.170762: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:44:15.173088: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:44:15.176069: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:44:15.178555: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:44:15.733633: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_30' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_30}}]]\n",
      "2023-12-28 11:44:15.733832: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_59' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_59}}]]\n",
      "2023-12-28 11:44:15.733922: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_77' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_77}}]]\n",
      "2023-12-28 11:44:15.734014: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_95' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_95}}]]\n",
      "2023-12-28 11:44:15.734129: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_104' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_104}}]]\n",
      "2023-12-28 11:44:15.734231: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_133' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_133}}]]\n",
      "2023-12-28 11:44:15.734302: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_151' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_151}}]]\n",
      "2023-12-28 11:44:15.734372: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_169' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_169}}]]\n",
      "2023-12-28 11:44:15.734523: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_178' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_178}}]]\n",
      "2023-12-28 11:44:15.734635: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_207' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_207}}]]\n",
      "2023-12-28 11:44:15.734752: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_225' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_225}}]]\n",
      "2023-12-28 11:44:15.734966: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_243' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_243}}]]\n",
      "2023-12-28 11:44:15.735178: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_252' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_252}}]]\n",
      "2023-12-28 11:44:15.735496: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_280' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_280}}]]\n",
      "2023-12-28 11:44:15.735680: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_291' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_291}}]]\n",
      "2023-12-28 11:44:15.735903: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_298' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_298}}]]\n",
      "2023-12-28 11:44:15.736123: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_303' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_303}}]]\n",
      "2023-12-28 11:44:15.736279: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_306' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_306}}]]\n",
      "2023-12-28 11:44:15.736504: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_309' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_309}}]]\n",
      "2023-12-28 11:44:15.736683: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_312' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_312}}]]\n",
      "2023-12-28 11:44:15.736884: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_315' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_315}}]]\n",
      "2023-12-28 11:44:15.737089: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_318' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_318}}]]\n",
      "2023-12-28 11:44:15.737185: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_321' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_321}}]]\n",
      "2023-12-28 11:44:15.737317: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_324' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_324}}]]\n",
      "2023-12-28 11:44:15.737464: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_327' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_327}}]]\n",
      "2023-12-28 11:44:15.737620: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_330' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_330}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 loss: 0.1100002730 time: 3.3562488555908203\n",
      "Iteration: 10 loss: 0.0882126888 time: 0.09075522422790527\n",
      "Iteration: 20 loss: 0.0697115218 time: 0.09273886680603027\n",
      "Iteration: 30 loss: 0.0546825841 time: 0.0969381332397461\n",
      "Iteration: 40 loss: 0.0430141998 time: 0.08442258834838867\n",
      "Iteration: 50 loss: 0.0343307376 time: 0.0957791805267334\n",
      "Iteration: 60 loss: 0.0280925175 time: 0.08499789237976074\n",
      "Iteration: 70 loss: 0.0237064616 time: 0.09234428405761719\n",
      "Iteration: 80 loss: 0.0206183735 time: 0.08205056190490723\n",
      "Iteration: 90 loss: 0.0183771221 time: 0.08803582191467285\n",
      "Iteration: 100 loss: 0.0166577159 time: 0.09089994430541992\n",
      "Iteration: 110 loss: 0.0152502235 time: 0.0788724422454834\n",
      "Iteration: 120 loss: 0.0140317960 time: 0.07854986190795898\n",
      "Iteration: 130 loss: 0.0129362480 time: 0.07275652885437012\n",
      "Iteration: 140 loss: 0.0119296525 time: 0.07467770576477051\n",
      "Iteration: 150 loss: 0.0109944384 time: 0.07689380645751953\n",
      "Iteration: 160 loss: 0.0101206704 time: 0.06829643249511719\n",
      "Iteration: 170 loss: 0.0093019327 time: 0.07550382614135742\n",
      "Iteration: 180 loss: 0.0085336087 time: 0.07034111022949219\n",
      "Iteration: 190 loss: 0.0078121978 time: 0.07773113250732422\n",
      "Iteration: 200 loss: 0.0071350075 time: 0.0708000659942627\n",
      "Iteration: 210 loss: 0.0064999660 time: 0.0707247257232666\n",
      "Iteration: 220 loss: 0.0059054755 time: 0.06942558288574219\n",
      "Iteration: 230 loss: 0.0053502869 time: 0.06725859642028809\n",
      "Iteration: 240 loss: 0.0048333890 time: 0.06570172309875488\n",
      "Iteration: 250 loss: 0.0043539111 time: 0.07205486297607422\n",
      "Iteration: 260 loss: 0.0039110338 time: 0.07181525230407715\n",
      "Iteration: 270 loss: 0.0035039103 time: 0.06635022163391113\n",
      "Iteration: 280 loss: 0.0031315995 time: 0.07033896446228027\n",
      "Iteration: 290 loss: 0.0027930114 time: 0.07433915138244629\n",
      "Iteration: 300 loss: 0.0024868693 time: 0.0695183277130127\n",
      "Iteration: 310 loss: 0.0022116888 time: 0.0735325813293457\n",
      "Iteration: 320 loss: 0.0019657755 time: 0.0695810317993164\n",
      "Iteration: 330 loss: 0.0017472412 time: 0.06870603561401367\n",
      "Iteration: 340 loss: 0.0015540350 time: 0.07014179229736328\n",
      "Iteration: 350 loss: 0.0013839892 time: 0.06932806968688965\n",
      "Iteration: 360 loss: 0.0012348728 time: 0.06499457359313965\n",
      "Iteration: 370 loss: 0.0011044493 time: 0.06580090522766113\n",
      "Iteration: 380 loss: 0.0009905334 time: 0.07279229164123535\n",
      "Iteration: 390 loss: 0.0008910417 time: 0.07236814498901367\n",
      "Iteration: 400 loss: 0.0008040343 time: 0.0759432315826416\n",
      "Iteration: 410 loss: 0.0007277451 time: 0.07377338409423828\n",
      "Iteration: 420 loss: 0.0006606001 time: 0.07641720771789551\n",
      "Iteration: 430 loss: 0.0006012231 time: 0.07649755477905273\n",
      "Iteration: 440 loss: 0.0005484329 time: 0.07129240036010742\n",
      "Iteration: 450 loss: 0.0005012316 time: 0.07154417037963867\n",
      "Iteration: 460 loss: 0.0004587876 time: 0.07226824760437012\n",
      "Iteration: 470 loss: 0.0004204155 time: 0.06904268264770508\n",
      "Iteration: 480 loss: 0.0003855555 time: 0.06955552101135254\n",
      "Iteration: 490 loss: 0.0003537524 time: 0.06928420066833496\n",
      "Iteration: 500 loss: 0.0003246369 time: 0.07387995719909668\n",
      "Iteration: 510 loss: 0.0002979081 time: 0.07686543464660645\n",
      "Iteration: 520 loss: 0.0002733200 time: 0.08107185363769531\n",
      "Iteration: 530 loss: 0.0002506687 time: 0.07700800895690918\n",
      "Iteration: 540 loss: 0.0002297833 time: 0.07065868377685547\n",
      "Iteration: 550 loss: 0.0002105183 time: 0.07825064659118652\n",
      "Iteration: 560 loss: 0.0001927475 time: 0.07506799697875977\n",
      "Iteration: 570 loss: 0.0001763597 time: 0.07755088806152344\n",
      "Iteration: 580 loss: 0.0001612553 time: 0.07099604606628418\n",
      "Iteration: 590 loss: 0.0001473440 time: 0.07793784141540527\n",
      "Iteration: 600 loss: 0.0001345430 time: 0.08000445365905762\n",
      "Iteration: 610 loss: 0.0001227754 time: 0.07063031196594238\n",
      "Iteration: 620 loss: 0.0001119697 time: 0.0662088394165039\n",
      "Iteration: 630 loss: 0.0001020588 time: 0.06834578514099121\n",
      "Iteration: 640 loss: 0.0000929797 time: 0.07330918312072754\n",
      "Iteration: 650 loss: 0.0000846732 time: 0.07144641876220703\n",
      "Iteration: 660 loss: 0.0000770833 time: 0.06947708129882812\n",
      "Iteration: 670 loss: 0.0000701574 time: 0.07022404670715332\n",
      "Iteration: 680 loss: 0.0000638459 time: 0.06500506401062012\n",
      "Iteration: 690 loss: 0.0000581021 time: 0.06322360038757324\n",
      "Iteration: 700 loss: 0.0000528821 time: 0.06820082664489746\n",
      "Iteration: 710 loss: 0.0000481446 time: 0.06018805503845215\n",
      "Iteration: 720 loss: 0.0000438511 time: 0.06553483009338379\n",
      "Iteration: 730 loss: 0.0000399653 time: 0.07089853286743164\n",
      "Iteration: 740 loss: 0.0000364534 time: 0.07356381416320801\n",
      "Iteration: 750 loss: 0.0000332839 time: 0.07115983963012695\n",
      "Iteration: 760 loss: 0.0000304275 time: 0.07162833213806152\n",
      "Iteration: 770 loss: 0.0000278568 time: 0.0714716911315918\n",
      "Iteration: 780 loss: 0.0000255464 time: 0.07289600372314453\n",
      "Iteration: 790 loss: 0.0000234730 time: 0.0702369213104248\n",
      "Iteration: 800 loss: 0.0000216148 time: 0.07052826881408691\n",
      "Iteration: 810 loss: 0.0000199518 time: 0.07294631004333496\n",
      "Iteration: 820 loss: 0.0000184656 time: 0.07011842727661133\n",
      "Iteration: 830 loss: 0.0000171391 time: 0.0693519115447998\n",
      "Iteration: 840 loss: 0.0000159570 time: 0.07150626182556152\n",
      "Iteration: 850 loss: 0.0000149049 time: 0.07362079620361328\n",
      "Iteration: 860 loss: 0.0000139697 time: 0.0746607780456543\n",
      "Iteration: 870 loss: 0.0000131397 time: 0.07418465614318848\n",
      "Iteration: 880 loss: 0.0000124039 time: 0.07236766815185547\n",
      "Iteration: 890 loss: 0.0000117526 time: 0.06644535064697266\n",
      "Iteration: 900 loss: 0.0000111767 time: 0.06984376907348633\n",
      "Iteration: 910 loss: 0.0000106683 time: 0.07346391677856445\n",
      "Iteration: 920 loss: 0.0000102199 time: 0.06540131568908691\n",
      "Iteration: 930 loss: 0.0000098250 time: 0.07201075553894043\n",
      "Iteration: 940 loss: 0.0000094776 time: 0.06644034385681152\n",
      "Iteration: 950 loss: 0.0000091724 time: 0.06368684768676758\n",
      "Iteration: 960 loss: 0.0000089045 time: 0.06759214401245117\n",
      "Iteration: 970 loss: 0.0000086696 time: 0.06398916244506836\n",
      "Iteration: 980 loss: 0.0000084639 time: 0.06421613693237305\n",
      "Iteration: 990 loss: 0.0000082839 time: 0.08228397369384766\n",
      "Iteration: 1000 loss: 0.0000081266 time: 0.07242608070373535\n",
      "Iteration: 1010 loss: 0.0000079892 time: 0.07164382934570312\n",
      "Iteration: 1020 loss: 0.0000078692 time: 0.07061243057250977\n",
      "Iteration: 1030 loss: 0.0000077645 time: 0.06928896903991699\n",
      "Iteration: 1040 loss: 0.0000076732 time: 0.05932903289794922\n",
      "Iteration: 1050 loss: 0.0000075937 time: 0.0632028579711914\n",
      "Iteration: 1060 loss: 0.0000075243 time: 0.06441712379455566\n",
      "Iteration: 1070 loss: 0.0000074638 time: 0.07441592216491699\n",
      "Iteration: 1080 loss: 0.0000074111 time: 0.06610441207885742\n",
      "Iteration: 1090 loss: 0.0000073650 time: 0.06900596618652344\n",
      "Iteration: 1100 loss: 0.0000073248 time: 0.07883167266845703\n",
      "Iteration: 1110 loss: 0.0000072896 time: 0.07174158096313477\n",
      "Iteration: 1120 loss: 0.0000072588 time: 0.06637787818908691\n",
      "Iteration: 1130 loss: 0.0000072317 time: 0.0671074390411377\n",
      "Iteration: 1140 loss: 0.0000072078 time: 0.0679011344909668\n",
      "Iteration: 1150 loss: 0.0000071867 time: 0.0712137222290039\n",
      "Iteration: 1160 loss: 0.0000071680 time: 0.07136011123657227\n",
      "Iteration: 1170 loss: 0.0000071514 time: 0.0661625862121582\n",
      "Iteration: 1180 loss: 0.0000071364 time: 0.06525421142578125\n",
      "Iteration: 1190 loss: 0.0000071230 time: 0.06977200508117676\n",
      "Iteration: 1200 loss: 0.0000071108 time: 0.06752705574035645\n",
      "Iteration: 1210 loss: 0.0000070998 time: 0.06237459182739258\n",
      "Iteration: 1220 loss: 0.0000070896 time: 0.06789636611938477\n",
      "Iteration: 1230 loss: 0.0000070802 time: 0.06679272651672363\n",
      "Iteration: 1240 loss: 0.0000070714 time: 0.07390451431274414\n",
      "Iteration: 1250 loss: 0.0000070632 time: 0.06959962844848633\n",
      "Iteration: 1260 loss: 0.0000070555 time: 0.07087898254394531\n",
      "Iteration: 1270 loss: 0.0000070481 time: 0.06634521484375\n",
      "Iteration: 1280 loss: 0.0000070410 time: 0.06415796279907227\n",
      "Iteration: 1290 loss: 0.0000070342 time: 0.06688141822814941\n",
      "Iteration: 1300 loss: 0.0000070277 time: 0.06622314453125\n",
      "Iteration: 1310 loss: 0.0000070213 time: 0.07293152809143066\n",
      "Iteration: 1320 loss: 0.0000070150 time: 0.07063794136047363\n",
      "Iteration: 1330 loss: 0.0000070089 time: 0.07109475135803223\n",
      "Iteration: 1340 loss: 0.0000070028 time: 0.07024574279785156\n",
      "Iteration: 1350 loss: 0.0000069968 time: 0.06971859931945801\n",
      "Iteration: 1360 loss: 0.0000069909 time: 0.07284426689147949\n",
      "Iteration: 1370 loss: 0.0000069850 time: 0.0688638687133789\n",
      "Iteration: 1380 loss: 0.0000069791 time: 0.07535791397094727\n",
      "Iteration: 1390 loss: 0.0000069733 time: 0.07221770286560059\n",
      "Iteration: 1400 loss: 0.0000069674 time: 0.07162189483642578\n",
      "Iteration: 1410 loss: 0.0000069616 time: 0.06943321228027344\n",
      "Iteration: 1420 loss: 0.0000069557 time: 0.06792688369750977\n",
      "Iteration: 1430 loss: 0.0000069499 time: 0.06546282768249512\n",
      "Iteration: 1440 loss: 0.0000069440 time: 0.06524968147277832\n",
      "Iteration: 1450 loss: 0.0000069381 time: 0.07028651237487793\n",
      "Iteration: 1460 loss: 0.0000069322 time: 0.07043886184692383\n",
      "Iteration: 1470 loss: 0.0000069262 time: 0.06253504753112793\n",
      "Iteration: 1480 loss: 0.0000069203 time: 0.06308603286743164\n",
      "Iteration: 1490 loss: 0.0000069143 time: 0.07072758674621582\n",
      "Iteration: 1500 loss: 0.0000069083 time: 0.06919121742248535\n",
      "Iteration: 1510 loss: 0.0000069022 time: 0.06421279907226562\n",
      "Iteration: 1520 loss: 0.0000068961 time: 0.06786060333251953\n",
      "Iteration: 1530 loss: 0.0000068900 time: 0.07130789756774902\n",
      "Iteration: 1540 loss: 0.0000068839 time: 0.07135796546936035\n",
      "Iteration: 1550 loss: 0.0000068777 time: 0.06353759765625\n",
      "Iteration: 1560 loss: 0.0000068715 time: 0.07054638862609863\n",
      "Iteration: 1570 loss: 0.0000068653 time: 0.07509016990661621\n",
      "Iteration: 1580 loss: 0.0000068590 time: 0.07343292236328125\n",
      "Iteration: 1590 loss: 0.0000068527 time: 0.07145500183105469\n",
      "Iteration: 1600 loss: 0.0000068463 time: 0.06930160522460938\n",
      "Iteration: 1610 loss: 0.0000068399 time: 0.06758379936218262\n",
      "Iteration: 1620 loss: 0.0000068335 time: 0.06686782836914062\n",
      "Iteration: 1630 loss: 0.0000068271 time: 0.06981396675109863\n",
      "Iteration: 1640 loss: 0.0000068206 time: 0.0646517276763916\n",
      "Iteration: 1650 loss: 0.0000068141 time: 0.07001876831054688\n",
      "Iteration: 1660 loss: 0.0000068075 time: 0.06459593772888184\n",
      "Iteration: 1670 loss: 0.0000068010 time: 0.05799078941345215\n",
      "Iteration: 1680 loss: 0.0000067943 time: 0.06471920013427734\n",
      "Iteration: 1690 loss: 0.0000067877 time: 0.07486605644226074\n",
      "Iteration: 1700 loss: 0.0000067810 time: 0.0718679428100586\n",
      "Iteration: 1710 loss: 0.0000067742 time: 0.07610893249511719\n",
      "Iteration: 1720 loss: 0.0000067675 time: 0.06829380989074707\n",
      "Iteration: 1730 loss: 0.0000067607 time: 0.07065343856811523\n",
      "Iteration: 1740 loss: 0.0000067539 time: 0.06499719619750977\n",
      "Iteration: 1750 loss: 0.0000067470 time: 0.06782293319702148\n",
      "Iteration: 1760 loss: 0.0000067401 time: 0.07133007049560547\n",
      "Iteration: 1770 loss: 0.0000067331 time: 0.06897544860839844\n",
      "Iteration: 1780 loss: 0.0000067262 time: 0.0689845085144043\n",
      "Iteration: 1790 loss: 0.0000067191 time: 0.06531620025634766\n",
      "Iteration: 1800 loss: 0.0000067121 time: 0.06654047966003418\n",
      "Iteration: 1810 loss: 0.0000067050 time: 0.06237912178039551\n",
      "Iteration: 1820 loss: 0.0000066979 time: 0.06379032135009766\n",
      "Iteration: 1830 loss: 0.0000066907 time: 0.06068539619445801\n",
      "Iteration: 1840 loss: 0.0000066835 time: 0.06669139862060547\n",
      "Iteration: 1850 loss: 0.0000066763 time: 0.06753802299499512\n",
      "Iteration: 1860 loss: 0.0000066690 time: 0.0711066722869873\n",
      "Iteration: 1870 loss: 0.0000066617 time: 0.07123470306396484\n",
      "Iteration: 1880 loss: 0.0000066544 time: 0.06931352615356445\n",
      "Iteration: 1890 loss: 0.0000066470 time: 0.0697789192199707\n",
      "Iteration: 1900 loss: 0.0000066396 time: 0.07234907150268555\n",
      "Iteration: 1910 loss: 0.0000066321 time: 0.06913089752197266\n",
      "Iteration: 1920 loss: 0.0000066246 time: 0.06414198875427246\n",
      "Iteration: 1930 loss: 0.0000066171 time: 0.06815338134765625\n",
      "Iteration: 1940 loss: 0.0000066095 time: 0.07060575485229492\n",
      "Iteration: 1950 loss: 0.0000066019 time: 0.07168912887573242\n",
      "Iteration: 1960 loss: 0.0000065943 time: 0.0721597671508789\n",
      "Iteration: 1970 loss: 0.0000065866 time: 0.0606379508972168\n",
      "Iteration: 1980 loss: 0.0000065789 time: 0.0643770694732666\n",
      "Iteration: 1990 loss: 0.0000065711 time: 0.0697317123413086\n",
      "Iteration: 2000 loss: 0.0000065633 time: 0.06841564178466797\n",
      "Iteration: 2010 loss: 0.0000065555 time: 0.06931757926940918\n",
      "Iteration: 2020 loss: 0.0000065476 time: 0.0721426010131836\n",
      "Iteration: 2030 loss: 0.0000065397 time: 0.0756683349609375\n",
      "Iteration: 2040 loss: 0.0000065318 time: 0.0679478645324707\n",
      "Iteration: 2050 loss: 0.0000065238 time: 0.06556391716003418\n",
      "Iteration: 2060 loss: 0.0000065158 time: 0.06717133522033691\n",
      "Iteration: 2070 loss: 0.0000065077 time: 0.06490445137023926\n",
      "Iteration: 2080 loss: 0.0000064996 time: 0.06485366821289062\n",
      "Iteration: 2090 loss: 0.0000064915 time: 0.07398533821105957\n",
      "Iteration: 2100 loss: 0.0000064833 time: 0.07020211219787598\n",
      "Iteration: 2110 loss: 0.0000064751 time: 0.06881093978881836\n",
      "Iteration: 2120 loss: 0.0000064669 time: 0.06980204582214355\n",
      "Iteration: 2130 loss: 0.0000064586 time: 0.07253170013427734\n",
      "Iteration: 2140 loss: 0.0000064503 time: 0.06560754776000977\n",
      "Iteration: 2150 loss: 0.0000064419 time: 0.06709027290344238\n",
      "Iteration: 2160 loss: 0.0000064335 time: 0.07016992568969727\n",
      "Iteration: 2170 loss: 0.0000064250 time: 0.07263970375061035\n",
      "Iteration: 2180 loss: 0.0000064166 time: 0.06999945640563965\n",
      "Iteration: 2190 loss: 0.0000064080 time: 0.06934380531311035\n",
      "Iteration: 2200 loss: 0.0000063995 time: 0.06368446350097656\n",
      "Iteration: 2210 loss: 0.0000063909 time: 0.06537032127380371\n",
      "Iteration: 2220 loss: 0.0000063822 time: 0.06591415405273438\n",
      "Iteration: 2230 loss: 0.0000063735 time: 0.06947493553161621\n",
      "Iteration: 2240 loss: 0.0000063648 time: 0.06962800025939941\n",
      "Iteration: 2250 loss: 0.0000063561 time: 0.06694269180297852\n",
      "Iteration: 2260 loss: 0.0000063473 time: 0.06706643104553223\n",
      "Iteration: 2270 loss: 0.0000063384 time: 0.07699418067932129\n",
      "Iteration: 2280 loss: 0.0000063295 time: 0.07288789749145508\n",
      "Iteration: 2290 loss: 0.0000063206 time: 0.07423591613769531\n",
      "Iteration: 2300 loss: 0.0000063117 time: 0.07108235359191895\n",
      "Iteration: 2310 loss: 0.0000063027 time: 0.07134675979614258\n",
      "Iteration: 2320 loss: 0.0000062936 time: 0.07204794883728027\n",
      "Iteration: 2330 loss: 0.0000062845 time: 0.06964349746704102\n",
      "Iteration: 2340 loss: 0.0000062754 time: 0.07137632369995117\n",
      "Iteration: 2350 loss: 0.0000062662 time: 0.06851744651794434\n",
      "Iteration: 2360 loss: 0.0000062570 time: 0.06551361083984375\n",
      "Iteration: 2370 loss: 0.0000062478 time: 0.0671534538269043\n",
      "Iteration: 2380 loss: 0.0000062385 time: 0.07297563552856445\n",
      "Iteration: 2390 loss: 0.0000062292 time: 0.06544780731201172\n",
      "Iteration: 2400 loss: 0.0000062198 time: 0.06522512435913086\n",
      "Iteration: 2410 loss: 0.0000062104 time: 0.06750941276550293\n",
      "Iteration: 2420 loss: 0.0000062010 time: 0.06952333450317383\n",
      "Iteration: 2430 loss: 0.0000061915 time: 0.06426119804382324\n",
      "Iteration: 2440 loss: 0.0000061819 time: 0.0745539665222168\n",
      "Iteration: 2450 loss: 0.0000061724 time: 0.06902194023132324\n",
      "Iteration: 2460 loss: 0.0000061627 time: 0.07110810279846191\n",
      "Iteration: 2470 loss: 0.0000061531 time: 0.06964302062988281\n",
      "Iteration: 2480 loss: 0.0000061434 time: 0.06852555274963379\n",
      "Iteration: 2490 loss: 0.0000061336 time: 0.06782960891723633\n",
      "Iteration: 2500 loss: 0.0000061238 time: 0.06176328659057617\n",
      "Iteration: 2510 loss: 0.0000061140 time: 0.0742344856262207\n",
      "Iteration: 2520 loss: 0.0000061042 time: 0.07298731803894043\n",
      "Iteration: 2530 loss: 0.0000060942 time: 0.06701064109802246\n",
      "Iteration: 2540 loss: 0.0000060843 time: 0.06691694259643555\n",
      "Iteration: 2550 loss: 0.0000060743 time: 0.07039189338684082\n",
      "Iteration: 2560 loss: 0.0000060643 time: 0.06680655479431152\n",
      "Iteration: 2570 loss: 0.0000060542 time: 0.07235860824584961\n",
      "Iteration: 2580 loss: 0.0000060441 time: 0.07069921493530273\n",
      "Iteration: 2590 loss: 0.0000060339 time: 0.07132220268249512\n",
      "Iteration: 2600 loss: 0.0000060237 time: 0.06950640678405762\n",
      "Iteration: 2610 loss: 0.0000060134 time: 0.07166433334350586\n",
      "Iteration: 2620 loss: 0.0000060031 time: 0.06713128089904785\n",
      "Iteration: 2630 loss: 0.0000059928 time: 0.07062911987304688\n",
      "Iteration: 2640 loss: 0.0000059824 time: 0.07774043083190918\n",
      "Iteration: 2650 loss: 0.0000059720 time: 0.07326340675354004\n",
      "Iteration: 2660 loss: 0.0000059615 time: 0.07068657875061035\n",
      "Iteration: 2670 loss: 0.0000059510 time: 0.06926631927490234\n",
      "Iteration: 2680 loss: 0.0000059405 time: 0.06694912910461426\n",
      "Iteration: 2690 loss: 0.0000059299 time: 0.06201529502868652\n",
      "Iteration: 2700 loss: 0.0000059192 time: 0.07160472869873047\n",
      "Iteration: 2710 loss: 0.0000059086 time: 0.07074451446533203\n",
      "Iteration: 2720 loss: 0.0000058978 time: 0.07089757919311523\n",
      "Iteration: 2730 loss: 0.0000058871 time: 0.06812572479248047\n",
      "Iteration: 2740 loss: 0.0000058762 time: 0.07279777526855469\n",
      "Iteration: 2750 loss: 0.0000058654 time: 0.07386302947998047\n",
      "Iteration: 2760 loss: 0.0000058545 time: 0.0710906982421875\n",
      "Iteration: 2770 loss: 0.0000058435 time: 0.06450319290161133\n",
      "Iteration: 2780 loss: 0.0000058325 time: 0.06165933609008789\n",
      "Iteration: 2790 loss: 0.0000058215 time: 0.06798100471496582\n",
      "Iteration: 2800 loss: 0.0000058104 time: 0.07335352897644043\n",
      "Iteration: 2810 loss: 0.0000057993 time: 0.07431364059448242\n",
      "Iteration: 2820 loss: 0.0000057881 time: 0.07409930229187012\n",
      "Iteration: 2830 loss: 0.0000057769 time: 0.06395268440246582\n",
      "Iteration: 2840 loss: 0.0000057657 time: 0.06813311576843262\n",
      "Iteration: 2850 loss: 0.0000057544 time: 0.06899356842041016\n",
      "Iteration: 2860 loss: 0.0000057430 time: 0.06319832801818848\n",
      "Iteration: 2870 loss: 0.0000057316 time: 0.0688776969909668\n",
      "Iteration: 2880 loss: 0.0000057202 time: 0.06648778915405273\n",
      "Iteration: 2890 loss: 0.0000057087 time: 0.06777095794677734\n",
      "Iteration: 2900 loss: 0.0000056972 time: 0.07211446762084961\n",
      "Iteration: 2910 loss: 0.0000056856 time: 0.07576680183410645\n",
      "Iteration: 2920 loss: 0.0000056740 time: 0.06649637222290039\n",
      "Iteration: 2930 loss: 0.0000056623 time: 0.06888389587402344\n",
      "Iteration: 2940 loss: 0.0000056506 time: 0.06732058525085449\n",
      "Iteration: 2950 loss: 0.0000056389 time: 0.06818151473999023\n",
      "Iteration: 2960 loss: 0.0000056271 time: 0.06887674331665039\n",
      "Iteration: 2970 loss: 0.0000056152 time: 0.07142901420593262\n",
      "Iteration: 2980 loss: 0.0000056034 time: 0.07297205924987793\n",
      "Iteration: 2990 loss: 0.0000055914 time: 0.06734609603881836\n",
      "Iteration: 3000 loss: 0.0000055794 time: 0.06691241264343262\n",
      "Iteration: 3010 loss: 0.0000055674 time: 0.0644235610961914\n",
      "Iteration: 3020 loss: 0.0000055553 time: 0.06420540809631348\n",
      "Iteration: 3030 loss: 0.0000055432 time: 0.06649947166442871\n",
      "Iteration: 3040 loss: 0.0000055311 time: 0.06823539733886719\n",
      "Iteration: 3050 loss: 0.0000055189 time: 0.06511116027832031\n",
      "Iteration: 3060 loss: 0.0000055066 time: 0.07086396217346191\n",
      "Iteration: 3070 loss: 0.0000054943 time: 0.07173442840576172\n",
      "Iteration: 3080 loss: 0.0000054820 time: 0.07371926307678223\n",
      "Iteration: 3090 loss: 0.0000054696 time: 0.06469845771789551\n",
      "Iteration: 3100 loss: 0.0000054571 time: 0.07310914993286133\n",
      "Iteration: 3110 loss: 0.0000054446 time: 0.07096076011657715\n",
      "Iteration: 3120 loss: 0.0000054321 time: 0.06914925575256348\n",
      "Iteration: 3130 loss: 0.0000054195 time: 0.0717620849609375\n",
      "Iteration: 3140 loss: 0.0000054069 time: 0.07386493682861328\n",
      "Iteration: 3150 loss: 0.0000053943 time: 0.07234883308410645\n",
      "Iteration: 3160 loss: 0.0000053815 time: 0.06921935081481934\n",
      "Iteration: 3170 loss: 0.0000053688 time: 0.07095122337341309\n",
      "Iteration: 3180 loss: 0.0000053560 time: 0.07372045516967773\n",
      "Iteration: 3190 loss: 0.0000053431 time: 0.07260894775390625\n",
      "Iteration: 3200 loss: 0.0000053302 time: 0.0686333179473877\n",
      "Iteration: 3210 loss: 0.0000053173 time: 0.06283140182495117\n",
      "Iteration: 3220 loss: 0.0000053043 time: 0.07245016098022461\n",
      "Iteration: 3230 loss: 0.0000052912 time: 0.06983494758605957\n",
      "Iteration: 3240 loss: 0.0000052782 time: 0.06950020790100098\n",
      "Iteration: 3250 loss: 0.0000052650 time: 0.07167172431945801\n",
      "Iteration: 3260 loss: 0.0000052518 time: 0.06888246536254883\n",
      "Iteration: 3270 loss: 0.0000052386 time: 0.07027506828308105\n",
      "Iteration: 3280 loss: 0.0000052253 time: 0.07318425178527832\n",
      "Iteration: 3290 loss: 0.0000052120 time: 0.0671236515045166\n",
      "Iteration: 3300 loss: 0.0000051987 time: 0.0717625617980957\n",
      "Iteration: 3310 loss: 0.0000051853 time: 0.06602144241333008\n",
      "Iteration: 3320 loss: 0.0000051718 time: 0.06870365142822266\n",
      "Iteration: 3330 loss: 0.0000051583 time: 0.06623029708862305\n",
      "Iteration: 3340 loss: 0.0000051447 time: 0.0680387020111084\n",
      "Iteration: 3350 loss: 0.0000051311 time: 0.06853389739990234\n",
      "Iteration: 3360 loss: 0.0000051175 time: 0.06688570976257324\n",
      "Iteration: 3370 loss: 0.0000051038 time: 0.06545186042785645\n",
      "Iteration: 3380 loss: 0.0000050901 time: 0.07332944869995117\n",
      "Iteration: 3390 loss: 0.0000050763 time: 0.07394289970397949\n",
      "Iteration: 3400 loss: 0.0000050625 time: 0.07108926773071289\n",
      "Iteration: 3410 loss: 0.0000050486 time: 0.07424449920654297\n",
      "Iteration: 3420 loss: 0.0000050347 time: 0.06414127349853516\n",
      "Iteration: 3430 loss: 0.0000050207 time: 0.07029271125793457\n",
      "Iteration: 3440 loss: 0.0000050067 time: 0.06662178039550781\n",
      "Iteration: 3450 loss: 0.0000049926 time: 0.0705270767211914\n",
      "Iteration: 3460 loss: 0.0000049785 time: 0.06925725936889648\n",
      "Iteration: 3470 loss: 0.0000049643 time: 0.06949234008789062\n",
      "Iteration: 3480 loss: 0.0000049501 time: 0.06547951698303223\n",
      "Iteration: 3490 loss: 0.0000049359 time: 0.06555533409118652\n",
      "Iteration: 3500 loss: 0.0000049216 time: 0.06450414657592773\n",
      "Iteration: 3510 loss: 0.0000049072 time: 0.07133865356445312\n",
      "Iteration: 3520 loss: 0.0000048929 time: 0.07530403137207031\n",
      "Iteration: 3530 loss: 0.0000048784 time: 0.07358527183532715\n",
      "Iteration: 3540 loss: 0.0000048639 time: 0.07037234306335449\n",
      "Iteration: 3550 loss: 0.0000048494 time: 0.06994962692260742\n",
      "Iteration: 3560 loss: 0.0000048348 time: 0.06930041313171387\n",
      "Iteration: 3570 loss: 0.0000048202 time: 0.06699538230895996\n",
      "Iteration: 3580 loss: 0.0000048056 time: 0.06958770751953125\n",
      "Iteration: 3590 loss: 0.0000047909 time: 0.06668758392333984\n",
      "Iteration: 3600 loss: 0.0000047761 time: 0.07074689865112305\n",
      "Iteration: 3610 loss: 0.0000047613 time: 0.07207393646240234\n",
      "Iteration: 3620 loss: 0.0000047464 time: 0.07317876815795898\n",
      "Iteration: 3630 loss: 0.0000047316 time: 0.07310152053833008\n",
      "Iteration: 3640 loss: 0.0000047166 time: 0.06998705863952637\n",
      "Iteration: 3650 loss: 0.0000047016 time: 0.07115435600280762\n",
      "Iteration: 3660 loss: 0.0000046866 time: 0.06787252426147461\n",
      "Iteration: 3670 loss: 0.0000046715 time: 0.06998896598815918\n",
      "Iteration: 3680 loss: 0.0000046564 time: 0.06426072120666504\n",
      "Iteration: 3690 loss: 0.0000046412 time: 0.07491660118103027\n",
      "Iteration: 3700 loss: 0.0000046260 time: 0.0740499496459961\n",
      "Iteration: 3710 loss: 0.0000046108 time: 0.06913280487060547\n",
      "Iteration: 3720 loss: 0.0000045955 time: 0.0663764476776123\n",
      "Iteration: 3730 loss: 0.0000045801 time: 0.06837797164916992\n",
      "Iteration: 3740 loss: 0.0000045648 time: 0.0723423957824707\n",
      "Iteration: 3750 loss: 0.0000045493 time: 0.06471395492553711\n",
      "Iteration: 3760 loss: 0.0000045338 time: 0.06396794319152832\n",
      "Iteration: 3770 loss: 0.0000045183 time: 0.06700825691223145\n",
      "Iteration: 3780 loss: 0.0000045028 time: 0.06512022018432617\n",
      "Iteration: 3790 loss: 0.0000044872 time: 0.07338595390319824\n",
      "Iteration: 3800 loss: 0.0000044715 time: 0.07194972038269043\n",
      "Iteration: 3810 loss: 0.0000044558 time: 0.06955456733703613\n",
      "Iteration: 3820 loss: 0.0000044401 time: 0.07250356674194336\n",
      "Iteration: 3830 loss: 0.0000044243 time: 0.0660405158996582\n",
      "Iteration: 3840 loss: 0.0000044085 time: 0.06400156021118164\n",
      "Iteration: 3850 loss: 0.0000043926 time: 0.06508183479309082\n",
      "Iteration: 3860 loss: 0.0000043767 time: 0.07410883903503418\n",
      "Iteration: 3870 loss: 0.0000043607 time: 0.06939053535461426\n",
      "Iteration: 3880 loss: 0.0000043447 time: 0.07011532783508301\n",
      "Iteration: 3890 loss: 0.0000043287 time: 0.06507396697998047\n",
      "Iteration: 3900 loss: 0.0000043126 time: 0.07404065132141113\n",
      "Iteration: 3910 loss: 0.0000042965 time: 0.07372689247131348\n",
      "Iteration: 3920 loss: 0.0000042803 time: 0.07336902618408203\n",
      "Iteration: 3930 loss: 0.0000042641 time: 0.0741875171661377\n",
      "Iteration: 3940 loss: 0.0000042479 time: 0.06353402137756348\n",
      "Iteration: 3950 loss: 0.0000042316 time: 0.07312417030334473\n",
      "Iteration: 3960 loss: 0.0000042153 time: 0.06501388549804688\n",
      "Iteration: 3970 loss: 0.0000041989 time: 0.06470537185668945\n",
      "Iteration: 3980 loss: 0.0000041825 time: 0.06681942939758301\n",
      "Iteration: 3990 loss: 0.0000041660 time: 0.06789040565490723\n",
      "Iteration: 4000 loss: 0.0000041495 time: 0.06846451759338379\n",
      "Iteration: 4010 loss: 0.0000041330 time: 0.07279562950134277\n",
      "Iteration: 4020 loss: 0.0000041164 time: 0.06604194641113281\n",
      "Iteration: 4030 loss: 0.0000040998 time: 0.06868720054626465\n",
      "Iteration: 4040 loss: 0.0000040832 time: 0.0684967041015625\n",
      "Iteration: 4050 loss: 0.0000040665 time: 0.06704449653625488\n",
      "Iteration: 4060 loss: 0.0000040498 time: 0.06973457336425781\n",
      "Iteration: 4070 loss: 0.0000040330 time: 0.05824422836303711\n",
      "Iteration: 4080 loss: 0.0000040162 time: 0.060915470123291016\n",
      "Iteration: 4090 loss: 0.0000039994 time: 0.06764340400695801\n",
      "Iteration: 4100 loss: 0.0000039825 time: 0.06975317001342773\n",
      "Iteration: 4110 loss: 0.0000039656 time: 0.06512713432312012\n",
      "Iteration: 4120 loss: 0.0000039486 time: 0.06636357307434082\n",
      "Iteration: 4130 loss: 0.0000039317 time: 0.07295727729797363\n",
      "Iteration: 4140 loss: 0.0000039146 time: 0.06837964057922363\n",
      "Iteration: 4150 loss: 0.0000038976 time: 0.07062029838562012\n",
      "Iteration: 4160 loss: 0.0000038805 time: 0.07055878639221191\n",
      "Iteration: 4170 loss: 0.0000038634 time: 0.0688009262084961\n",
      "Iteration: 4180 loss: 0.0000038462 time: 0.06680440902709961\n",
      "Iteration: 4190 loss: 0.0000038290 time: 0.06974959373474121\n",
      "Iteration: 4200 loss: 0.0000038118 time: 0.07463955879211426\n",
      "Iteration: 4210 loss: 0.0000037945 time: 0.0642538070678711\n",
      "Iteration: 4220 loss: 0.0000037772 time: 0.07528519630432129\n",
      "Iteration: 4230 loss: 0.0000037599 time: 0.06097555160522461\n",
      "Iteration: 4240 loss: 0.0000037425 time: 0.07136225700378418\n",
      "Iteration: 4250 loss: 0.0000037251 time: 0.06746673583984375\n",
      "Iteration: 4260 loss: 0.0000037077 time: 0.06837725639343262\n",
      "Iteration: 4270 loss: 0.0000036902 time: 0.06573486328125\n",
      "Iteration: 4280 loss: 0.0000036727 time: 0.06843805313110352\n",
      "Iteration: 4290 loss: 0.0000036552 time: 0.06936073303222656\n",
      "Iteration: 4300 loss: 0.0000036377 time: 0.07483077049255371\n",
      "Iteration: 4310 loss: 0.0000036201 time: 0.07323241233825684\n",
      "Iteration: 4320 loss: 0.0000036025 time: 0.07417106628417969\n",
      "Iteration: 4330 loss: 0.0000035848 time: 0.07364964485168457\n",
      "Iteration: 4340 loss: 0.0000035672 time: 0.07418155670166016\n",
      "Iteration: 4350 loss: 0.0000035495 time: 0.07001805305480957\n",
      "Iteration: 4360 loss: 0.0000035317 time: 0.06046485900878906\n",
      "Iteration: 4370 loss: 0.0000035140 time: 0.06393980979919434\n",
      "Iteration: 4380 loss: 0.0000034962 time: 0.06916141510009766\n",
      "Iteration: 4390 loss: 0.0000034784 time: 0.07215213775634766\n",
      "Iteration: 4400 loss: 0.0000034606 time: 0.07446527481079102\n",
      "Iteration: 4410 loss: 0.0000034427 time: 0.07157063484191895\n",
      "Iteration: 4420 loss: 0.0000034248 time: 0.0639791488647461\n",
      "Iteration: 4430 loss: 0.0000034069 time: 0.07008814811706543\n",
      "Iteration: 4440 loss: 0.0000033890 time: 0.07132339477539062\n",
      "Iteration: 4450 loss: 0.0000033710 time: 0.0649714469909668\n",
      "Iteration: 4460 loss: 0.0000033531 time: 0.06870651245117188\n",
      "Iteration: 4470 loss: 0.0000033351 time: 0.0670938491821289\n",
      "Iteration: 4480 loss: 0.0000033171 time: 0.06404995918273926\n",
      "Iteration: 4490 loss: 0.0000032990 time: 0.06807589530944824\n",
      "Iteration: 4500 loss: 0.0000032810 time: 0.06646847724914551\n",
      "Iteration: 4510 loss: 0.0000032629 time: 0.06511855125427246\n",
      "Iteration: 4520 loss: 0.0000032448 time: 0.06853938102722168\n",
      "Iteration: 4530 loss: 0.0000032267 time: 0.07093071937561035\n",
      "Iteration: 4540 loss: 0.0000032085 time: 0.07314205169677734\n",
      "Iteration: 4550 loss: 0.0000031904 time: 0.07324671745300293\n",
      "Iteration: 4560 loss: 0.0000031722 time: 0.07462525367736816\n",
      "Iteration: 4570 loss: 0.0000031540 time: 0.0702517032623291\n",
      "Iteration: 4580 loss: 0.0000031358 time: 0.06931281089782715\n",
      "Iteration: 4590 loss: 0.0000031176 time: 0.06494951248168945\n",
      "Iteration: 4600 loss: 0.0000030993 time: 0.06738042831420898\n",
      "Iteration: 4610 loss: 0.0000030811 time: 0.06996369361877441\n",
      "Iteration: 4620 loss: 0.0000030628 time: 0.06764912605285645\n",
      "Iteration: 4630 loss: 0.0000030445 time: 0.06922626495361328\n",
      "Iteration: 4640 loss: 0.0000030263 time: 0.0691213607788086\n",
      "Iteration: 4650 loss: 0.0000030080 time: 0.07281088829040527\n",
      "Iteration: 4660 loss: 0.0000029896 time: 0.07487821578979492\n",
      "Iteration: 4670 loss: 0.0000029713 time: 0.0638575553894043\n",
      "Iteration: 4680 loss: 0.0000029530 time: 0.06588363647460938\n",
      "Iteration: 4690 loss: 0.0000029346 time: 0.07271289825439453\n",
      "Iteration: 4700 loss: 0.0000029163 time: 0.07282257080078125\n",
      "Iteration: 4710 loss: 0.0000028979 time: 0.07226109504699707\n",
      "Iteration: 4720 loss: 0.0000028796 time: 0.07325601577758789\n",
      "Iteration: 4730 loss: 0.0000028612 time: 0.06900835037231445\n",
      "Iteration: 4740 loss: 0.0000028428 time: 0.07042837142944336\n",
      "Iteration: 4750 loss: 0.0000028245 time: 0.06456208229064941\n",
      "Iteration: 4760 loss: 0.0000028061 time: 0.06496691703796387\n",
      "Iteration: 4770 loss: 0.0000027877 time: 0.05855250358581543\n",
      "Iteration: 4780 loss: 0.0000027693 time: 0.0712120532989502\n",
      "Iteration: 4790 loss: 0.0000027509 time: 0.06922030448913574\n",
      "Iteration: 4800 loss: 0.0000027325 time: 0.06974244117736816\n",
      "Iteration: 4810 loss: 0.0000027141 time: 0.06635117530822754\n",
      "Iteration: 4820 loss: 0.0000026957 time: 0.07245826721191406\n",
      "Iteration: 4830 loss: 0.0000026773 time: 0.06648898124694824\n",
      "Iteration: 4840 loss: 0.0000026590 time: 0.06914782524108887\n",
      "Iteration: 4850 loss: 0.0000026406 time: 0.07205080986022949\n",
      "Iteration: 4860 loss: 0.0000026222 time: 0.07348918914794922\n",
      "Iteration: 4870 loss: 0.0000026038 time: 0.06382060050964355\n",
      "Iteration: 4880 loss: 0.0000025854 time: 0.07212328910827637\n",
      "Iteration: 4890 loss: 0.0000025671 time: 0.07300496101379395\n",
      "Iteration: 4900 loss: 0.0000025487 time: 0.07221651077270508\n",
      "Iteration: 4910 loss: 0.0000025304 time: 0.07450747489929199\n",
      "Iteration: 4920 loss: 0.0000025120 time: 0.07163047790527344\n",
      "Iteration: 4930 loss: 0.0000024937 time: 0.07345962524414062\n",
      "Iteration: 4940 loss: 0.0000024754 time: 0.0739748477935791\n",
      "Iteration: 4950 loss: 0.0000024571 time: 0.07364177703857422\n",
      "Iteration: 4960 loss: 0.0000024388 time: 0.06870794296264648\n",
      "Iteration: 4970 loss: 0.0000024205 time: 0.06766915321350098\n",
      "Iteration: 4980 loss: 0.0000024022 time: 0.06244158744812012\n",
      "Iteration: 4990 loss: 0.0000023839 time: 0.06205320358276367\n",
      "Iteration: 5000 loss: 0.0000023657 time: 0.06286954879760742\n",
      "Iteration: 5010 loss: 0.0000023475 time: 0.06299805641174316\n",
      "Iteration: 5020 loss: 0.0000023293 time: 0.07162642478942871\n",
      "Iteration: 5030 loss: 0.0000023111 time: 0.06831789016723633\n",
      "Iteration: 5040 loss: 0.0000022929 time: 0.07704877853393555\n",
      "Iteration: 5050 loss: 0.0000022748 time: 0.06966686248779297\n",
      "Iteration: 5060 loss: 0.0000022566 time: 0.06322288513183594\n",
      "Iteration: 5070 loss: 0.0000022385 time: 0.07399296760559082\n",
      "Iteration: 5080 loss: 0.0000022204 time: 0.07162952423095703\n",
      "Iteration: 5090 loss: 0.0000022024 time: 0.07413506507873535\n",
      "Iteration: 5100 loss: 0.0000021843 time: 0.07456469535827637\n",
      "Iteration: 5110 loss: 0.0000021663 time: 0.06531286239624023\n",
      "Iteration: 5120 loss: 0.0000021483 time: 0.0640864372253418\n",
      "Iteration: 5130 loss: 0.0000021304 time: 0.06615257263183594\n",
      "Iteration: 5140 loss: 0.0000021125 time: 0.07366442680358887\n",
      "Iteration: 5150 loss: 0.0000020946 time: 0.06929802894592285\n",
      "Iteration: 5160 loss: 0.0000020767 time: 0.07610082626342773\n",
      "Iteration: 5170 loss: 0.0000020588 time: 0.0721743106842041\n",
      "Iteration: 5180 loss: 0.0000020410 time: 0.0750725269317627\n",
      "Iteration: 5190 loss: 0.0000020233 time: 0.07587003707885742\n",
      "Iteration: 5200 loss: 0.0000020055 time: 0.06717610359191895\n",
      "Iteration: 5210 loss: 0.0000019878 time: 0.06704998016357422\n",
      "Iteration: 5220 loss: 0.0000019702 time: 0.06937694549560547\n",
      "Iteration: 5230 loss: 0.0000019525 time: 0.0713968276977539\n",
      "Iteration: 5240 loss: 0.0000019349 time: 0.06355428695678711\n",
      "Iteration: 5250 loss: 0.0000019174 time: 0.06627583503723145\n",
      "Iteration: 5260 loss: 0.0000018999 time: 0.0695798397064209\n",
      "Iteration: 5270 loss: 0.0000018824 time: 0.07087016105651855\n",
      "Iteration: 5280 loss: 0.0000018650 time: 0.07170248031616211\n",
      "Iteration: 5290 loss: 0.0000018476 time: 0.06859350204467773\n",
      "Iteration: 5300 loss: 0.0000018302 time: 0.0696723461151123\n",
      "Iteration: 5310 loss: 0.0000018129 time: 0.07128071784973145\n",
      "Iteration: 5320 loss: 0.0000017957 time: 0.07430219650268555\n",
      "Iteration: 5330 loss: 0.0000017785 time: 0.07318902015686035\n",
      "Iteration: 5340 loss: 0.0000017613 time: 0.07553434371948242\n",
      "Iteration: 5350 loss: 0.0000017442 time: 0.07191824913024902\n",
      "Iteration: 5360 loss: 0.0000017271 time: 0.07400178909301758\n",
      "Iteration: 5370 loss: 0.0000017101 time: 0.07320332527160645\n",
      "Iteration: 5380 loss: 0.0000016931 time: 0.0674278736114502\n",
      "Iteration: 5390 loss: 0.0000016762 time: 0.07032060623168945\n",
      "Iteration: 5400 loss: 0.0000016594 time: 0.07235360145568848\n",
      "Iteration: 5410 loss: 0.0000016426 time: 0.07143688201904297\n",
      "Iteration: 5420 loss: 0.0000016258 time: 0.06972765922546387\n",
      "Iteration: 5430 loss: 0.0000016092 time: 0.07045125961303711\n",
      "Iteration: 5440 loss: 0.0000015925 time: 0.07501959800720215\n",
      "Iteration: 5450 loss: 0.0000015760 time: 0.07337665557861328\n",
      "Iteration: 5460 loss: 0.0000015594 time: 0.07346463203430176\n",
      "Iteration: 5470 loss: 0.0000015430 time: 0.06537270545959473\n",
      "Iteration: 5480 loss: 0.0000015266 time: 0.07097291946411133\n",
      "Iteration: 5490 loss: 0.0000015103 time: 0.07013225555419922\n",
      "Iteration: 5500 loss: 0.0000014940 time: 0.07130670547485352\n",
      "Iteration: 5510 loss: 0.0000014778 time: 0.057752370834350586\n",
      "Iteration: 5520 loss: 0.0000014617 time: 0.06269526481628418\n",
      "Iteration: 5530 loss: 0.0000014456 time: 0.0700063705444336\n",
      "Iteration: 5540 loss: 0.0000014296 time: 0.06751489639282227\n",
      "Iteration: 5550 loss: 0.0000014137 time: 0.06981420516967773\n",
      "Iteration: 5560 loss: 0.0000013978 time: 0.07091498374938965\n",
      "Iteration: 5570 loss: 0.0000013820 time: 0.06959724426269531\n",
      "Iteration: 5580 loss: 0.0000013663 time: 0.06810164451599121\n",
      "Iteration: 5590 loss: 0.0000013506 time: 0.07020282745361328\n",
      "Iteration: 5600 loss: 0.0000013350 time: 0.06932377815246582\n",
      "Iteration: 5610 loss: 0.0000013195 time: 0.06375718116760254\n",
      "Iteration: 5620 loss: 0.0000013041 time: 0.07452082633972168\n",
      "Iteration: 5630 loss: 0.0000012887 time: 0.07159113883972168\n",
      "Iteration: 5640 loss: 0.0000012735 time: 0.07183122634887695\n",
      "Iteration: 5650 loss: 0.0000012583 time: 0.07422709465026855\n",
      "Iteration: 5660 loss: 0.0000012431 time: 0.07064294815063477\n",
      "Iteration: 5670 loss: 0.0000012281 time: 0.06621718406677246\n",
      "Iteration: 5680 loss: 0.0000012131 time: 0.07459235191345215\n",
      "Iteration: 5690 loss: 0.0000011982 time: 0.07140135765075684\n",
      "Iteration: 5700 loss: 0.0000011834 time: 0.06699466705322266\n",
      "Iteration: 5710 loss: 0.0000011687 time: 0.07117557525634766\n",
      "Iteration: 5720 loss: 0.0000011541 time: 0.07016253471374512\n",
      "Iteration: 5730 loss: 0.0000011395 time: 0.06923508644104004\n",
      "Iteration: 5740 loss: 0.0000011250 time: 0.07137584686279297\n",
      "Iteration: 5750 loss: 0.0000011106 time: 0.06833291053771973\n",
      "Iteration: 5760 loss: 0.0000010963 time: 0.0695033073425293\n",
      "Iteration: 5770 loss: 0.0000010821 time: 0.06441998481750488\n",
      "Iteration: 5780 loss: 0.0000010680 time: 0.07628989219665527\n",
      "Iteration: 5790 loss: 0.0000010539 time: 0.07606959342956543\n",
      "Iteration: 5800 loss: 0.0000010400 time: 0.07544064521789551\n",
      "Iteration: 5810 loss: 0.0000010261 time: 0.0731050968170166\n",
      "Iteration: 5820 loss: 0.0000010124 time: 0.07364344596862793\n",
      "Iteration: 5830 loss: 0.0000009987 time: 0.06719040870666504\n",
      "Iteration: 5840 loss: 0.0000009851 time: 0.06857514381408691\n",
      "Iteration: 5850 loss: 0.0000009716 time: 0.07083678245544434\n",
      "Iteration: 5860 loss: 0.0000009582 time: 0.06392359733581543\n",
      "Iteration: 5870 loss: 0.0000009449 time: 0.06254196166992188\n",
      "Iteration: 5880 loss: 0.0000009317 time: 0.0672006607055664\n",
      "Iteration: 5890 loss: 0.0000009186 time: 0.0637962818145752\n",
      "Iteration: 5900 loss: 0.0000009055 time: 0.07233452796936035\n",
      "Iteration: 5910 loss: 0.0000008926 time: 0.07130169868469238\n",
      "Iteration: 5920 loss: 0.0000008798 time: 0.07343173027038574\n",
      "Iteration: 5930 loss: 0.0000008671 time: 0.07549476623535156\n",
      "Iteration: 5940 loss: 0.0000008544 time: 0.061809539794921875\n",
      "Iteration: 5950 loss: 0.0000008419 time: 0.06126689910888672\n",
      "Iteration: 5960 loss: 0.0000008295 time: 0.06832051277160645\n",
      "Iteration: 5970 loss: 0.0000008171 time: 0.06888318061828613\n",
      "Iteration: 5980 loss: 0.0000008049 time: 0.07022714614868164\n",
      "Iteration: 5990 loss: 0.0000007927 time: 0.06735754013061523\n",
      "Iteration: 6000 loss: 0.0000007807 time: 0.06000232696533203\n",
      "Iteration: 6010 loss: 0.0000007688 time: 0.06445717811584473\n",
      "Iteration: 6020 loss: 0.0000007569 time: 0.06702876091003418\n",
      "Iteration: 6030 loss: 0.0000007452 time: 0.0660398006439209\n",
      "Iteration: 6040 loss: 0.0000007336 time: 0.06556510925292969\n",
      "Iteration: 6050 loss: 0.0000007221 time: 0.06970667839050293\n",
      "Iteration: 6060 loss: 0.0000007107 time: 0.07130050659179688\n",
      "Iteration: 6070 loss: 0.0000006993 time: 0.06633353233337402\n",
      "Iteration: 6080 loss: 0.0000006881 time: 0.07393074035644531\n",
      "Iteration: 6090 loss: 0.0000006770 time: 0.07123279571533203\n",
      "Iteration: 6100 loss: 0.0000006660 time: 0.07141590118408203\n",
      "Iteration: 6110 loss: 0.0000006551 time: 0.06762981414794922\n",
      "Iteration: 6120 loss: 0.0000006443 time: 0.06640124320983887\n",
      "Iteration: 6130 loss: 0.0000006336 time: 0.07027816772460938\n",
      "Iteration: 6140 loss: 0.0000006231 time: 0.06883621215820312\n",
      "Iteration: 6150 loss: 0.0000006126 time: 0.06870102882385254\n",
      "Iteration: 6160 loss: 0.0000006022 time: 0.07283759117126465\n",
      "Iteration: 6170 loss: 0.0000005920 time: 0.07088065147399902\n",
      "Iteration: 6180 loss: 0.0000005818 time: 0.0644540786743164\n",
      "Iteration: 6190 loss: 0.0000005717 time: 0.06246352195739746\n",
      "Iteration: 6200 loss: 0.0000005618 time: 0.06594347953796387\n",
      "Iteration: 6210 loss: 0.0000005520 time: 0.06562519073486328\n",
      "Iteration: 6220 loss: 0.0000005422 time: 0.07047367095947266\n",
      "Iteration: 6230 loss: 0.0000005326 time: 0.06879925727844238\n",
      "Iteration: 6240 loss: 0.0000005231 time: 0.0626821517944336\n",
      "Iteration: 6250 loss: 0.0000005137 time: 0.06782722473144531\n",
      "Iteration: 6260 loss: 0.0000005044 time: 0.06988692283630371\n",
      "Iteration: 6270 loss: 0.0000004952 time: 0.06888294219970703\n",
      "Iteration: 6280 loss: 0.0000004861 time: 0.05937361717224121\n",
      "Iteration: 6290 loss: 0.0000004771 time: 0.06378936767578125\n",
      "Iteration: 6300 loss: 0.0000004682 time: 0.07270598411560059\n",
      "Iteration: 6310 loss: 0.0000004595 time: 0.07357239723205566\n",
      "Iteration: 6320 loss: 0.0000004508 time: 0.07260918617248535\n",
      "Iteration: 6330 loss: 0.0000004422 time: 0.08017182350158691\n",
      "Iteration: 6340 loss: 0.0000004338 time: 0.06432557106018066\n",
      "Iteration: 6350 loss: 0.0000004255 time: 0.06191754341125488\n",
      "Iteration: 6360 loss: 0.0000004172 time: 0.0694117546081543\n",
      "Iteration: 6370 loss: 0.0000004091 time: 0.06796455383300781\n",
      "Iteration: 6380 loss: 0.0000004010 time: 0.07200002670288086\n",
      "Iteration: 6390 loss: 0.0000003931 time: 0.07223987579345703\n",
      "Iteration: 6400 loss: 0.0000003853 time: 0.06594491004943848\n",
      "Iteration: 6410 loss: 0.0000003776 time: 0.07419252395629883\n",
      "Iteration: 6420 loss: 0.0000003700 time: 0.07052087783813477\n",
      "Iteration: 6430 loss: 0.0000003625 time: 0.06952548027038574\n",
      "Iteration: 6440 loss: 0.0000003551 time: 0.0717780590057373\n",
      "Iteration: 6450 loss: 0.0000003478 time: 0.07034683227539062\n",
      "Iteration: 6460 loss: 0.0000003406 time: 0.06618022918701172\n",
      "Iteration: 6470 loss: 0.0000003335 time: 0.06582283973693848\n",
      "Iteration: 6480 loss: 0.0000003265 time: 0.06305599212646484\n",
      "Iteration: 6490 loss: 0.0000003196 time: 0.06455111503601074\n",
      "Iteration: 6500 loss: 0.0000003128 time: 0.06453275680541992\n",
      "Iteration: 6510 loss: 0.0000003061 time: 0.058598995208740234\n",
      "Iteration: 6520 loss: 0.0000002995 time: 0.06988644599914551\n",
      "Iteration: 6530 loss: 0.0000002930 time: 0.06115317344665527\n",
      "Iteration: 6540 loss: 0.0000002866 time: 0.06936478614807129\n",
      "Iteration: 6550 loss: 0.0000002803 time: 0.07018041610717773\n",
      "Iteration: 6560 loss: 0.0000002741 time: 0.06815695762634277\n",
      "Iteration: 6570 loss: 0.0000002680 time: 0.06936812400817871\n",
      "Iteration: 6580 loss: 0.0000002620 time: 0.06748676300048828\n",
      "Iteration: 6590 loss: 0.0000002561 time: 0.06577944755554199\n",
      "Iteration: 6600 loss: 0.0000002503 time: 0.06243324279785156\n",
      "Iteration: 6610 loss: 0.0000002446 time: 0.07070684432983398\n",
      "Iteration: 6620 loss: 0.0000002390 time: 0.06937980651855469\n",
      "Iteration: 6630 loss: 0.0000002334 time: 0.07060456275939941\n",
      "Iteration: 6640 loss: 0.0000002280 time: 0.07096385955810547\n",
      "Iteration: 6650 loss: 0.0000002227 time: 0.07086634635925293\n",
      "Iteration: 6660 loss: 0.0000002174 time: 0.07056021690368652\n",
      "Iteration: 6670 loss: 0.0000002122 time: 0.06969642639160156\n",
      "Iteration: 6680 loss: 0.0000002072 time: 0.0716698169708252\n",
      "Iteration: 6690 loss: 0.0000002022 time: 0.07028341293334961\n",
      "Iteration: 6700 loss: 0.0000001973 time: 0.0708322525024414\n",
      "Iteration: 6710 loss: 0.0000001925 time: 0.07176661491394043\n",
      "Iteration: 6720 loss: 0.0000001878 time: 0.06338882446289062\n",
      "Iteration: 6730 loss: 0.0000001831 time: 0.06768107414245605\n",
      "Iteration: 6740 loss: 0.0000001786 time: 0.07223224639892578\n",
      "Iteration: 6750 loss: 0.0000001741 time: 0.06493449211120605\n",
      "Iteration: 6760 loss: 0.0000001697 time: 0.06728100776672363\n",
      "Iteration: 6770 loss: 0.0000001654 time: 0.06702446937561035\n",
      "Iteration: 6780 loss: 0.0000001612 time: 0.07080721855163574\n",
      "Iteration: 6790 loss: 0.0000001571 time: 0.07009339332580566\n",
      "Iteration: 6800 loss: 0.0000001530 time: 0.07092452049255371\n",
      "Iteration: 6810 loss: 0.0000001491 time: 0.06664156913757324\n",
      "Iteration: 6820 loss: 0.0000001452 time: 0.06580901145935059\n",
      "Iteration: 6830 loss: 0.0000001413 time: 0.06588149070739746\n",
      "Iteration: 6840 loss: 0.0000001376 time: 0.06997013092041016\n",
      "Iteration: 6850 loss: 0.0000001339 time: 0.0673060417175293\n",
      "Iteration: 6860 loss: 0.0000001303 time: 0.07288646697998047\n",
      "Iteration: 6870 loss: 0.0000001268 time: 0.07176852226257324\n",
      "Iteration: 6880 loss: 0.0000001234 time: 0.0731050968170166\n",
      "Iteration: 6890 loss: 0.0000001200 time: 0.07403254508972168\n",
      "Iteration: 6900 loss: 0.0000001167 time: 0.0617830753326416\n",
      "Iteration: 6910 loss: 0.0000001135 time: 0.06346344947814941\n",
      "Iteration: 6920 loss: 0.0000001103 time: 0.0676279067993164\n",
      "Iteration: 6930 loss: 0.0000001072 time: 0.06672787666320801\n",
      "Iteration: 6940 loss: 0.0000001042 time: 0.06660199165344238\n",
      "Iteration: 6950 loss: 0.0000001013 time: 0.07527685165405273\n",
      "Iteration: 6960 loss: 0.0000000984 time: 0.07519006729125977\n",
      "Iteration: 6970 loss: 0.0000000955 time: 0.07329368591308594\n",
      "Iteration: 6980 loss: 0.0000000928 time: 0.07257652282714844\n",
      "Iteration: 6990 loss: 0.0000000901 time: 0.07498860359191895\n",
      "Iteration: 7000 loss: 0.0000000875 time: 0.07523989677429199\n",
      "Iteration: 7010 loss: 0.0000000849 time: 0.07573127746582031\n",
      "Iteration: 7020 loss: 0.0000000824 time: 0.07622838020324707\n",
      "Iteration: 7030 loss: 0.0000000799 time: 0.06710124015808105\n",
      "Iteration: 7040 loss: 0.0000000775 time: 0.06769347190856934\n",
      "Iteration: 7050 loss: 0.0000000752 time: 0.07258176803588867\n",
      "Iteration: 7060 loss: 0.0000000729 time: 0.06532931327819824\n",
      "Iteration: 7070 loss: 0.0000000707 time: 0.06755328178405762\n",
      "Iteration: 7080 loss: 0.0000000685 time: 0.06596660614013672\n",
      "Iteration: 7090 loss: 0.0000000664 time: 0.06982970237731934\n",
      "Iteration: 7100 loss: 0.0000000643 time: 0.06704044342041016\n",
      "Iteration: 7110 loss: 0.0000000623 time: 0.06686067581176758\n",
      "Iteration: 7120 loss: 0.0000000604 time: 0.06734395027160645\n",
      "Iteration: 7130 loss: 0.0000000584 time: 0.07236909866333008\n",
      "Iteration: 7140 loss: 0.0000000566 time: 0.0677192211151123\n",
      "Iteration: 7150 loss: 0.0000000548 time: 0.06334543228149414\n",
      "Iteration: 7160 loss: 0.0000000530 time: 0.0638265609741211\n",
      "Iteration: 7170 loss: 0.0000000513 time: 0.06654500961303711\n",
      "Iteration: 7180 loss: 0.0000000496 time: 0.06529045104980469\n",
      "Iteration: 7190 loss: 0.0000000480 time: 0.061739206314086914\n",
      "Iteration: 7200 loss: 0.0000000464 time: 0.05608725547790527\n",
      "Iteration: 7210 loss: 0.0000000449 time: 0.06091737747192383\n",
      "Iteration: 7220 loss: 0.0000000434 time: 0.0654604434967041\n",
      "Iteration: 7230 loss: 0.0000000419 time: 0.06085205078125\n",
      "Iteration: 7240 loss: 0.0000000405 time: 0.06060290336608887\n",
      "Iteration: 7250 loss: 0.0000000391 time: 0.06142282485961914\n",
      "Iteration: 7260 loss: 0.0000000378 time: 0.062094688415527344\n",
      "Iteration: 7270 loss: 0.0000000365 time: 0.06432723999023438\n",
      "Iteration: 7280 loss: 0.0000000352 time: 0.06861019134521484\n",
      "Iteration: 7290 loss: 0.0000000340 time: 0.06687450408935547\n",
      "Iteration: 7300 loss: 0.0000000328 time: 0.07297205924987793\n",
      "Iteration: 7310 loss: 0.0000000316 time: 0.06333065032958984\n",
      "Iteration: 7320 loss: 0.0000000305 time: 0.06905746459960938\n",
      "Iteration: 7330 loss: 0.0000000294 time: 0.07227277755737305\n",
      "Iteration: 7340 loss: 0.0000000284 time: 0.06844186782836914\n",
      "Iteration: 7350 loss: 0.0000000274 time: 0.06719756126403809\n",
      "Iteration: 7360 loss: 0.0000000264 time: 0.06585931777954102\n",
      "Iteration: 7370 loss: 0.0000000254 time: 0.07114434242248535\n",
      "Iteration: 7380 loss: 0.0000000245 time: 0.0664210319519043\n",
      "Iteration: 7390 loss: 0.0000000236 time: 0.07219696044921875\n",
      "Iteration: 7400 loss: 0.0000000227 time: 0.07250523567199707\n",
      "Iteration: 7410 loss: 0.0000000218 time: 0.07174944877624512\n",
      "Iteration: 7420 loss: 0.0000000210 time: 0.07074546813964844\n",
      "Iteration: 7430 loss: 0.0000000202 time: 0.06255674362182617\n",
      "Iteration: 7440 loss: 0.0000000195 time: 0.06945514678955078\n",
      "Iteration: 7450 loss: 0.0000000187 time: 0.06745004653930664\n",
      "Iteration: 7460 loss: 0.0000000180 time: 0.0661478042602539\n",
      "Iteration: 7470 loss: 0.0000000173 time: 0.06613492965698242\n",
      "Iteration: 7480 loss: 0.0000000166 time: 0.06750631332397461\n",
      "Iteration: 7490 loss: 0.0000000160 time: 0.0667877197265625\n",
      "Iteration: 7500 loss: 0.0000000154 time: 0.07043099403381348\n",
      "Iteration: 7510 loss: 0.0000000147 time: 0.0644838809967041\n",
      "Iteration: 7520 loss: 0.0000000142 time: 0.0683739185333252\n",
      "Iteration: 7530 loss: 0.0000000136 time: 0.06836152076721191\n",
      "Iteration: 7540 loss: 0.0000000130 time: 0.06766915321350098\n",
      "Iteration: 7550 loss: 0.0000000125 time: 0.0699622631072998\n",
      "Iteration: 7560 loss: 0.0000000120 time: 0.06843042373657227\n",
      "Iteration: 7570 loss: 0.0000000115 time: 0.06783652305603027\n",
      "Iteration: 7580 loss: 0.0000000110 time: 0.0712270736694336\n",
      "Iteration: 7590 loss: 0.0000000106 time: 0.06304049491882324\n",
      "Iteration: 7600 loss: 0.0000000102 time: 0.06673693656921387\n",
      "Iteration: 7610 loss: 0.0000000097 time: 0.07722830772399902\n",
      "Iteration: 7620 loss: 0.0000000093 time: 0.0709991455078125\n",
      "Iteration: 7630 loss: 0.0000000089 time: 0.07318520545959473\n",
      "Iteration: 7640 loss: 0.0000000085 time: 0.07209634780883789\n",
      "Iteration: 7650 loss: 0.0000000082 time: 0.07540416717529297\n",
      "Iteration: 7660 loss: 0.0000000078 time: 0.07195186614990234\n",
      "Iteration: 7670 loss: 0.0000000075 time: 0.0696721076965332\n",
      "Iteration: 7680 loss: 0.0000000072 time: 0.07096409797668457\n",
      "Iteration: 7690 loss: 0.0000000069 time: 0.07342362403869629\n",
      "Iteration: 7700 loss: 0.0000000066 time: 0.07319855690002441\n",
      "Iteration: 7710 loss: 0.0000000063 time: 0.07467389106750488\n",
      "Iteration: 7720 loss: 0.0000000060 time: 0.07260799407958984\n",
      "Iteration: 7730 loss: 0.0000000057 time: 0.0726923942565918\n",
      "Iteration: 7740 loss: 0.0000000055 time: 0.06718587875366211\n",
      "Iteration: 7750 loss: 0.0000000052 time: 0.057340383529663086\n",
      "Iteration: 7760 loss: 0.0000000050 time: 0.06831812858581543\n",
      "Iteration: 7770 loss: 0.0000000048 time: 0.07280588150024414\n",
      "Iteration: 7780 loss: 0.0000000045 time: 0.07069063186645508\n",
      "Iteration: 7790 loss: 0.0000000043 time: 0.07046699523925781\n",
      "Iteration: 7800 loss: 0.0000000041 time: 0.06359100341796875\n",
      "Iteration: 7810 loss: 0.0000000039 time: 0.06094789505004883\n",
      "Iteration: 7820 loss: 0.0000000037 time: 0.07461380958557129\n",
      "Iteration: 7830 loss: 0.0000000036 time: 0.07152414321899414\n",
      "Iteration: 7840 loss: 0.0000000034 time: 0.07538127899169922\n",
      "Iteration: 7850 loss: 0.0000000032 time: 0.07265758514404297\n",
      "Iteration: 7860 loss: 0.0000000031 time: 0.06815481185913086\n",
      "Iteration: 7870 loss: 0.0000000029 time: 0.06614375114440918\n",
      "Iteration: 7880 loss: 0.0000000028 time: 0.06337237358093262\n",
      "Iteration: 7890 loss: 0.0000000027 time: 0.05844569206237793\n",
      "Iteration: 7900 loss: 0.0000000025 time: 0.07102322578430176\n",
      "Iteration: 7910 loss: 0.0000000024 time: 0.06806540489196777\n",
      "Iteration: 7920 loss: 0.0000000023 time: 0.06927037239074707\n",
      "Iteration: 7930 loss: 0.0000000022 time: 0.07182741165161133\n",
      "Iteration: 7940 loss: 0.0000000021 time: 0.07265353202819824\n",
      "Iteration: 7950 loss: 0.0000000020 time: 0.07090640068054199\n",
      "Iteration: 7960 loss: 0.0000000019 time: 0.07257604598999023\n",
      "Iteration: 7970 loss: 0.0000000018 time: 0.07526397705078125\n",
      "Iteration: 7980 loss: 0.0000000017 time: 0.06533145904541016\n",
      "Iteration: 7990 loss: 0.0000000016 time: 0.06809425354003906\n",
      "Iteration: 8000 loss: 0.0000000015 time: 0.07094407081604004\n",
      "Iteration: 8010 loss: 0.0000000014 time: 0.06661367416381836\n",
      "Iteration: 8020 loss: 0.0000000014 time: 0.0696101188659668\n",
      "Iteration: 8030 loss: 0.0000000013 time: 0.0725717544555664\n",
      "Iteration: 8040 loss: 0.0000000012 time: 0.0701899528503418\n",
      "Iteration: 8050 loss: 0.0000000012 time: 0.07003188133239746\n",
      "Iteration: 8060 loss: 0.0000000011 time: 0.06654691696166992\n",
      "Iteration: 8070 loss: 0.0000000010 time: 0.0638742446899414\n",
      "Iteration: 8080 loss: 0.0000000010 time: 0.06602668762207031\n",
      "Iteration: 8090 loss: 0.0000000009 time: 0.0675053596496582\n",
      "Iteration: 8100 loss: 0.0000000009 time: 0.06374049186706543\n",
      "Iteration: 8110 loss: 0.0000000008 time: 0.06320667266845703\n",
      "Iteration: 8120 loss: 0.0000000008 time: 0.0684356689453125\n",
      "Iteration: 8130 loss: 0.0000000007 time: 0.07094550132751465\n",
      "Iteration: 8140 loss: 0.0000000007 time: 0.06992030143737793\n",
      "Iteration: 8150 loss: 0.0000000007 time: 0.0650331974029541\n",
      "Iteration: 8160 loss: 0.0000000006 time: 0.06475520133972168\n",
      "Iteration: 8170 loss: 0.0000000006 time: 0.06808757781982422\n",
      "Iteration: 8180 loss: 0.0000000006 time: 0.06615686416625977\n",
      "Iteration: 8190 loss: 0.0000000005 time: 0.06958937644958496\n",
      "Iteration: 8200 loss: 0.0000000005 time: 0.07462120056152344\n",
      "Iteration: 8210 loss: 0.0000000005 time: 0.07461690902709961\n",
      "Iteration: 8220 loss: 0.0000000004 time: 0.07401418685913086\n",
      "Iteration: 8230 loss: 0.0000000004 time: 0.07188606262207031\n",
      "Iteration: 8240 loss: 0.0000000004 time: 0.07503795623779297\n",
      "Iteration: 8250 loss: 0.0000000004 time: 0.07056689262390137\n",
      "Iteration: 8260 loss: 0.0000000003 time: 0.0739588737487793\n",
      "Iteration: 8270 loss: 0.0000000003 time: 0.06971168518066406\n",
      "Iteration: 8280 loss: 0.0000000003 time: 0.05943751335144043\n",
      "Iteration: 8290 loss: 0.0000000003 time: 0.07033467292785645\n",
      "Iteration: 8300 loss: 0.0000000003 time: 0.06927180290222168\n",
      "Iteration: 8310 loss: 0.0000000003 time: 0.074737548828125\n",
      "Iteration: 8320 loss: 0.0000000002 time: 0.07577824592590332\n",
      "Iteration: 8330 loss: 0.0000000002 time: 0.06432056427001953\n",
      "Iteration: 8340 loss: 0.0000000002 time: 0.0682382583618164\n",
      "Iteration: 8350 loss: 0.0000000002 time: 0.06147336959838867\n",
      "Iteration: 8360 loss: 0.0000000002 time: 0.07106423377990723\n",
      "Iteration: 8370 loss: 0.0000000002 time: 0.07567501068115234\n",
      "Iteration: 8380 loss: 0.0000000002 time: 0.0646672248840332\n",
      "Iteration: 8390 loss: 0.0000000002 time: 0.06788396835327148\n",
      "Iteration: 8400 loss: 0.0000000001 time: 0.06649279594421387\n",
      "Iteration: 8410 loss: 0.0000000001 time: 0.06936216354370117\n",
      "Iteration: 8420 loss: 0.0000000001 time: 0.0718388557434082\n",
      "Iteration: 8430 loss: 0.0000000001 time: 0.06778812408447266\n",
      "Iteration: 8440 loss: 0.0000000001 time: 0.06935930252075195\n",
      "Iteration: 8450 loss: 0.0000000001 time: 0.07179975509643555\n",
      "Iteration: 8460 loss: 0.0000000001 time: 0.07309460639953613\n",
      "Iteration: 8470 loss: 0.0000000001 time: 0.058982133865356445\n",
      "Iteration: 8480 loss: 0.0000000001 time: 0.0655219554901123\n",
      "Iteration: 8490 loss: 0.0000000001 time: 0.06316184997558594\n",
      "Iteration: 8500 loss: 0.0000000001 time: 0.06822967529296875\n",
      "Iteration: 8510 loss: 0.0000000001 time: 0.0708310604095459\n",
      "Iteration: 8520 loss: 0.0000000001 time: 0.06980037689208984\n",
      "Iteration: 8530 loss: 0.0000000001 time: 0.07298684120178223\n",
      "Iteration: 8540 loss: 0.0000000001 time: 0.07490968704223633\n",
      "Iteration: 8550 loss: 0.0000000001 time: 0.06786704063415527\n",
      "Iteration: 8560 loss: 0.0000000000 time: 0.06628704071044922\n",
      "Iteration: 8570 loss: 0.0000000000 time: 0.06677532196044922\n",
      "Iteration: 8580 loss: 0.0000000000 time: 0.07331371307373047\n",
      "Iteration: 8590 loss: 0.0000000000 time: 0.06241774559020996\n",
      "Iteration: 8600 loss: 0.0000000000 time: 0.07278060913085938\n",
      "Iteration: 8610 loss: 0.0000000000 time: 0.07328915596008301\n",
      "Iteration: 8620 loss: 0.0000000000 time: 0.0742957592010498\n",
      "Iteration: 8630 loss: 0.0000000000 time: 0.07597589492797852\n",
      "Iteration: 8640 loss: 0.0000000000 time: 0.06543707847595215\n",
      "Iteration: 8650 loss: 0.0000000000 time: 0.07085776329040527\n",
      "Iteration: 8660 loss: 0.0000000000 time: 0.06803131103515625\n",
      "Iteration: 8670 loss: 0.0000000000 time: 0.0686643123626709\n",
      "Iteration: 8680 loss: 0.0000000000 time: 0.0712289810180664\n",
      "Iteration: 8690 loss: 0.0000000000 time: 0.07044672966003418\n",
      "Iteration: 8700 loss: 0.0000000000 time: 0.07093334197998047\n",
      "Iteration: 8710 loss: 0.0000000000 time: 0.07151126861572266\n",
      "Iteration: 8720 loss: 0.0000000000 time: 0.0652153491973877\n",
      "Iteration: 8730 loss: 0.0000000000 time: 0.06966257095336914\n",
      "Iteration: 8740 loss: 0.0000000000 time: 0.07394051551818848\n",
      "Iteration: 8750 loss: 0.0000000000 time: 0.07280325889587402\n",
      "Iteration: 8760 loss: 0.0000000000 time: 0.06398725509643555\n",
      "Iteration: 8770 loss: 0.0000000000 time: 0.06993722915649414\n",
      "Iteration: 8780 loss: 0.0000000000 time: 0.05896615982055664\n",
      "Iteration: 8790 loss: 0.0000000000 time: 0.06295347213745117\n",
      "Iteration: 8800 loss: 0.0000000000 time: 0.06614470481872559\n",
      "Iteration: 8810 loss: 0.0000000000 time: 0.06761503219604492\n",
      "Iteration: 8820 loss: 0.0000000000 time: 0.07452011108398438\n",
      "Iteration: 8830 loss: 0.0000000000 time: 0.06142377853393555\n",
      "Iteration: 8840 loss: 0.0000000000 time: 0.06316757202148438\n",
      "Iteration: 8850 loss: 0.0000000000 time: 0.06567072868347168\n",
      "Iteration: 8860 loss: 0.0000000000 time: 0.0733175277709961\n",
      "Iteration: 8870 loss: 0.0000000000 time: 0.07429862022399902\n",
      "Iteration: 8880 loss: 0.0000000000 time: 0.07322001457214355\n",
      "Iteration: 8890 loss: 0.0000000000 time: 0.07182168960571289\n",
      "Iteration: 8900 loss: 0.0000000000 time: 0.07300615310668945\n",
      "Iteration: 8910 loss: 0.0000000000 time: 0.07233381271362305\n",
      "Iteration: 8920 loss: 0.0000000000 time: 0.07467293739318848\n",
      "Iteration: 8930 loss: 0.0000000000 time: 0.07081985473632812\n",
      "Iteration: 8940 loss: 0.0000000000 time: 0.07151103019714355\n",
      "Iteration: 8950 loss: 0.0000000000 time: 0.06889820098876953\n",
      "Iteration: 8960 loss: 0.0000000000 time: 0.07489418983459473\n",
      "Iteration: 8970 loss: 0.0000000000 time: 0.07055234909057617\n",
      "Iteration: 8980 loss: 0.0000000000 time: 0.06655216217041016\n",
      "Iteration: 8990 loss: 0.0000000000 time: 0.06924819946289062\n",
      "Iteration: 9000 loss: 0.0000000000 time: 0.07444071769714355\n",
      "Iteration: 9010 loss: 0.0000000000 time: 0.07358551025390625\n",
      "Iteration: 9020 loss: 0.0000000000 time: 0.07117605209350586\n",
      "Iteration: 9030 loss: 0.0000000000 time: 0.0596613883972168\n",
      "Iteration: 9040 loss: 0.0000000000 time: 0.06504249572753906\n",
      "Iteration: 9050 loss: 0.0000000000 time: 0.06588292121887207\n",
      "Iteration: 9060 loss: 0.0000000000 time: 0.06388664245605469\n",
      "Iteration: 9070 loss: 0.0000000000 time: 0.06954431533813477\n",
      "Iteration: 9080 loss: 0.0000000000 time: 0.0655519962310791\n",
      "Iteration: 9090 loss: 0.0000000000 time: 0.06771278381347656\n",
      "Iteration: 9100 loss: 0.0000000000 time: 0.06502532958984375\n",
      "Iteration: 9110 loss: 0.0000000000 time: 0.06856417655944824\n",
      "Iteration: 9120 loss: 0.0000000000 time: 0.06599044799804688\n",
      "Iteration: 9130 loss: 0.0000000000 time: 0.06929349899291992\n",
      "Iteration: 9140 loss: 0.0000000000 time: 0.06752753257751465\n",
      "Iteration: 9150 loss: 0.0000000000 time: 0.06791090965270996\n",
      "Iteration: 9160 loss: 0.0000000000 time: 0.06894302368164062\n",
      "Iteration: 9170 loss: 0.0000000000 time: 0.07326912879943848\n",
      "Iteration: 9180 loss: 0.0000000000 time: 0.07103562355041504\n",
      "Iteration: 9190 loss: 0.0000000000 time: 0.0727689266204834\n",
      "Iteration: 9200 loss: 0.0000000000 time: 0.06983160972595215\n",
      "Iteration: 9210 loss: 0.0000000000 time: 0.07352066040039062\n",
      "Iteration: 9220 loss: 0.0000000000 time: 0.07131743431091309\n",
      "Iteration: 9230 loss: 0.0000000000 time: 0.07489252090454102\n",
      "Iteration: 9240 loss: 0.0000000000 time: 0.07542943954467773\n",
      "Iteration: 9250 loss: 0.0000000000 time: 0.06837582588195801\n",
      "Iteration: 9260 loss: 0.0000000000 time: 0.070587158203125\n",
      "Iteration: 9270 loss: 0.0000000000 time: 0.0659494400024414\n",
      "Iteration: 9280 loss: 0.0000000000 time: 0.06598591804504395\n",
      "Iteration: 9290 loss: 0.0000000000 time: 0.06993842124938965\n",
      "Iteration: 9300 loss: 0.0000000000 time: 0.06610584259033203\n",
      "Iteration: 9310 loss: 0.0000000000 time: 0.06772565841674805\n",
      "Iteration: 9320 loss: 0.0000000000 time: 0.07008075714111328\n",
      "Iteration: 9330 loss: 0.0000000000 time: 0.06956601142883301\n",
      "Iteration: 9340 loss: 0.0000000000 time: 0.06908988952636719\n",
      "Iteration: 9350 loss: 0.0000000000 time: 0.0715639591217041\n",
      "Iteration: 9360 loss: 0.0000000000 time: 0.06331205368041992\n",
      "Iteration: 9370 loss: 0.0000000000 time: 0.06900429725646973\n",
      "Iteration: 9380 loss: 0.0000000000 time: 0.07310867309570312\n",
      "Iteration: 9390 loss: 0.0000000000 time: 0.06584048271179199\n",
      "Iteration: 9400 loss: 0.0000000000 time: 0.06980657577514648\n",
      "Iteration: 9410 loss: 0.0000000000 time: 0.06526374816894531\n",
      "Iteration: 9420 loss: 0.0000000000 time: 0.07100057601928711\n",
      "Iteration: 9430 loss: 0.0000000000 time: 0.06843256950378418\n",
      "Iteration: 9440 loss: 0.0000000000 time: 0.07306265830993652\n",
      "Iteration: 9450 loss: 0.0000000000 time: 0.0667872428894043\n",
      "Iteration: 9460 loss: 0.0000000000 time: 0.0714108943939209\n",
      "Iteration: 9470 loss: 0.0000000000 time: 0.06630444526672363\n",
      "Iteration: 9480 loss: 0.0000000000 time: 0.06622982025146484\n",
      "Iteration: 9490 loss: 0.0000000000 time: 0.06800389289855957\n",
      "Iteration: 9500 loss: 0.0000000000 time: 0.07034134864807129\n",
      "Iteration: 9510 loss: 0.0000000000 time: 0.06516575813293457\n",
      "Iteration: 9520 loss: 0.0000000000 time: 0.06656050682067871\n",
      "Iteration: 9530 loss: 0.0000000000 time: 0.07403159141540527\n",
      "Iteration: 9540 loss: 0.0000000000 time: 0.07361912727355957\n",
      "Iteration: 9550 loss: 0.0000000000 time: 0.07204604148864746\n",
      "Iteration: 9560 loss: 0.0000000000 time: 0.07102036476135254\n",
      "Iteration: 9570 loss: 0.0000000000 time: 0.0722498893737793\n",
      "Iteration: 9580 loss: 0.0000000000 time: 0.06971287727355957\n",
      "Iteration: 9590 loss: 0.0000000000 time: 0.07288575172424316\n",
      "Iteration: 9600 loss: 0.0000000000 time: 0.07487797737121582\n",
      "Iteration: 9610 loss: 0.0000000000 time: 0.07239866256713867\n",
      "Iteration: 9620 loss: 0.0000000000 time: 0.07401514053344727\n",
      "Iteration: 9630 loss: 0.0000000000 time: 0.07137632369995117\n",
      "Iteration: 9640 loss: 0.0000000000 time: 0.06907320022583008\n",
      "Iteration: 9650 loss: 0.0000000000 time: 0.06672835350036621\n",
      "Iteration: 9660 loss: 0.0000000000 time: 0.06685590744018555\n",
      "Iteration: 9670 loss: 0.0000000000 time: 0.07300877571105957\n",
      "Iteration: 9680 loss: 0.0000000000 time: 0.07286596298217773\n",
      "Iteration: 9690 loss: 0.0000000000 time: 0.07359623908996582\n",
      "Iteration: 9700 loss: 0.0000000000 time: 0.07147669792175293\n",
      "Iteration: 9710 loss: 0.0000000000 time: 0.07405352592468262\n",
      "Iteration: 9720 loss: 0.0000000000 time: 0.06916117668151855\n",
      "Iteration: 9730 loss: 0.0000000000 time: 0.07291913032531738\n",
      "Iteration: 9740 loss: 0.0000000000 time: 0.07027602195739746\n",
      "Iteration: 9750 loss: 0.0000000000 time: 0.07351398468017578\n",
      "Iteration: 9760 loss: 0.0000000000 time: 0.07149982452392578\n",
      "Iteration: 9770 loss: 0.0000000000 time: 0.07235980033874512\n",
      "Iteration: 9780 loss: 0.0000000000 time: 0.07628631591796875\n",
      "Iteration: 9790 loss: 0.0000000000 time: 0.07478523254394531\n",
      "Iteration: 9800 loss: 0.0000000000 time: 0.06677079200744629\n",
      "Iteration: 9810 loss: 0.0000000000 time: 0.06600022315979004\n",
      "Iteration: 9820 loss: 0.0000000000 time: 0.06639456748962402\n",
      "Iteration: 9830 loss: 0.0000000000 time: 0.0654599666595459\n",
      "Iteration: 9840 loss: 0.0000000000 time: 0.06859683990478516\n",
      "Iteration: 9850 loss: 0.0000000000 time: 0.06638860702514648\n",
      "Iteration: 9860 loss: 0.0000000000 time: 0.06925678253173828\n",
      "Iteration: 9870 loss: 0.0000000000 time: 0.0714881420135498\n",
      "Iteration: 9880 loss: 0.0000000000 time: 0.07466006278991699\n",
      "Iteration: 9890 loss: 0.0000000000 time: 0.06855392456054688\n",
      "Iteration: 9900 loss: 0.0000000000 time: 0.0677790641784668\n",
      "Iteration: 9910 loss: 0.0000000000 time: 0.05991101264953613\n",
      "Iteration: 9920 loss: 0.0000000000 time: 0.06082296371459961\n",
      "Iteration: 9930 loss: 0.0000000000 time: 0.06711173057556152\n",
      "Iteration: 9940 loss: 0.0000000000 time: 0.07068061828613281\n",
      "Iteration: 9950 loss: 0.0000000000 time: 0.07365989685058594\n",
      "Iteration: 9960 loss: 0.0000000000 time: 0.07176709175109863\n",
      "Iteration: 9970 loss: 0.0000000000 time: 0.06584811210632324\n",
      "Iteration: 9980 loss: 0.0000000000 time: 0.06866765022277832\n",
      "Iteration: 9990 loss: 0.0000000000 time: 0.0675208568572998\n",
      "Iteration: 10000 loss: 0.0000000000 time: 0.06715965270996094\n",
      "Iteration: 10010 loss: 0.0000000000 time: 0.06777215003967285\n",
      "Iteration: 10020 loss: 0.0000000000 time: 0.06702208518981934\n",
      "Iteration: 10030 loss: 0.0000000000 time: 0.06975030899047852\n",
      "Iteration: 10040 loss: 0.0000000000 time: 0.06847143173217773\n",
      "Iteration: 10050 loss: 0.0000000000 time: 0.06629729270935059\n",
      "Iteration: 10060 loss: 0.0000000000 time: 0.06724405288696289\n",
      "Iteration: 10070 loss: 0.0000000000 time: 0.06847548484802246\n",
      "Iteration: 10080 loss: 0.0000000000 time: 0.06440329551696777\n",
      "Iteration: 10090 loss: 0.0000000000 time: 0.0671849250793457\n",
      "Iteration: 10100 loss: 0.0000000000 time: 0.06761455535888672\n",
      "Iteration: 10110 loss: 0.0000000000 time: 0.0720367431640625\n",
      "Iteration: 10120 loss: 0.0000000000 time: 0.059271812438964844\n",
      "Iteration: 10130 loss: 0.0000000000 time: 0.06060314178466797\n",
      "Iteration: 10140 loss: 0.0000000000 time: 0.0596311092376709\n",
      "Iteration: 10150 loss: 0.0000000000 time: 0.06579804420471191\n",
      "Iteration: 10160 loss: 0.0000000000 time: 0.06726503372192383\n",
      "Iteration: 10170 loss: 0.0000000000 time: 0.06220221519470215\n",
      "Iteration: 10180 loss: 0.0000000000 time: 0.06707239151000977\n",
      "Iteration: 10190 loss: 0.0000000000 time: 0.06682991981506348\n",
      "Iteration: 10200 loss: 0.0000000000 time: 0.07058072090148926\n",
      "Iteration: 10210 loss: 0.0000000000 time: 0.06734132766723633\n",
      "Iteration: 10220 loss: 0.0000000000 time: 0.07346820831298828\n",
      "Iteration: 10230 loss: 0.0000000000 time: 0.06962728500366211\n",
      "Iteration: 10240 loss: 0.0000000000 time: 0.06855368614196777\n",
      "Iteration: 10250 loss: 0.0000000000 time: 0.06268954277038574\n",
      "Iteration: 10260 loss: 0.0000000000 time: 0.06539630889892578\n",
      "Iteration: 10270 loss: 0.0000000000 time: 0.07167911529541016\n",
      "Iteration: 10280 loss: 0.0000000000 time: 0.07388734817504883\n",
      "Iteration: 10290 loss: 0.0000000000 time: 0.06731009483337402\n",
      "Iteration: 10300 loss: 0.0000000000 time: 0.06679630279541016\n",
      "Iteration: 10310 loss: 0.0000000000 time: 0.06374549865722656\n",
      "Iteration: 10320 loss: 0.0000000000 time: 0.06609916687011719\n",
      "Iteration: 10330 loss: 0.0000000000 time: 0.06726574897766113\n",
      "Iteration: 10340 loss: 0.0000000000 time: 0.06708264350891113\n",
      "Iteration: 10350 loss: 0.0000000000 time: 0.06954097747802734\n",
      "Iteration: 10360 loss: 0.0000000000 time: 0.07602810859680176\n",
      "Iteration: 10370 loss: 0.0000000000 time: 0.07256627082824707\n",
      "Iteration: 10380 loss: 0.0000000000 time: 0.06380963325500488\n",
      "Iteration: 10390 loss: 0.0000000000 time: 0.06840157508850098\n",
      "Iteration: 10400 loss: 0.0000000000 time: 0.06738853454589844\n",
      "Iteration: 10410 loss: 0.0000000000 time: 0.06708598136901855\n",
      "Iteration: 10420 loss: 0.0000000000 time: 0.07367181777954102\n",
      "Iteration: 10430 loss: 0.0000000000 time: 0.06473088264465332\n",
      "Iteration: 10440 loss: 0.0000000000 time: 0.06796026229858398\n",
      "Iteration: 10450 loss: 0.0000000000 time: 0.06757473945617676\n",
      "Iteration: 10460 loss: 0.0000000000 time: 0.07283949851989746\n",
      "Iteration: 10470 loss: 0.0000000000 time: 0.07324743270874023\n",
      "Iteration: 10480 loss: 0.0000000000 time: 0.07545280456542969\n",
      "Iteration: 10490 loss: 0.0000000000 time: 0.07118463516235352\n",
      "Iteration: 10500 loss: 0.0000000000 time: 0.06638836860656738\n",
      "Iteration: 10510 loss: 0.0000000000 time: 0.072509765625\n",
      "Iteration: 10520 loss: 0.0000000000 time: 0.06882500648498535\n",
      "Iteration: 10530 loss: 0.0000000000 time: 0.07442879676818848\n",
      "Iteration: 10540 loss: 0.0000000000 time: 0.07515621185302734\n",
      "Iteration: 10550 loss: 0.0000000000 time: 0.06815338134765625\n",
      "Iteration: 10560 loss: 0.0000000000 time: 0.06912088394165039\n",
      "Iteration: 10570 loss: 0.0000000000 time: 0.06938695907592773\n",
      "Iteration: 10580 loss: 0.0000000000 time: 0.06758928298950195\n",
      "Iteration: 10590 loss: 0.0000000000 time: 0.06929445266723633\n",
      "Iteration: 10600 loss: 0.0000000000 time: 0.06712031364440918\n",
      "Iteration: 10610 loss: 0.0000000000 time: 0.06406879425048828\n",
      "Iteration: 10620 loss: 0.0000000000 time: 0.06388235092163086\n",
      "Iteration: 10630 loss: 0.0000000000 time: 0.06651926040649414\n",
      "Iteration: 10640 loss: 0.0000000000 time: 0.06475114822387695\n",
      "Iteration: 10650 loss: 0.0000000000 time: 0.06418251991271973\n",
      "Iteration: 10660 loss: 0.0000000000 time: 0.05977678298950195\n",
      "Iteration: 10670 loss: 0.0000000000 time: 0.05846047401428223\n",
      "Iteration: 10680 loss: 0.0000000000 time: 0.06576061248779297\n",
      "Iteration: 10690 loss: 0.0000000000 time: 0.06320476531982422\n",
      "Iteration: 10700 loss: 0.0000000000 time: 0.060051679611206055\n",
      "Iteration: 10710 loss: 0.0000000000 time: 0.07076168060302734\n",
      "Iteration: 10720 loss: 0.0000000000 time: 0.07288336753845215\n",
      "Iteration: 10730 loss: 0.0000000000 time: 0.06036829948425293\n",
      "Iteration: 10740 loss: 0.0000000000 time: 0.0633089542388916\n",
      "Iteration: 10750 loss: 0.0000000000 time: 0.07094669342041016\n",
      "Iteration: 10760 loss: 0.0000000000 time: 0.07325029373168945\n",
      "Iteration: 10770 loss: 0.0000000000 time: 0.06949019432067871\n",
      "Iteration: 10780 loss: 0.0000000000 time: 0.06951403617858887\n",
      "Iteration: 10790 loss: 0.0000000000 time: 0.07291793823242188\n",
      "Iteration: 10800 loss: 0.0000000000 time: 0.0662393569946289\n",
      "Iteration: 10810 loss: 0.0000000000 time: 0.062005043029785156\n",
      "Iteration: 10820 loss: 0.0000000000 time: 0.06599211692810059\n",
      "Iteration: 10830 loss: 0.0000000000 time: 0.07256865501403809\n",
      "Iteration: 10840 loss: 0.0000000000 time: 0.0702047348022461\n",
      "Iteration: 10850 loss: 0.0000000000 time: 0.07119202613830566\n",
      "Iteration: 10860 loss: 0.0000000000 time: 0.075286865234375\n",
      "Iteration: 10870 loss: 0.0000000000 time: 0.07269430160522461\n",
      "Iteration: 10880 loss: 0.0000000000 time: 0.07417607307434082\n",
      "Iteration: 10890 loss: 0.0000000000 time: 0.06961512565612793\n",
      "Iteration: 10900 loss: 0.0000000000 time: 0.06671833992004395\n",
      "Iteration: 10910 loss: 0.0000000000 time: 0.0674295425415039\n",
      "Iteration: 10920 loss: 0.0000000000 time: 0.07138395309448242\n",
      "Iteration: 10930 loss: 0.0000000000 time: 0.07210898399353027\n",
      "Iteration: 10940 loss: 0.0000000000 time: 0.06925749778747559\n",
      "Iteration: 10950 loss: 0.0000000000 time: 0.07059216499328613\n",
      "Iteration: 10960 loss: 0.0000000000 time: 0.06885504722595215\n",
      "Iteration: 10970 loss: 0.0000000000 time: 0.06806182861328125\n",
      "Iteration: 10980 loss: 0.0000000000 time: 0.06704926490783691\n",
      "Iteration: 10990 loss: 0.0000000000 time: 0.06644177436828613\n",
      "Iteration: 11000 loss: 0.0000000000 time: 0.0745401382446289\n",
      "Iteration: 11010 loss: 0.0000000000 time: 0.07390213012695312\n",
      "Iteration: 11020 loss: 0.0000000000 time: 0.07222723960876465\n",
      "Iteration: 11030 loss: 0.0000000000 time: 0.07165884971618652\n",
      "Iteration: 11040 loss: 0.0000000000 time: 0.07286572456359863\n",
      "Iteration: 11050 loss: 0.0000000000 time: 0.07175517082214355\n",
      "Iteration: 11060 loss: 0.0000000000 time: 0.07358932495117188\n",
      "Iteration: 11070 loss: 0.0000000000 time: 0.07545828819274902\n",
      "Iteration: 11080 loss: 0.0000000000 time: 0.07539558410644531\n",
      "Iteration: 11090 loss: 0.0000000000 time: 0.06075096130371094\n",
      "Iteration: 11100 loss: 0.0000000000 time: 0.06534457206726074\n",
      "Iteration: 11110 loss: 0.0000000000 time: 0.06272149085998535\n",
      "Iteration: 11120 loss: 0.0000000000 time: 0.0682682991027832\n",
      "Iteration: 11130 loss: 0.0000000000 time: 0.07038259506225586\n",
      "Iteration: 11140 loss: 0.0000000000 time: 0.0675811767578125\n",
      "Iteration: 11150 loss: 0.0000000003 time: 0.06961822509765625\n",
      "Iteration: 11160 loss: 0.0000002926 time: 0.0702657699584961\n",
      "Iteration: 11170 loss: 0.0000000516 time: 0.07287454605102539\n",
      "Iteration: 11180 loss: 0.0000000345 time: 0.06892108917236328\n",
      "Iteration: 11190 loss: 0.0000000071 time: 0.06739258766174316\n",
      "Iteration: 11200 loss: 0.0000000000 time: 0.06937003135681152\n",
      "Iteration: 11210 loss: 0.0000000005 time: 0.06628990173339844\n",
      "Iteration: 11220 loss: 0.0000000005 time: 0.06645631790161133\n",
      "Iteration: 11230 loss: 0.0000000002 time: 0.0698537826538086\n",
      "Iteration: 11240 loss: 0.0000000001 time: 0.06659817695617676\n",
      "Iteration: 11250 loss: 0.0000000000 time: 0.062072038650512695\n",
      "Iteration: 11260 loss: 0.0000000000 time: 0.06476855278015137\n",
      "Iteration: 11270 loss: 0.0000000000 time: 0.06536412239074707\n",
      "Iteration: 11280 loss: 0.0000000000 time: 0.06473278999328613\n",
      "Iteration: 11290 loss: 0.0000000000 time: 0.07200002670288086\n",
      "Iteration: 11300 loss: 0.0000000000 time: 0.07340621948242188\n",
      "Iteration: 11310 loss: 0.0000000000 time: 0.07529592514038086\n",
      "Iteration: 11320 loss: 0.0000000000 time: 0.07212972640991211\n",
      "Iteration: 11330 loss: 0.0000000000 time: 0.0712738037109375\n",
      "Iteration: 11340 loss: 0.0000000000 time: 0.06871604919433594\n",
      "Iteration: 11350 loss: 0.0000000000 time: 0.07014918327331543\n",
      "Iteration: 11360 loss: 0.0000000000 time: 0.07304501533508301\n",
      "Iteration: 11370 loss: 0.0000000000 time: 0.06978201866149902\n",
      "Iteration: 11380 loss: 0.0000000000 time: 0.07082247734069824\n",
      "Iteration: 11390 loss: 0.0000000000 time: 0.0712277889251709\n",
      "Iteration: 11400 loss: 0.0000000000 time: 0.07018756866455078\n",
      "Iteration: 11410 loss: 0.0000000000 time: 0.06980776786804199\n",
      "Iteration: 11420 loss: 0.0000000000 time: 0.07328128814697266\n",
      "Iteration: 11430 loss: 0.0000000000 time: 0.06850147247314453\n",
      "Iteration: 11440 loss: 0.0000000000 time: 0.06749963760375977\n",
      "Iteration: 11450 loss: 0.0000000000 time: 0.06685996055603027\n",
      "Iteration: 11460 loss: 0.0000000000 time: 0.07460498809814453\n",
      "Iteration: 11470 loss: 0.0000000000 time: 0.07472801208496094\n",
      "Iteration: 11480 loss: 0.0000000000 time: 0.06940269470214844\n",
      "Iteration: 11490 loss: 0.0000000000 time: 0.06757545471191406\n",
      "Iteration: 11500 loss: 0.0000000000 time: 0.06729483604431152\n",
      "Iteration: 11510 loss: 0.0000000000 time: 0.07418107986450195\n",
      "Iteration: 11520 loss: 0.0000000006 time: 0.06962704658508301\n",
      "Iteration: 11530 loss: 0.0000002349 time: 0.06912064552307129\n",
      "Iteration: 11540 loss: 0.0000000456 time: 0.07210564613342285\n",
      "Iteration: 11550 loss: 0.0000000040 time: 0.07082939147949219\n",
      "Iteration: 11560 loss: 0.0000000013 time: 0.06750369071960449\n",
      "Iteration: 11570 loss: 0.0000000034 time: 0.0696718692779541\n",
      "Iteration: 11580 loss: 0.0000000022 time: 0.06613850593566895\n",
      "Iteration: 11590 loss: 0.0000000009 time: 0.06958508491516113\n",
      "Iteration: 11600 loss: 0.0000000003 time: 0.06713294982910156\n",
      "Iteration: 11610 loss: 0.0000000001 time: 0.06506967544555664\n",
      "Iteration: 11620 loss: 0.0000000000 time: 0.06483983993530273\n",
      "Iteration: 11630 loss: 0.0000000000 time: 0.06937718391418457\n",
      "Iteration: 11640 loss: 0.0000000000 time: 0.06977391242980957\n",
      "Iteration: 11650 loss: 0.0000000000 time: 0.06088113784790039\n",
      "Iteration: 11660 loss: 0.0000000000 time: 0.06470012664794922\n",
      "Iteration: 11670 loss: 0.0000000000 time: 0.06951308250427246\n",
      "Iteration: 11680 loss: 0.0000000000 time: 0.06636476516723633\n",
      "Iteration: 11690 loss: 0.0000000000 time: 0.06568646430969238\n",
      "Iteration: 11700 loss: 0.0000000000 time: 0.06514143943786621\n",
      "Iteration: 11710 loss: 0.0000000000 time: 0.07458925247192383\n",
      "Iteration: 11720 loss: 0.0000000000 time: 0.071746826171875\n",
      "Iteration: 11730 loss: 0.0000000000 time: 0.07198405265808105\n",
      "Iteration: 11740 loss: 0.0000000000 time: 0.07180905342102051\n",
      "Iteration: 11750 loss: 0.0000000000 time: 0.07660126686096191\n",
      "Iteration: 11760 loss: 0.0000000000 time: 0.0728616714477539\n",
      "Iteration: 11770 loss: 0.0000000000 time: 0.06867742538452148\n",
      "Iteration: 11780 loss: 0.0000000000 time: 0.07564115524291992\n",
      "Iteration: 11790 loss: 0.0000000000 time: 0.0681159496307373\n",
      "Iteration: 11800 loss: 0.0000000000 time: 0.06625723838806152\n",
      "Iteration: 11810 loss: 0.0000000000 time: 0.06735682487487793\n",
      "Iteration: 11820 loss: 0.0000000000 time: 0.07083463668823242\n",
      "Iteration: 11830 loss: 0.0000000000 time: 0.06027340888977051\n",
      "Iteration: 11840 loss: 0.0000000000 time: 0.06818652153015137\n",
      "Iteration: 11850 loss: 0.0000000000 time: 0.07248759269714355\n",
      "Iteration: 11860 loss: 0.0000000000 time: 0.06787109375\n",
      "Iteration: 11870 loss: 0.0000000048 time: 0.07423830032348633\n",
      "Iteration: 11880 loss: 0.0000004753 time: 0.05944061279296875\n",
      "Iteration: 11890 loss: 0.0000001320 time: 0.06965780258178711\n",
      "Iteration: 11900 loss: 0.0000000497 time: 0.06765961647033691\n",
      "Iteration: 11910 loss: 0.0000000120 time: 0.06828546524047852\n",
      "Iteration: 11920 loss: 0.0000000014 time: 0.06706428527832031\n",
      "Iteration: 11930 loss: 0.0000000000 time: 0.06616616249084473\n",
      "Iteration: 11940 loss: 0.0000000000 time: 0.07080817222595215\n",
      "Iteration: 11950 loss: 0.0000000000 time: 0.06881332397460938\n",
      "Iteration: 11960 loss: 0.0000000000 time: 0.06949520111083984\n",
      "Iteration: 11970 loss: 0.0000000000 time: 0.06858158111572266\n",
      "Iteration: 11980 loss: 0.0000000000 time: 0.06716227531433105\n",
      "Iteration: 11990 loss: 0.0000000000 time: 0.06544804573059082\n",
      "Iteration: 12000 loss: 0.0000000000 time: 0.06797981262207031\n",
      "Iteration: 12010 loss: 0.0000000000 time: 0.06978106498718262\n",
      "Iteration: 12020 loss: 0.0000000000 time: 0.06919288635253906\n",
      "Iteration: 12030 loss: 0.0000000000 time: 0.07027769088745117\n",
      "Iteration: 12040 loss: 0.0000000000 time: 0.07037663459777832\n",
      "Iteration: 12050 loss: 0.0000000000 time: 0.06337165832519531\n",
      "Iteration: 12060 loss: 0.0000000000 time: 0.06962227821350098\n",
      "Iteration: 12070 loss: 0.0000000000 time: 0.06878352165222168\n",
      "Iteration: 12080 loss: 0.0000000000 time: 0.0730748176574707\n",
      "Iteration: 12090 loss: 0.0000000000 time: 0.07310771942138672\n",
      "Iteration: 12100 loss: 0.0000000000 time: 0.06564211845397949\n",
      "Iteration: 12110 loss: 0.0000000000 time: 0.07561016082763672\n",
      "Iteration: 12120 loss: 0.0000000000 time: 0.07290005683898926\n",
      "Iteration: 12130 loss: 0.0000000000 time: 0.07325983047485352\n",
      "Iteration: 12140 loss: 0.0000000000 time: 0.07248377799987793\n",
      "Iteration: 12150 loss: 0.0000000000 time: 0.0693979263305664\n",
      "Iteration: 12160 loss: 0.0000000000 time: 0.06575655937194824\n",
      "Iteration: 12170 loss: 0.0000000000 time: 0.06591558456420898\n",
      "Iteration: 12180 loss: 0.0000000000 time: 0.05958199501037598\n",
      "Iteration: 12190 loss: 0.0000000000 time: 0.07162594795227051\n",
      "Iteration: 12200 loss: 0.0000000000 time: 0.06728506088256836\n",
      "Iteration: 12210 loss: 0.0000000002 time: 0.07023024559020996\n",
      "Iteration: 12220 loss: 0.0000000599 time: 0.0708928108215332\n",
      "Iteration: 12230 loss: 0.0000001296 time: 0.07047200202941895\n",
      "Iteration: 12240 loss: 0.0000000510 time: 0.0680532455444336\n",
      "Iteration: 12250 loss: 0.0000000349 time: 0.07100057601928711\n",
      "Iteration: 12260 loss: 0.0000000120 time: 0.07647585868835449\n",
      "Iteration: 12270 loss: 0.0000000021 time: 0.06960082054138184\n",
      "Iteration: 12280 loss: 0.0000000002 time: 0.07137274742126465\n",
      "Iteration: 12290 loss: 0.0000000000 time: 0.07221269607543945\n",
      "Iteration: 12300 loss: 0.0000000000 time: 0.0654306411743164\n",
      "Iteration: 12310 loss: 0.0000000000 time: 0.06412053108215332\n",
      "Iteration: 12320 loss: 0.0000000000 time: 0.06551790237426758\n",
      "Iteration: 12330 loss: 0.0000000000 time: 0.06680440902709961\n",
      "Iteration: 12340 loss: 0.0000000000 time: 0.07049870491027832\n",
      "Iteration: 12350 loss: 0.0000000000 time: 0.07378363609313965\n",
      "Iteration: 12360 loss: 0.0000000000 time: 0.06848549842834473\n",
      "Iteration: 12370 loss: 0.0000000000 time: 0.07026386260986328\n",
      "Iteration: 12380 loss: 0.0000000000 time: 0.07035493850708008\n",
      "Iteration: 12390 loss: 0.0000000000 time: 0.07088828086853027\n",
      "Iteration: 12400 loss: 0.0000000000 time: 0.07066941261291504\n",
      "Iteration: 12410 loss: 0.0000000000 time: 0.07419657707214355\n",
      "Iteration: 12420 loss: 0.0000000000 time: 0.06983399391174316\n",
      "Iteration: 12430 loss: 0.0000000000 time: 0.07223725318908691\n",
      "Iteration: 12440 loss: 0.0000000000 time: 0.06656599044799805\n",
      "Iteration: 12450 loss: 0.0000000000 time: 0.07307195663452148\n",
      "Iteration: 12460 loss: 0.0000000000 time: 0.07571625709533691\n",
      "Iteration: 12470 loss: 0.0000000000 time: 0.06977534294128418\n",
      "Iteration: 12480 loss: 0.0000000000 time: 0.07139706611633301\n",
      "Iteration: 12490 loss: 0.0000000000 time: 0.06939888000488281\n",
      "Iteration: 12500 loss: 0.0000000000 time: 0.07309889793395996\n",
      "Iteration: 12510 loss: 0.0000000000 time: 0.06902575492858887\n",
      "Iteration: 12520 loss: 0.0000000000 time: 0.0686495304107666\n",
      "Iteration: 12530 loss: 0.0000000000 time: 0.07255220413208008\n",
      "Iteration: 12540 loss: 0.0000000000 time: 0.06973028182983398\n",
      "Iteration: 12550 loss: 0.0000000000 time: 0.06968474388122559\n",
      "Iteration: 12560 loss: 0.0000000000 time: 0.07129406929016113\n",
      "Iteration: 12570 loss: 0.0000000103 time: 0.06756162643432617\n",
      "Iteration: 12580 loss: 0.0000003673 time: 0.07014751434326172\n",
      "Iteration: 12590 loss: 0.0000001322 time: 0.06264710426330566\n",
      "Iteration: 12600 loss: 0.0000000110 time: 0.07039928436279297\n",
      "Iteration: 12610 loss: 0.0000000020 time: 0.07349300384521484\n",
      "Iteration: 12620 loss: 0.0000000055 time: 0.07401394844055176\n",
      "Iteration: 12630 loss: 0.0000000026 time: 0.0689396858215332\n",
      "Iteration: 12640 loss: 0.0000000006 time: 0.06812047958374023\n",
      "Iteration: 12650 loss: 0.0000000001 time: 0.06778883934020996\n",
      "Iteration: 12660 loss: 0.0000000000 time: 0.06970787048339844\n",
      "Iteration: 12670 loss: 0.0000000000 time: 0.0616605281829834\n",
      "Iteration: 12680 loss: 0.0000000000 time: 0.07769393920898438\n",
      "Iteration: 12690 loss: 0.0000000000 time: 0.073638916015625\n",
      "Iteration: 12700 loss: 0.0000000000 time: 0.07457494735717773\n",
      "Iteration: 12710 loss: 0.0000000000 time: 0.0677034854888916\n",
      "Iteration: 12720 loss: 0.0000000000 time: 0.06856131553649902\n",
      "Iteration: 12730 loss: 0.0000000000 time: 0.06945204734802246\n",
      "Iteration: 12740 loss: 0.0000000000 time: 0.06764650344848633\n",
      "Iteration: 12750 loss: 0.0000000000 time: 0.07285857200622559\n",
      "Iteration: 12760 loss: 0.0000000000 time: 0.06976866722106934\n",
      "Iteration: 12770 loss: 0.0000000000 time: 0.06868505477905273\n",
      "Iteration: 12780 loss: 0.0000000000 time: 0.06870150566101074\n",
      "Iteration: 12790 loss: 0.0000000000 time: 0.05675315856933594\n",
      "Iteration: 12800 loss: 0.0000000000 time: 0.06054973602294922\n",
      "Iteration: 12810 loss: 0.0000000000 time: 0.06027364730834961\n",
      "Iteration: 12820 loss: 0.0000000000 time: 0.07057738304138184\n",
      "Iteration: 12830 loss: 0.0000000000 time: 0.07004833221435547\n",
      "Iteration: 12840 loss: 0.0000000000 time: 0.0688023567199707\n",
      "Iteration: 12850 loss: 0.0000000000 time: 0.0718684196472168\n",
      "Iteration: 12860 loss: 0.0000000000 time: 0.06775903701782227\n",
      "Iteration: 12870 loss: 0.0000000000 time: 0.06814384460449219\n",
      "Iteration: 12880 loss: 0.0000000000 time: 0.07189702987670898\n",
      "Iteration: 12890 loss: 0.0000000000 time: 0.0678246021270752\n",
      "Iteration: 12900 loss: 0.0000000000 time: 0.07014846801757812\n",
      "Iteration: 12910 loss: 0.0000000000 time: 0.06530928611755371\n",
      "Iteration: 12920 loss: 0.0000000000 time: 0.07456302642822266\n",
      "Iteration: 12930 loss: 0.0000000000 time: 0.07210111618041992\n",
      "Iteration: 12940 loss: 0.0000000191 time: 0.07250404357910156\n",
      "Iteration: 12950 loss: 0.0000000724 time: 0.061307668685913086\n",
      "Iteration: 12960 loss: 0.0000000053 time: 0.060137271881103516\n",
      "Iteration: 12970 loss: 0.0000000248 time: 0.06444358825683594\n",
      "Iteration: 12980 loss: 0.0000000202 time: 0.06291508674621582\n",
      "Iteration: 12990 loss: 0.0000000015 time: 0.059595584869384766\n",
      "Iteration: 13000 loss: 0.0000000004 time: 0.06890106201171875\n",
      "Iteration: 13010 loss: 0.0000000008 time: 0.07084250450134277\n",
      "Iteration: 13020 loss: 0.0000000003 time: 0.06076788902282715\n",
      "Iteration: 13030 loss: 0.0000000001 time: 0.06224417686462402\n",
      "Iteration: 13040 loss: 0.0000000000 time: 0.07274270057678223\n",
      "Iteration: 13050 loss: 0.0000000000 time: 0.0693044662475586\n",
      "Iteration: 13060 loss: 0.0000000000 time: 0.07494759559631348\n",
      "Iteration: 13070 loss: 0.0000000000 time: 0.07061553001403809\n",
      "Iteration: 13080 loss: 0.0000000000 time: 0.07146883010864258\n",
      "Iteration: 13090 loss: 0.0000000000 time: 0.07106828689575195\n",
      "Iteration: 13100 loss: 0.0000000000 time: 0.06895732879638672\n",
      "Iteration: 13110 loss: 0.0000000000 time: 0.06907820701599121\n",
      "Iteration: 13120 loss: 0.0000000000 time: 0.07093286514282227\n",
      "Iteration: 13130 loss: 0.0000000000 time: 0.06958842277526855\n",
      "Iteration: 13140 loss: 0.0000000000 time: 0.07355642318725586\n",
      "Iteration: 13150 loss: 0.0000000000 time: 0.07520270347595215\n",
      "Iteration: 13160 loss: 0.0000000000 time: 0.07457685470581055\n",
      "Iteration: 13170 loss: 0.0000000000 time: 0.0725560188293457\n",
      "Iteration: 13180 loss: 0.0000000000 time: 0.06902813911437988\n",
      "Iteration: 13190 loss: 0.0000000000 time: 0.06374168395996094\n",
      "Iteration: 13200 loss: 0.0000000000 time: 0.06561422348022461\n",
      "Iteration: 13210 loss: 0.0000000000 time: 0.06772255897521973\n",
      "Iteration: 13220 loss: 0.0000000000 time: 0.07642459869384766\n",
      "Iteration: 13230 loss: 0.0000000000 time: 0.07407283782958984\n",
      "Iteration: 13240 loss: 0.0000000000 time: 0.07094335556030273\n",
      "Iteration: 13250 loss: 0.0000000000 time: 0.0676567554473877\n",
      "Iteration: 13260 loss: 0.0000000000 time: 0.06458449363708496\n",
      "Iteration: 13270 loss: 0.0000000000 time: 0.0638422966003418\n",
      "Iteration: 13280 loss: 0.0000000000 time: 0.059693098068237305\n",
      "Iteration: 13290 loss: 0.0000000000 time: 0.06366276741027832\n",
      "Iteration: 13300 loss: 0.0000000000 time: 0.06583547592163086\n",
      "Iteration: 13310 loss: 0.0000000000 time: 0.06950712203979492\n",
      "Iteration: 13320 loss: 0.0000000001 time: 0.0694272518157959\n",
      "Iteration: 13330 loss: 0.0000000284 time: 0.06789278984069824\n",
      "Iteration: 13340 loss: 0.0000000003 time: 0.07167339324951172\n",
      "Iteration: 13350 loss: 0.0000000308 time: 0.07214903831481934\n",
      "Iteration: 13360 loss: 0.0000000590 time: 0.06743192672729492\n",
      "Iteration: 13370 loss: 0.0000000043 time: 0.0665292739868164\n",
      "Iteration: 13380 loss: 0.0000000028 time: 0.06699919700622559\n",
      "Iteration: 13390 loss: 0.0000000026 time: 0.0663149356842041\n",
      "Iteration: 13400 loss: 0.0000000002 time: 0.06980299949645996\n",
      "Iteration: 13410 loss: 0.0000000000 time: 0.06924653053283691\n",
      "Iteration: 13420 loss: 0.0000000001 time: 0.06954312324523926\n",
      "Iteration: 13430 loss: 0.0000000000 time: 0.0749197006225586\n",
      "Iteration: 13440 loss: 0.0000000000 time: 0.06904983520507812\n",
      "Iteration: 13450 loss: 0.0000000000 time: 0.06827926635742188\n",
      "Iteration: 13460 loss: 0.0000000000 time: 0.06745600700378418\n",
      "Iteration: 13470 loss: 0.0000000000 time: 0.06789803504943848\n",
      "Iteration: 13480 loss: 0.0000000000 time: 0.06378769874572754\n",
      "Iteration: 13490 loss: 0.0000000000 time: 0.07496309280395508\n",
      "Iteration: 13500 loss: 0.0000000000 time: 0.07067084312438965\n",
      "Iteration: 13510 loss: 0.0000000000 time: 0.07036018371582031\n",
      "Iteration: 13520 loss: 0.0000000000 time: 0.0674448013305664\n",
      "Iteration: 13530 loss: 0.0000000000 time: 0.062001705169677734\n",
      "Iteration: 13540 loss: 0.0000000000 time: 0.0710756778717041\n",
      "Iteration: 13550 loss: 0.0000000000 time: 0.06634187698364258\n",
      "Iteration: 13560 loss: 0.0000000000 time: 0.07068395614624023\n",
      "Iteration: 13570 loss: 0.0000000000 time: 0.07320690155029297\n",
      "Iteration: 13580 loss: 0.0000000000 time: 0.07020258903503418\n",
      "Iteration: 13590 loss: 0.0000000000 time: 0.06941366195678711\n",
      "Iteration: 13600 loss: 0.0000000000 time: 0.06951189041137695\n",
      "Iteration: 13610 loss: 0.0000000000 time: 0.06896567344665527\n",
      "Iteration: 13620 loss: 0.0000000000 time: 0.07723569869995117\n",
      "Iteration: 13630 loss: 0.0000000000 time: 0.07629036903381348\n",
      "Iteration: 13640 loss: 0.0000000000 time: 0.0734713077545166\n",
      "Iteration: 13650 loss: 0.0000000000 time: 0.06754922866821289\n",
      "Iteration: 13660 loss: 0.0000000000 time: 0.06972002983093262\n",
      "Iteration: 13670 loss: 0.0000000000 time: 0.06477665901184082\n",
      "Iteration: 13680 loss: 0.0000000000 time: 0.06573271751403809\n",
      "Iteration: 13690 loss: 0.0000000000 time: 0.06913042068481445\n",
      "Iteration: 13700 loss: 0.0000000000 time: 0.07045555114746094\n",
      "Iteration: 13710 loss: 0.0000000000 time: 0.0714869499206543\n",
      "Iteration: 13720 loss: 0.0000000000 time: 0.06998944282531738\n",
      "Iteration: 13730 loss: 0.0000000030 time: 0.07130002975463867\n",
      "Iteration: 13740 loss: 0.0000008500 time: 0.0697481632232666\n",
      "Iteration: 13750 loss: 0.0000002188 time: 0.0678861141204834\n",
      "Iteration: 13760 loss: 0.0000000620 time: 0.07110309600830078\n",
      "Iteration: 13770 loss: 0.0000000017 time: 0.06989932060241699\n",
      "Iteration: 13780 loss: 0.0000000109 time: 0.06864428520202637\n",
      "Iteration: 13790 loss: 0.0000000010 time: 0.0735476016998291\n",
      "Iteration: 13800 loss: 0.0000000003 time: 0.06998634338378906\n",
      "Iteration: 13810 loss: 0.0000000005 time: 0.06842207908630371\n",
      "Iteration: 13820 loss: 0.0000000001 time: 0.07158493995666504\n",
      "Iteration: 13830 loss: 0.0000000000 time: 0.06850147247314453\n",
      "Iteration: 13840 loss: 0.0000000000 time: 0.07074356079101562\n",
      "Iteration: 13850 loss: 0.0000000000 time: 0.07381486892700195\n",
      "Iteration: 13860 loss: 0.0000000000 time: 0.06886053085327148\n",
      "Iteration: 13870 loss: 0.0000000000 time: 0.07124495506286621\n",
      "Iteration: 13880 loss: 0.0000000000 time: 0.07187175750732422\n",
      "Iteration: 13890 loss: 0.0000000000 time: 0.06981945037841797\n",
      "Iteration: 13900 loss: 0.0000000000 time: 0.06952810287475586\n",
      "Iteration: 13910 loss: 0.0000000000 time: 0.07102084159851074\n",
      "Iteration: 13920 loss: 0.0000000000 time: 0.07073688507080078\n",
      "Iteration: 13930 loss: 0.0000000000 time: 0.07339262962341309\n",
      "Iteration: 13940 loss: 0.0000000000 time: 0.07520842552185059\n",
      "Iteration: 13950 loss: 0.0000000000 time: 0.07248687744140625\n",
      "Iteration: 13960 loss: 0.0000000000 time: 0.07416009902954102\n",
      "Iteration: 13970 loss: 0.0000000000 time: 0.07134151458740234\n",
      "Iteration: 13980 loss: 0.0000000000 time: 0.07073688507080078\n",
      "Iteration: 13990 loss: 0.0000000000 time: 0.0702207088470459\n",
      "Iteration: 14000 loss: 0.0000000000 time: 0.06695938110351562\n",
      "Iteration: 14010 loss: 0.0000000000 time: 0.06781697273254395\n",
      "Iteration: 14020 loss: 0.0000000000 time: 0.06446576118469238\n",
      "Iteration: 14030 loss: 0.0000000000 time: 0.0689845085144043\n",
      "Iteration: 14040 loss: 0.0000000000 time: 0.0670316219329834\n",
      "Iteration: 14050 loss: 0.0000000000 time: 0.06626772880554199\n",
      "Iteration: 14060 loss: 0.0000000000 time: 0.07148098945617676\n",
      "Iteration: 14070 loss: 0.0000000000 time: 0.07135176658630371\n",
      "Iteration: 14080 loss: 0.0000000000 time: 0.07146477699279785\n",
      "Iteration: 14090 loss: 0.0000000000 time: 0.0623934268951416\n",
      "Iteration: 14100 loss: 0.0000000000 time: 0.06894707679748535\n",
      "Iteration: 14110 loss: 0.0000000000 time: 0.061005592346191406\n",
      "Iteration: 14120 loss: 0.0000000000 time: 0.06160449981689453\n",
      "Iteration: 14130 loss: 0.0000000000 time: 0.07376742362976074\n",
      "Iteration: 14140 loss: 0.0000000004 time: 0.07236576080322266\n",
      "Iteration: 14150 loss: 0.0000002588 time: 0.07087826728820801\n",
      "Iteration: 14160 loss: 0.0000002121 time: 0.07547593116760254\n",
      "Iteration: 14170 loss: 0.0000000092 time: 0.06316447257995605\n",
      "Iteration: 14180 loss: 0.0000000459 time: 0.07012581825256348\n",
      "Iteration: 14190 loss: 0.0000000002 time: 0.06903076171875\n",
      "Iteration: 14200 loss: 0.0000000059 time: 0.07233262062072754\n",
      "Iteration: 14210 loss: 0.0000000001 time: 0.06208610534667969\n",
      "Iteration: 14220 loss: 0.0000000005 time: 0.06391406059265137\n",
      "Iteration: 14230 loss: 0.0000000002 time: 0.06728148460388184\n",
      "Iteration: 14240 loss: 0.0000000000 time: 0.0676124095916748\n",
      "Iteration: 14250 loss: 0.0000000000 time: 0.06143808364868164\n",
      "Iteration: 14260 loss: 0.0000000000 time: 0.061895132064819336\n",
      "Iteration: 14270 loss: 0.0000000000 time: 0.07537007331848145\n",
      "Iteration: 14280 loss: 0.0000000000 time: 0.0672905445098877\n",
      "Iteration: 14290 loss: 0.0000000000 time: 0.06707620620727539\n",
      "Iteration: 14300 loss: 0.0000000000 time: 0.07414674758911133\n",
      "Iteration: 14310 loss: 0.0000000000 time: 0.06765937805175781\n",
      "Iteration: 14320 loss: 0.0000000000 time: 0.07056784629821777\n",
      "Iteration: 14330 loss: 0.0000000000 time: 0.0720524787902832\n",
      "Iteration: 14340 loss: 0.0000000000 time: 0.06764984130859375\n",
      "Iteration: 14350 loss: 0.0000000000 time: 0.07583880424499512\n",
      "Iteration: 14360 loss: 0.0000000000 time: 0.06384158134460449\n",
      "Iteration: 14370 loss: 0.0000000000 time: 0.06786251068115234\n",
      "Iteration: 14380 loss: 0.0000000000 time: 0.06359219551086426\n",
      "Iteration: 14390 loss: 0.0000000000 time: 0.06576251983642578\n",
      "Iteration: 14400 loss: 0.0000000000 time: 0.06312155723571777\n",
      "Iteration: 14410 loss: 0.0000000000 time: 0.07078981399536133\n",
      "Iteration: 14420 loss: 0.0000000000 time: 0.06566333770751953\n",
      "Iteration: 14430 loss: 0.0000000000 time: 0.06303238868713379\n",
      "Iteration: 14440 loss: 0.0000000000 time: 0.06682658195495605\n",
      "Iteration: 14450 loss: 0.0000000000 time: 0.06376934051513672\n",
      "Iteration: 14460 loss: 0.0000000000 time: 0.06895208358764648\n",
      "Iteration: 14470 loss: 0.0000000000 time: 0.0710601806640625\n",
      "Iteration: 14480 loss: 0.0000000000 time: 0.070587158203125\n",
      "Iteration: 14490 loss: 0.0000000000 time: 0.06516838073730469\n",
      "Iteration: 14500 loss: 0.0000000000 time: 0.06038522720336914\n",
      "Iteration: 14510 loss: 0.0000000000 time: 0.06840372085571289\n",
      "Iteration: 14520 loss: 0.0000000000 time: 0.06832194328308105\n",
      "Iteration: 14530 loss: 0.0000000000 time: 0.06389331817626953\n",
      "Iteration: 14540 loss: 0.0000000000 time: 0.06432986259460449\n",
      "Iteration: 14550 loss: 0.0000000000 time: 0.0667881965637207\n",
      "Iteration: 14560 loss: 0.0000000000 time: 0.06614804267883301\n",
      "Iteration: 14570 loss: 0.0000000000 time: 0.07209014892578125\n",
      "Iteration: 14580 loss: 0.0000000004 time: 0.06751441955566406\n",
      "Iteration: 14590 loss: 0.0000003051 time: 0.06635689735412598\n",
      "Iteration: 14600 loss: 0.0000001128 time: 0.0729362964630127\n",
      "Iteration: 14610 loss: 0.0000000652 time: 0.07175445556640625\n",
      "Iteration: 14620 loss: 0.0000000210 time: 0.07568740844726562\n",
      "Iteration: 14630 loss: 0.0000000128 time: 0.06641530990600586\n",
      "Iteration: 14640 loss: 0.0000000011 time: 0.07042527198791504\n",
      "Iteration: 14650 loss: 0.0000000018 time: 0.07178306579589844\n",
      "Iteration: 14660 loss: 0.0000000002 time: 0.07040524482727051\n",
      "Iteration: 14670 loss: 0.0000000002 time: 0.06876826286315918\n",
      "Iteration: 14680 loss: 0.0000000001 time: 0.06328535079956055\n",
      "Iteration: 14690 loss: 0.0000000000 time: 0.07220840454101562\n",
      "Iteration: 14700 loss: 0.0000000000 time: 0.07016420364379883\n",
      "Iteration: 14710 loss: 0.0000000000 time: 0.07111644744873047\n",
      "Iteration: 14720 loss: 0.0000000000 time: 0.06985735893249512\n",
      "Iteration: 14730 loss: 0.0000000000 time: 0.0687868595123291\n",
      "Iteration: 14740 loss: 0.0000000000 time: 0.062300920486450195\n",
      "Iteration: 14750 loss: 0.0000000000 time: 0.06797313690185547\n",
      "Iteration: 14760 loss: 0.0000000000 time: 0.06470942497253418\n",
      "Iteration: 14770 loss: 0.0000000000 time: 0.06813240051269531\n",
      "Iteration: 14780 loss: 0.0000000000 time: 0.06648755073547363\n",
      "Iteration: 14790 loss: 0.0000000000 time: 0.06722545623779297\n",
      "Iteration: 14800 loss: 0.0000000000 time: 0.06658291816711426\n",
      "Iteration: 14810 loss: 0.0000000000 time: 0.06737685203552246\n",
      "Iteration: 14820 loss: 0.0000000000 time: 0.06075477600097656\n",
      "Iteration: 14830 loss: 0.0000000000 time: 0.06413078308105469\n",
      "Iteration: 14840 loss: 0.0000000000 time: 0.06758642196655273\n",
      "Iteration: 14850 loss: 0.0000000000 time: 0.07259035110473633\n",
      "Iteration: 14860 loss: 0.0000000000 time: 0.07048416137695312\n",
      "Iteration: 14870 loss: 0.0000000000 time: 0.06889104843139648\n",
      "Iteration: 14880 loss: 0.0000000000 time: 0.06435084342956543\n",
      "Iteration: 14890 loss: 0.0000000000 time: 0.0747840404510498\n",
      "Iteration: 14900 loss: 0.0000000000 time: 0.07051253318786621\n",
      "Iteration: 14910 loss: 0.0000000000 time: 0.07109355926513672\n",
      "Iteration: 14920 loss: 0.0000000000 time: 0.06858444213867188\n",
      "Iteration: 14930 loss: 0.0000000000 time: 0.07459735870361328\n",
      "Iteration: 14940 loss: 0.0000000000 time: 0.0763251781463623\n",
      "Iteration: 14950 loss: 0.0000000000 time: 0.07626628875732422\n",
      "Iteration: 14960 loss: 0.0000000000 time: 0.07088994979858398\n",
      "Iteration: 14970 loss: 0.0000000000 time: 0.07161140441894531\n",
      "Iteration: 14980 loss: 0.0000000000 time: 0.07048320770263672\n",
      "Iteration: 14990 loss: 0.0000000000 time: 0.06049752235412598\n",
      "Iteration: 15000 loss: 0.0000000000 time: 0.06350064277648926\n",
      "-->mesh : \n",
      "     n_triangles :  64\n",
      "     n_vertices  :  41\n",
      "     n_edges     :  104\n",
      "     h_max           :  0.2500000000006653\n",
      "     h_min           :  0.1767766952961665\n",
      "-->test_fun      : \n",
      "     order       :  1\n",
      "     dof         :  25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 11:46:08.325349: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:46:08.329488: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:46:08.331141: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:46:08.332992: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:46:08.334707: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:46:08.336729: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:46:08.339390: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:46:08.342423: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:46:08.346050: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:46:08.348921: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:46:11.414425: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_26' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_26}}]]\n",
      "2023-12-28 11:46:11.414611: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_41' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_41}}]]\n",
      "2023-12-28 11:46:11.414687: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_58' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_58}}]]\n",
      "2023-12-28 11:46:11.414860: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_74' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_74}}]]\n",
      "2023-12-28 11:46:11.414972: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_94' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_94}}]]\n",
      "2023-12-28 11:46:11.415052: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_123' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_123}}]]\n",
      "2023-12-28 11:46:11.415117: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_141' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_141}}]]\n",
      "2023-12-28 11:46:11.415234: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_159' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_159}}]]\n",
      "2023-12-28 11:46:11.415343: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_164' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_164}}]]\n",
      "2023-12-28 11:46:11.415459: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_180' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_180}}]]\n",
      "2023-12-28 11:46:11.415567: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_198' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_198}}]]\n",
      "2023-12-28 11:46:11.415675: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_226' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_226}}]]\n",
      "2023-12-28 11:46:11.415783: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_234' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_234}}]]\n",
      "2023-12-28 11:46:11.415856: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_263' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_263}}]]\n",
      "2023-12-28 11:46:11.415962: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_281' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_281}}]]\n",
      "2023-12-28 11:46:11.416070: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_299' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_299}}]]\n",
      "2023-12-28 11:46:11.416179: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_304' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_304}}]]\n",
      "2023-12-28 11:46:11.416287: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_320' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_320}}]]\n",
      "2023-12-28 11:46:11.416397: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_336' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_336}}]]\n",
      "2023-12-28 11:46:11.416506: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_352' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_352}}]]\n",
      "2023-12-28 11:46:11.416618: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_372' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_372}}]]\n",
      "2023-12-28 11:46:11.416687: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_401' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_401}}]]\n",
      "2023-12-28 11:46:11.416751: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_419' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_419}}]]\n",
      "2023-12-28 11:46:11.416860: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_437' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_437}}]]\n",
      "2023-12-28 11:46:11.416970: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_442' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_442}}]]\n",
      "2023-12-28 11:46:11.417078: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_458' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_458}}]]\n",
      "2023-12-28 11:46:11.417148: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_476' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_476}}]]\n",
      "2023-12-28 11:46:11.417256: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_504' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_504}}]]\n",
      "2023-12-28 11:46:11.417365: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_512' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_512}}]]\n",
      "2023-12-28 11:46:11.417433: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_541' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_541}}]]\n",
      "2023-12-28 11:46:11.417542: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_559' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_559}}]]\n",
      "2023-12-28 11:46:11.417664: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_577' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_577}}]]\n",
      "2023-12-28 11:46:11.417774: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_582' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_582}}]]\n",
      "2023-12-28 11:46:11.417885: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_598' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_598}}]]\n",
      "2023-12-28 11:46:11.417994: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_614' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_614}}]]\n",
      "2023-12-28 11:46:11.418105: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_630' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_630}}]]\n",
      "2023-12-28 11:46:11.418213: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_650' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_650}}]]\n",
      "2023-12-28 11:46:11.418322: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_679' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_679}}]]\n",
      "2023-12-28 11:46:11.418432: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_697' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_697}}]]\n",
      "2023-12-28 11:46:11.418502: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_715' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_715}}]]\n",
      "2023-12-28 11:46:11.418612: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_720' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_720}}]]\n",
      "2023-12-28 11:46:11.418721: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_736' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_736}}]]\n",
      "2023-12-28 11:46:11.418832: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_754' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_754}}]]\n",
      "2023-12-28 11:46:11.418941: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_782' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_782}}]]\n",
      "2023-12-28 11:46:11.419011: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_790' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_790}}]]\n",
      "2023-12-28 11:46:11.419077: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_819' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_819}}]]\n",
      "2023-12-28 11:46:11.419186: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_837' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_837}}]]\n",
      "2023-12-28 11:46:11.419295: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_855' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_855}}]]\n",
      "2023-12-28 11:46:11.419404: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_860' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_860}}]]\n",
      "2023-12-28 11:46:11.419513: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_876' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_876}}]]\n",
      "2023-12-28 11:46:11.419623: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_892' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_892}}]]\n",
      "2023-12-28 11:46:11.419734: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_908' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_908}}]]\n",
      "2023-12-28 11:46:11.419843: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_928' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_928}}]]\n",
      "2023-12-28 11:46:11.419953: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_957' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_957}}]]\n",
      "2023-12-28 11:46:11.420063: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_975' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_975}}]]\n",
      "2023-12-28 11:46:11.420172: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_993' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_993}}]]\n",
      "2023-12-28 11:46:11.420283: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_998' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_998}}]]\n",
      "2023-12-28 11:46:11.420395: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1014' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1014}}]]\n",
      "2023-12-28 11:46:11.420505: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1032' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1032}}]]\n",
      "2023-12-28 11:46:11.420617: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1060' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1060}}]]\n",
      "2023-12-28 11:46:11.420730: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1068' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1068}}]]\n",
      "2023-12-28 11:46:11.420841: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1096' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1096}}]]\n",
      "2023-12-28 11:46:11.420948: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1107' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1107}}]]\n",
      "2023-12-28 11:46:11.421056: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1114' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1114}}]]\n",
      "2023-12-28 11:46:11.421164: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1119' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1119}}]]\n",
      "2023-12-28 11:46:11.421273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1122' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1122}}]]\n",
      "2023-12-28 11:46:11.421391: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1125' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1125}}]]\n",
      "2023-12-28 11:46:11.421500: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1128' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1128}}]]\n",
      "2023-12-28 11:46:11.421608: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1131' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1131}}]]\n",
      "2023-12-28 11:46:11.421716: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1134' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1134}}]]\n",
      "2023-12-28 11:46:11.421823: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1137' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1137}}]]\n",
      "2023-12-28 11:46:11.421931: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1140' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1140}}]]\n",
      "2023-12-28 11:46:11.422039: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1143' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1143}}]]\n",
      "2023-12-28 11:46:11.422148: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1146' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1146}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 loss: 0.0085826756 time: 11.898211240768433\n",
      "Iteration: 10 loss: 0.0071611359 time: 0.23240399360656738\n",
      "Iteration: 20 loss: 0.0059404160 time: 0.22037601470947266\n",
      "Iteration: 30 loss: 0.0049304590 time: 0.21485352516174316\n",
      "Iteration: 40 loss: 0.0041230936 time: 0.22535228729248047\n",
      "Iteration: 50 loss: 0.0034958106 time: 0.22911977767944336\n",
      "Iteration: 60 loss: 0.0030176763 time: 0.21701455116271973\n",
      "Iteration: 70 loss: 0.0026553762 time: 0.22573089599609375\n",
      "Iteration: 80 loss: 0.0023779999 time: 0.21624040603637695\n",
      "Iteration: 90 loss: 0.0021600404 time: 0.21898531913757324\n",
      "Iteration: 100 loss: 0.0019824547 time: 0.21654701232910156\n",
      "Iteration: 110 loss: 0.0018322063 time: 0.19746685028076172\n",
      "Iteration: 120 loss: 0.0017009833 time: 0.19463872909545898\n",
      "Iteration: 130 loss: 0.0015837352 time: 0.18590021133422852\n",
      "Iteration: 140 loss: 0.0014774445 time: 0.18724370002746582\n",
      "Iteration: 150 loss: 0.0013802722 time: 0.19594573974609375\n",
      "Iteration: 160 loss: 0.0012910416 time: 0.18442273139953613\n",
      "Iteration: 170 loss: 0.0012089554 time: 0.18837714195251465\n",
      "Iteration: 180 loss: 0.0011334476 time: 0.19173502922058105\n",
      "Iteration: 190 loss: 0.0010641025 time: 0.18838834762573242\n",
      "Iteration: 200 loss: 0.0010006048 time: 0.1866002082824707\n",
      "Iteration: 210 loss: 0.0009427025 time: 0.18563532829284668\n",
      "Iteration: 220 loss: 0.0008901780 time: 0.18171310424804688\n",
      "Iteration: 230 loss: 0.0008428242 time: 0.1890580654144287\n",
      "Iteration: 240 loss: 0.0008004257 time: 0.18551063537597656\n",
      "Iteration: 250 loss: 0.0007627454 time: 0.17723870277404785\n",
      "Iteration: 260 loss: 0.0007295157 time: 0.1868913173675537\n",
      "Iteration: 270 loss: 0.0007004359 time: 0.18604469299316406\n",
      "Iteration: 280 loss: 0.0006751736 time: 0.19138431549072266\n",
      "Iteration: 290 loss: 0.0006533718 time: 0.17852568626403809\n",
      "Iteration: 300 loss: 0.0006346579 time: 0.18771147727966309\n",
      "Iteration: 310 loss: 0.0006186561 time: 0.18271565437316895\n",
      "Iteration: 320 loss: 0.0006049994 time: 0.1760094165802002\n",
      "Iteration: 330 loss: 0.0005933409 time: 0.18042898178100586\n",
      "Iteration: 340 loss: 0.0005833624 time: 0.17549920082092285\n",
      "Iteration: 350 loss: 0.0005747806 time: 0.17928099632263184\n",
      "Iteration: 360 loss: 0.0005673504 time: 0.18558955192565918\n",
      "Iteration: 370 loss: 0.0005608644 time: 0.17267799377441406\n",
      "Iteration: 380 loss: 0.0005551516 time: 0.18925786018371582\n",
      "Iteration: 390 loss: 0.0005500728 time: 0.17922329902648926\n",
      "Iteration: 400 loss: 0.0005455170 time: 0.17451143264770508\n",
      "Iteration: 410 loss: 0.0005413960 time: 0.1779918670654297\n",
      "Iteration: 420 loss: 0.0005376402 time: 0.18613862991333008\n",
      "Iteration: 430 loss: 0.0005341948 time: 0.18909335136413574\n",
      "Iteration: 440 loss: 0.0005310157 time: 0.1787891387939453\n",
      "Iteration: 450 loss: 0.0005280677 time: 0.1856706142425537\n",
      "Iteration: 460 loss: 0.0005253216 time: 0.1798553466796875\n",
      "Iteration: 470 loss: 0.0005227530 time: 0.18080949783325195\n",
      "Iteration: 480 loss: 0.0005203411 time: 0.1905214786529541\n",
      "Iteration: 490 loss: 0.0005180678 time: 0.1788179874420166\n",
      "Iteration: 500 loss: 0.0005159171 time: 0.18191981315612793\n",
      "Iteration: 510 loss: 0.0005138747 time: 0.18039178848266602\n",
      "Iteration: 520 loss: 0.0005119277 time: 0.18892598152160645\n",
      "Iteration: 530 loss: 0.0005100646 time: 0.1863260269165039\n",
      "Iteration: 540 loss: 0.0005082746 time: 0.18621087074279785\n",
      "Iteration: 550 loss: 0.0005065481 time: 0.18072056770324707\n",
      "Iteration: 560 loss: 0.0005048762 time: 0.18502211570739746\n",
      "Iteration: 570 loss: 0.0005032510 time: 0.18793320655822754\n",
      "Iteration: 580 loss: 0.0005016651 time: 0.19074511528015137\n",
      "Iteration: 590 loss: 0.0005001117 time: 0.19726228713989258\n",
      "Iteration: 600 loss: 0.0004985849 time: 0.19298958778381348\n",
      "Iteration: 610 loss: 0.0004970789 time: 0.18988966941833496\n",
      "Iteration: 620 loss: 0.0004955889 time: 0.18801474571228027\n",
      "Iteration: 630 loss: 0.0004941101 time: 0.18943548202514648\n",
      "Iteration: 640 loss: 0.0004926384 time: 0.19413161277770996\n",
      "Iteration: 650 loss: 0.0004911701 time: 0.1830141544342041\n",
      "Iteration: 660 loss: 0.0004897016 time: 0.17621326446533203\n",
      "Iteration: 670 loss: 0.0004882298 time: 0.18428850173950195\n",
      "Iteration: 680 loss: 0.0004867518 time: 0.19095540046691895\n",
      "Iteration: 690 loss: 0.0004852650 time: 0.18618345260620117\n",
      "Iteration: 700 loss: 0.0004837670 time: 0.19015192985534668\n",
      "Iteration: 710 loss: 0.0004822556 time: 0.18975496292114258\n",
      "Iteration: 720 loss: 0.0004807287 time: 0.1895580291748047\n",
      "Iteration: 730 loss: 0.0004791844 time: 0.18354105949401855\n",
      "Iteration: 740 loss: 0.0004776209 time: 0.18480873107910156\n",
      "Iteration: 750 loss: 0.0004760366 time: 0.18086624145507812\n",
      "Iteration: 760 loss: 0.0004744300 time: 0.17991399765014648\n",
      "Iteration: 770 loss: 0.0004727995 time: 0.17890620231628418\n",
      "Iteration: 780 loss: 0.0004711438 time: 0.19512724876403809\n",
      "Iteration: 790 loss: 0.0004694616 time: 0.1800706386566162\n",
      "Iteration: 800 loss: 0.0004677515 time: 0.18161606788635254\n",
      "Iteration: 810 loss: 0.0004660123 time: 0.17848682403564453\n",
      "Iteration: 820 loss: 0.0004642428 time: 0.17687344551086426\n",
      "Iteration: 830 loss: 0.0004624419 time: 0.18323445320129395\n",
      "Iteration: 840 loss: 0.0004606083 time: 0.18620896339416504\n",
      "Iteration: 850 loss: 0.0004587411 time: 0.18161559104919434\n",
      "Iteration: 860 loss: 0.0004568391 time: 0.1807880401611328\n",
      "Iteration: 870 loss: 0.0004549013 time: 0.17786860466003418\n",
      "Iteration: 880 loss: 0.0004529265 time: 0.18459463119506836\n",
      "Iteration: 890 loss: 0.0004509138 time: 0.18702960014343262\n",
      "Iteration: 900 loss: 0.0004488621 time: 0.18398523330688477\n",
      "Iteration: 910 loss: 0.0004467705 time: 0.17904019355773926\n",
      "Iteration: 920 loss: 0.0004446380 time: 0.17963004112243652\n",
      "Iteration: 930 loss: 0.0004424636 time: 0.18384885787963867\n",
      "Iteration: 940 loss: 0.0004402464 time: 0.19515538215637207\n",
      "Iteration: 950 loss: 0.0004379857 time: 0.19343304634094238\n",
      "Iteration: 960 loss: 0.0004356804 time: 0.1778700351715088\n",
      "Iteration: 970 loss: 0.0004333300 time: 0.18555474281311035\n",
      "Iteration: 980 loss: 0.0004309335 time: 0.18762898445129395\n",
      "Iteration: 990 loss: 0.0004284904 time: 0.18534612655639648\n",
      "Iteration: 1000 loss: 0.0004260001 time: 0.18608713150024414\n",
      "Iteration: 1010 loss: 0.0004234620 time: 0.18735003471374512\n",
      "Iteration: 1020 loss: 0.0004208758 time: 0.1960887908935547\n",
      "Iteration: 1030 loss: 0.0004182410 time: 0.18836545944213867\n",
      "Iteration: 1040 loss: 0.0004155576 time: 0.18839716911315918\n",
      "Iteration: 1050 loss: 0.0004128254 time: 0.18810701370239258\n",
      "Iteration: 1060 loss: 0.0004100445 time: 0.18618226051330566\n",
      "Iteration: 1070 loss: 0.0004072152 time: 0.18471288681030273\n",
      "Iteration: 1080 loss: 0.0004043378 time: 0.18268942832946777\n",
      "Iteration: 1090 loss: 0.0004014130 time: 0.19064903259277344\n",
      "Iteration: 1100 loss: 0.0003984416 time: 0.18300271034240723\n",
      "Iteration: 1110 loss: 0.0003954245 time: 0.18621492385864258\n",
      "Iteration: 1120 loss: 0.0003923631 time: 0.1817467212677002\n",
      "Iteration: 1130 loss: 0.0003892590 time: 0.18607068061828613\n",
      "Iteration: 1140 loss: 0.0003861139 time: 0.18050169944763184\n",
      "Iteration: 1150 loss: 0.0003829300 time: 0.1940600872039795\n",
      "Iteration: 1160 loss: 0.0003797096 time: 0.18632769584655762\n",
      "Iteration: 1170 loss: 0.0003764556 time: 0.19035863876342773\n",
      "Iteration: 1180 loss: 0.0003731710 time: 0.1832420825958252\n",
      "Iteration: 1190 loss: 0.0003698593 time: 0.18205547332763672\n",
      "Iteration: 1200 loss: 0.0003665241 time: 0.17975544929504395\n",
      "Iteration: 1210 loss: 0.0003631698 time: 0.17711472511291504\n",
      "Iteration: 1220 loss: 0.0003598007 time: 0.18313121795654297\n",
      "Iteration: 1230 loss: 0.0003564216 time: 0.17781996726989746\n",
      "Iteration: 1240 loss: 0.0003530378 time: 0.18420791625976562\n",
      "Iteration: 1250 loss: 0.0003496547 time: 0.17760658264160156\n",
      "Iteration: 1260 loss: 0.0003462781 time: 0.18117833137512207\n",
      "Iteration: 1270 loss: 0.0003429140 time: 0.18373918533325195\n",
      "Iteration: 1280 loss: 0.0003395686 time: 0.18098664283752441\n",
      "Iteration: 1290 loss: 0.0003362482 time: 0.1824808120727539\n",
      "Iteration: 1300 loss: 0.0003329595 time: 0.18381071090698242\n",
      "Iteration: 1310 loss: 0.0003297089 time: 0.18834948539733887\n",
      "Iteration: 1320 loss: 0.0003265029 time: 0.18928956985473633\n",
      "Iteration: 1330 loss: 0.0003233481 time: 0.18433403968811035\n",
      "Iteration: 1340 loss: 0.0003202505 time: 0.1901850700378418\n",
      "Iteration: 1350 loss: 0.0003172163 time: 0.19173431396484375\n",
      "Iteration: 1360 loss: 0.0003142510 time: 0.18600130081176758\n",
      "Iteration: 1370 loss: 0.0003113598 time: 0.18381285667419434\n",
      "Iteration: 1380 loss: 0.0003085476 time: 0.18245577812194824\n",
      "Iteration: 1390 loss: 0.0003058185 time: 0.18271589279174805\n",
      "Iteration: 1400 loss: 0.0003031760 time: 0.18597412109375\n",
      "Iteration: 1410 loss: 0.0003006229 time: 0.1876688003540039\n",
      "Iteration: 1420 loss: 0.0002981613 time: 0.19131064414978027\n",
      "Iteration: 1430 loss: 0.0002957927 time: 0.18386220932006836\n",
      "Iteration: 1440 loss: 0.0002935176 time: 0.1858985424041748\n",
      "Iteration: 1450 loss: 0.0002913358 time: 0.17707014083862305\n",
      "Iteration: 1460 loss: 0.0002892464 time: 0.1836249828338623\n",
      "Iteration: 1470 loss: 0.0002872478 time: 0.1778268814086914\n",
      "Iteration: 1480 loss: 0.0002853378 time: 0.1782393455505371\n",
      "Iteration: 1490 loss: 0.0002835134 time: 0.1828005313873291\n",
      "Iteration: 1500 loss: 0.0002817712 time: 0.18007802963256836\n",
      "Iteration: 1510 loss: 0.0002801073 time: 0.18243694305419922\n",
      "Iteration: 1520 loss: 0.0002785176 time: 0.17467212677001953\n",
      "Iteration: 1530 loss: 0.0002769974 time: 0.18089842796325684\n",
      "Iteration: 1540 loss: 0.0002755422 time: 0.19085383415222168\n",
      "Iteration: 1550 loss: 0.0002741469 time: 0.191056489944458\n",
      "Iteration: 1560 loss: 0.0002728069 time: 0.17962336540222168\n",
      "Iteration: 1570 loss: 0.0002715173 time: 0.1850881576538086\n",
      "Iteration: 1580 loss: 0.0002702733 time: 0.18044066429138184\n",
      "Iteration: 1590 loss: 0.0002690704 time: 0.1787574291229248\n",
      "Iteration: 1600 loss: 0.0002679043 time: 0.18069219589233398\n",
      "Iteration: 1610 loss: 0.0002667706 time: 0.17956233024597168\n",
      "Iteration: 1620 loss: 0.0002656657 time: 0.17794132232666016\n",
      "Iteration: 1630 loss: 0.0002645859 time: 0.17516088485717773\n",
      "Iteration: 1640 loss: 0.0002635278 time: 0.1888735294342041\n",
      "Iteration: 1650 loss: 0.0002624885 time: 0.18809795379638672\n",
      "Iteration: 1660 loss: 0.0002614651 time: 0.179917573928833\n",
      "Iteration: 1670 loss: 0.0002604552 time: 0.18798065185546875\n",
      "Iteration: 1680 loss: 0.0002594566 time: 0.18688344955444336\n",
      "Iteration: 1690 loss: 0.0002584672 time: 0.1870565414428711\n",
      "Iteration: 1700 loss: 0.0002574853 time: 0.18460988998413086\n",
      "Iteration: 1710 loss: 0.0002565094 time: 0.1797010898590088\n",
      "Iteration: 1720 loss: 0.0002555381 time: 0.18101882934570312\n",
      "Iteration: 1730 loss: 0.0002545702 time: 0.18269634246826172\n",
      "Iteration: 1740 loss: 0.0002536046 time: 0.1784508228302002\n",
      "Iteration: 1750 loss: 0.0002526405 time: 0.1844165325164795\n",
      "Iteration: 1760 loss: 0.0002516770 time: 0.18369793891906738\n",
      "Iteration: 1770 loss: 0.0002507135 time: 0.18301606178283691\n",
      "Iteration: 1780 loss: 0.0002497494 time: 0.17686748504638672\n",
      "Iteration: 1790 loss: 0.0002487841 time: 0.17911982536315918\n",
      "Iteration: 1800 loss: 0.0002478173 time: 0.17629051208496094\n",
      "Iteration: 1810 loss: 0.0002468484 time: 0.17995095252990723\n",
      "Iteration: 1820 loss: 0.0002458773 time: 0.17652678489685059\n",
      "Iteration: 1830 loss: 0.0002449035 time: 0.18759465217590332\n",
      "Iteration: 1840 loss: 0.0002439268 time: 0.1809217929840088\n",
      "Iteration: 1850 loss: 0.0002429470 time: 0.18328571319580078\n",
      "Iteration: 1860 loss: 0.0002419640 time: 0.18100595474243164\n",
      "Iteration: 1870 loss: 0.0002409774 time: 0.18344950675964355\n",
      "Iteration: 1880 loss: 0.0002399871 time: 0.17968177795410156\n",
      "Iteration: 1890 loss: 0.0002389931 time: 0.18407535552978516\n",
      "Iteration: 1900 loss: 0.0002379952 time: 0.18726754188537598\n",
      "Iteration: 1910 loss: 0.0002369933 time: 0.17907142639160156\n",
      "Iteration: 1920 loss: 0.0002359873 time: 0.1814432144165039\n",
      "Iteration: 1930 loss: 0.0002349770 time: 0.18575644493103027\n",
      "Iteration: 1940 loss: 0.0002339626 time: 0.1936047077178955\n",
      "Iteration: 1950 loss: 0.0002329438 time: 0.18142390251159668\n",
      "Iteration: 1960 loss: 0.0002319208 time: 0.18616509437561035\n",
      "Iteration: 1970 loss: 0.0002308933 time: 0.17834091186523438\n",
      "Iteration: 1980 loss: 0.0002298615 time: 0.17531561851501465\n",
      "Iteration: 1990 loss: 0.0002288253 time: 0.18186187744140625\n",
      "Iteration: 2000 loss: 0.0002277847 time: 0.1763620376586914\n",
      "Iteration: 2010 loss: 0.0002267397 time: 0.17928385734558105\n",
      "Iteration: 2020 loss: 0.0002256904 time: 0.19105052947998047\n",
      "Iteration: 2030 loss: 0.0002246368 time: 0.1840989589691162\n",
      "Iteration: 2040 loss: 0.0002235789 time: 0.17339253425598145\n",
      "Iteration: 2050 loss: 0.0002225168 time: 0.18264174461364746\n",
      "Iteration: 2060 loss: 0.0002214506 time: 0.1856250762939453\n",
      "Iteration: 2070 loss: 0.0002203804 time: 0.18211865425109863\n",
      "Iteration: 2080 loss: 0.0002193062 time: 0.18625140190124512\n",
      "Iteration: 2090 loss: 0.0002182281 time: 0.18184185028076172\n",
      "Iteration: 2100 loss: 0.0002171462 time: 0.1820833683013916\n",
      "Iteration: 2110 loss: 0.0002160607 time: 0.1815204620361328\n",
      "Iteration: 2120 loss: 0.0002149716 time: 0.17978477478027344\n",
      "Iteration: 2130 loss: 0.0002138791 time: 0.18419575691223145\n",
      "Iteration: 2140 loss: 0.0002127833 time: 0.17891955375671387\n",
      "Iteration: 2150 loss: 0.0002116843 time: 0.16534161567687988\n",
      "Iteration: 2160 loss: 0.0002105822 time: 0.17791295051574707\n",
      "Iteration: 2170 loss: 0.0002094771 time: 0.17978358268737793\n",
      "Iteration: 2180 loss: 0.0002083692 time: 0.18490099906921387\n",
      "Iteration: 2190 loss: 0.0002072585 time: 0.17992830276489258\n",
      "Iteration: 2200 loss: 0.0002061451 time: 0.17612195014953613\n",
      "Iteration: 2210 loss: 0.0002050292 time: 0.1780242919921875\n",
      "Iteration: 2220 loss: 0.0002039108 time: 0.1855158805847168\n",
      "Iteration: 2230 loss: 0.0002027899 time: 0.17708110809326172\n",
      "Iteration: 2240 loss: 0.0002016666 time: 0.1820814609527588\n",
      "Iteration: 2250 loss: 0.0002005409 time: 0.18495416641235352\n",
      "Iteration: 2260 loss: 0.0001994128 time: 0.17490434646606445\n",
      "Iteration: 2270 loss: 0.0001982822 time: 0.18989276885986328\n",
      "Iteration: 2280 loss: 0.0001971491 time: 0.18750572204589844\n",
      "Iteration: 2290 loss: 0.0001960133 time: 0.1844165325164795\n",
      "Iteration: 2300 loss: 0.0001948748 time: 0.19009757041931152\n",
      "Iteration: 2310 loss: 0.0001937333 time: 0.18117880821228027\n",
      "Iteration: 2320 loss: 0.0001925886 time: 0.18192100524902344\n",
      "Iteration: 2330 loss: 0.0001914404 time: 0.181135892868042\n",
      "Iteration: 2340 loss: 0.0001902886 time: 0.17241811752319336\n",
      "Iteration: 2350 loss: 0.0001891326 time: 0.18372440338134766\n",
      "Iteration: 2360 loss: 0.0001879722 time: 0.1873149871826172\n",
      "Iteration: 2370 loss: 0.0001868068 time: 0.18767786026000977\n",
      "Iteration: 2380 loss: 0.0001856360 time: 0.18499994277954102\n",
      "Iteration: 2390 loss: 0.0001844592 time: 0.18810653686523438\n",
      "Iteration: 2400 loss: 0.0001832758 time: 0.1879415512084961\n",
      "Iteration: 2410 loss: 0.0001820852 time: 0.20085406303405762\n",
      "Iteration: 2420 loss: 0.0001808867 time: 0.1938793659210205\n",
      "Iteration: 2430 loss: 0.0001796795 time: 0.17756319046020508\n",
      "Iteration: 2440 loss: 0.0001784628 time: 0.18449187278747559\n",
      "Iteration: 2450 loss: 0.0001772358 time: 0.17721271514892578\n",
      "Iteration: 2460 loss: 0.0001759975 time: 0.1841597557067871\n",
      "Iteration: 2470 loss: 0.0001747471 time: 0.18169260025024414\n",
      "Iteration: 2480 loss: 0.0001734834 time: 0.19150400161743164\n",
      "Iteration: 2490 loss: 0.0001722056 time: 0.19320011138916016\n",
      "Iteration: 2500 loss: 0.0001709124 time: 0.1893632411956787\n",
      "Iteration: 2510 loss: 0.0001696028 time: 0.19153523445129395\n",
      "Iteration: 2520 loss: 0.0001682756 time: 0.18893742561340332\n",
      "Iteration: 2530 loss: 0.0001669296 time: 0.20798134803771973\n",
      "Iteration: 2540 loss: 0.0001655635 time: 0.18985962867736816\n",
      "Iteration: 2550 loss: 0.0001641762 time: 0.1781470775604248\n",
      "Iteration: 2560 loss: 0.0001627663 time: 0.18607234954833984\n",
      "Iteration: 2570 loss: 0.0001613325 time: 0.19324183464050293\n",
      "Iteration: 2580 loss: 0.0001598736 time: 0.17868781089782715\n",
      "Iteration: 2590 loss: 0.0001583880 time: 0.18345117568969727\n",
      "Iteration: 2600 loss: 0.0001568745 time: 0.17697405815124512\n",
      "Iteration: 2610 loss: 0.0001553318 time: 0.18958330154418945\n",
      "Iteration: 2620 loss: 0.0001537585 time: 0.19064879417419434\n",
      "Iteration: 2630 loss: 0.0001521532 time: 0.17951440811157227\n",
      "Iteration: 2640 loss: 0.0001505146 time: 0.20130181312561035\n",
      "Iteration: 2650 loss: 0.0001488414 time: 0.19579386711120605\n",
      "Iteration: 2660 loss: 0.0001471323 time: 0.1849663257598877\n",
      "Iteration: 2670 loss: 0.0001453862 time: 0.1927480697631836\n",
      "Iteration: 2680 loss: 0.0001436018 time: 0.17969799041748047\n",
      "Iteration: 2690 loss: 0.0001417780 time: 0.17604851722717285\n",
      "Iteration: 2700 loss: 0.0001399139 time: 0.18086981773376465\n",
      "Iteration: 2710 loss: 0.0001380085 time: 0.18279433250427246\n",
      "Iteration: 2720 loss: 0.0001360610 time: 0.1796705722808838\n",
      "Iteration: 2730 loss: 0.0001340707 time: 0.19052743911743164\n",
      "Iteration: 2740 loss: 0.0001320371 time: 0.18357014656066895\n",
      "Iteration: 2750 loss: 0.0001299599 time: 0.1906132698059082\n",
      "Iteration: 2760 loss: 0.0001278388 time: 0.18105316162109375\n",
      "Iteration: 2770 loss: 0.0001256739 time: 0.1833658218383789\n",
      "Iteration: 2780 loss: 0.0001234655 time: 0.18389034271240234\n",
      "Iteration: 2790 loss: 0.0001212140 time: 0.18292617797851562\n",
      "Iteration: 2800 loss: 0.0001189204 time: 0.1841597557067871\n",
      "Iteration: 2810 loss: 0.0001165855 time: 0.18615150451660156\n",
      "Iteration: 2820 loss: 0.0001142110 time: 0.177947998046875\n",
      "Iteration: 2830 loss: 0.0001117983 time: 0.18099665641784668\n",
      "Iteration: 2840 loss: 0.0001093496 time: 0.1790928840637207\n",
      "Iteration: 2850 loss: 0.0001068673 time: 0.17816662788391113\n",
      "Iteration: 2860 loss: 0.0001043541 time: 0.18233370780944824\n",
      "Iteration: 2870 loss: 0.0001018131 time: 0.1871635913848877\n",
      "Iteration: 2880 loss: 0.0000992477 time: 0.1826341152191162\n",
      "Iteration: 2890 loss: 0.0000966617 time: 0.1904921531677246\n",
      "Iteration: 2900 loss: 0.0000940592 time: 0.17891168594360352\n",
      "Iteration: 2910 loss: 0.0000914445 time: 0.18236446380615234\n",
      "Iteration: 2920 loss: 0.0000888224 time: 0.1850278377532959\n",
      "Iteration: 2930 loss: 0.0000861977 time: 0.18002057075500488\n",
      "Iteration: 2940 loss: 0.0000835755 time: 0.19057607650756836\n",
      "Iteration: 2950 loss: 0.0000809611 time: 0.18359136581420898\n",
      "Iteration: 2960 loss: 0.0000783597 time: 0.1809711456298828\n",
      "Iteration: 2970 loss: 0.0000757768 time: 0.18755292892456055\n",
      "Iteration: 2980 loss: 0.0000732177 time: 0.18186330795288086\n",
      "Iteration: 2990 loss: 0.0000706877 time: 0.18827438354492188\n",
      "Iteration: 3000 loss: 0.0000681920 time: 0.18174123764038086\n",
      "Iteration: 3010 loss: 0.0000657357 time: 0.18076515197753906\n",
      "Iteration: 3020 loss: 0.0000633234 time: 0.18323373794555664\n",
      "Iteration: 3030 loss: 0.0000609599 time: 0.17232561111450195\n",
      "Iteration: 3040 loss: 0.0000586492 time: 0.19059491157531738\n",
      "Iteration: 3050 loss: 0.0000563953 time: 0.18282508850097656\n",
      "Iteration: 3060 loss: 0.0000542017 time: 0.18848204612731934\n",
      "Iteration: 3070 loss: 0.0000520714 time: 0.18313956260681152\n",
      "Iteration: 3080 loss: 0.0000500071 time: 0.17848896980285645\n",
      "Iteration: 3090 loss: 0.0000480111 time: 0.18973970413208008\n",
      "Iteration: 3100 loss: 0.0000460851 time: 0.19250106811523438\n",
      "Iteration: 3110 loss: 0.0000442307 time: 0.19211316108703613\n",
      "Iteration: 3120 loss: 0.0000424486 time: 0.17963838577270508\n",
      "Iteration: 3130 loss: 0.0000407395 time: 0.1859130859375\n",
      "Iteration: 3140 loss: 0.0000391034 time: 0.1859734058380127\n",
      "Iteration: 3150 loss: 0.0000375402 time: 0.176161527633667\n",
      "Iteration: 3160 loss: 0.0000360493 time: 0.1924295425415039\n",
      "Iteration: 3170 loss: 0.0000346296 time: 0.17800688743591309\n",
      "Iteration: 3180 loss: 0.0000332800 time: 0.17766356468200684\n",
      "Iteration: 3190 loss: 0.0000319990 time: 0.18758082389831543\n",
      "Iteration: 3200 loss: 0.0000307848 time: 0.17732739448547363\n",
      "Iteration: 3210 loss: 0.0000296354 time: 0.17450404167175293\n",
      "Iteration: 3220 loss: 0.0000285488 time: 0.178084135055542\n",
      "Iteration: 3230 loss: 0.0000275227 time: 0.18171310424804688\n",
      "Iteration: 3240 loss: 0.0000265547 time: 0.1726524829864502\n",
      "Iteration: 3250 loss: 0.0000256423 time: 0.19017815589904785\n",
      "Iteration: 3260 loss: 0.0000247830 time: 0.1767263412475586\n",
      "Iteration: 3270 loss: 0.0000239742 time: 0.17679476737976074\n",
      "Iteration: 3280 loss: 0.0000232134 time: 0.17664694786071777\n",
      "Iteration: 3290 loss: 0.0000224980 time: 0.1813516616821289\n",
      "Iteration: 3300 loss: 0.0000218256 time: 0.17069125175476074\n",
      "Iteration: 3310 loss: 0.0000211936 time: 0.17733263969421387\n",
      "Iteration: 3320 loss: 0.0000205996 time: 0.1825275421142578\n",
      "Iteration: 3330 loss: 0.0000200414 time: 0.18386411666870117\n",
      "Iteration: 3340 loss: 0.0000195167 time: 0.17611002922058105\n",
      "Iteration: 3350 loss: 0.0000190233 time: 0.17767739295959473\n",
      "Iteration: 3360 loss: 0.0000185592 time: 0.17566370964050293\n",
      "Iteration: 3370 loss: 0.0000181226 time: 0.1867063045501709\n",
      "Iteration: 3380 loss: 0.0000177114 time: 0.1801145076751709\n",
      "Iteration: 3390 loss: 0.0000173241 time: 0.17750930786132812\n",
      "Iteration: 3400 loss: 0.0000169589 time: 0.17821931838989258\n",
      "Iteration: 3410 loss: 0.0000166144 time: 0.18602442741394043\n",
      "Iteration: 3420 loss: 0.0000162891 time: 0.18094635009765625\n",
      "Iteration: 3430 loss: 0.0000159816 time: 0.17662835121154785\n",
      "Iteration: 3440 loss: 0.0000156908 time: 0.1806790828704834\n",
      "Iteration: 3450 loss: 0.0000154154 time: 0.17046165466308594\n",
      "Iteration: 3460 loss: 0.0000151544 time: 0.1762678623199463\n",
      "Iteration: 3470 loss: 0.0000149068 time: 0.1773080825805664\n",
      "Iteration: 3480 loss: 0.0000146716 time: 0.1800251007080078\n",
      "Iteration: 3490 loss: 0.0000144480 time: 0.18539977073669434\n",
      "Iteration: 3500 loss: 0.0000142352 time: 0.1816089153289795\n",
      "Iteration: 3510 loss: 0.0000140324 time: 0.17876744270324707\n",
      "Iteration: 3520 loss: 0.0000138389 time: 0.18631219863891602\n",
      "Iteration: 3530 loss: 0.0000136541 time: 0.17900848388671875\n",
      "Iteration: 3540 loss: 0.0000134775 time: 0.1809711456298828\n",
      "Iteration: 3550 loss: 0.0000133084 time: 0.17589569091796875\n",
      "Iteration: 3560 loss: 0.0000131464 time: 0.17672085762023926\n",
      "Iteration: 3570 loss: 0.0000129910 time: 0.17879962921142578\n",
      "Iteration: 3580 loss: 0.0000128418 time: 0.18413376808166504\n",
      "Iteration: 3590 loss: 0.0000126984 time: 0.17676115036010742\n",
      "Iteration: 3600 loss: 0.0000125603 time: 0.17803597450256348\n",
      "Iteration: 3610 loss: 0.0000124274 time: 0.19090008735656738\n",
      "Iteration: 3620 loss: 0.0000122992 time: 0.1820051670074463\n",
      "Iteration: 3630 loss: 0.0000121754 time: 0.18735313415527344\n",
      "Iteration: 3640 loss: 0.0000120559 time: 0.18512821197509766\n",
      "Iteration: 3650 loss: 0.0000119403 time: 0.18326592445373535\n",
      "Iteration: 3660 loss: 0.0000118284 time: 0.17283415794372559\n",
      "Iteration: 3670 loss: 0.0000117200 time: 0.18131017684936523\n",
      "Iteration: 3680 loss: 0.0000116149 time: 0.1742720603942871\n",
      "Iteration: 3690 loss: 0.0000115129 time: 0.1774156093597412\n",
      "Iteration: 3700 loss: 0.0000114139 time: 0.17788052558898926\n",
      "Iteration: 3710 loss: 0.0000113176 time: 0.17840814590454102\n",
      "Iteration: 3720 loss: 0.0000112240 time: 0.1791059970855713\n",
      "Iteration: 3730 loss: 0.0000111329 time: 0.18472003936767578\n",
      "Iteration: 3740 loss: 0.0000110441 time: 0.19165706634521484\n",
      "Iteration: 3750 loss: 0.0000109576 time: 0.17987298965454102\n",
      "Iteration: 3760 loss: 0.0000108732 time: 0.18552231788635254\n",
      "Iteration: 3770 loss: 0.0000107908 time: 0.17808866500854492\n",
      "Iteration: 3780 loss: 0.0000107103 time: 0.17159485816955566\n",
      "Iteration: 3790 loss: 0.0000106317 time: 0.17974019050598145\n",
      "Iteration: 3800 loss: 0.0000105548 time: 0.17989063262939453\n",
      "Iteration: 3810 loss: 0.0000104795 time: 0.1802692413330078\n",
      "Iteration: 3820 loss: 0.0000104058 time: 0.17740988731384277\n",
      "Iteration: 3830 loss: 0.0000103336 time: 0.18172407150268555\n",
      "Iteration: 3840 loss: 0.0000102629 time: 0.17983007431030273\n",
      "Iteration: 3850 loss: 0.0000101935 time: 0.1817185878753662\n",
      "Iteration: 3860 loss: 0.0000101253 time: 0.1837317943572998\n",
      "Iteration: 3870 loss: 0.0000100584 time: 0.19353365898132324\n",
      "Iteration: 3880 loss: 0.0000099927 time: 0.17912054061889648\n",
      "Iteration: 3890 loss: 0.0000099281 time: 0.18241071701049805\n",
      "Iteration: 3900 loss: 0.0000098646 time: 0.1797349452972412\n",
      "Iteration: 3910 loss: 0.0000098020 time: 0.1735999584197998\n",
      "Iteration: 3920 loss: 0.0000097404 time: 0.1905074119567871\n",
      "Iteration: 3930 loss: 0.0000096798 time: 0.18538761138916016\n",
      "Iteration: 3940 loss: 0.0000096200 time: 0.18274950981140137\n",
      "Iteration: 3950 loss: 0.0000095611 time: 0.18561005592346191\n",
      "Iteration: 3960 loss: 0.0000095029 time: 0.18502092361450195\n",
      "Iteration: 3970 loss: 0.0000094455 time: 0.18744826316833496\n",
      "Iteration: 3980 loss: 0.0000093888 time: 0.18567728996276855\n",
      "Iteration: 3990 loss: 0.0000093328 time: 0.19385123252868652\n",
      "Iteration: 4000 loss: 0.0000092775 time: 0.1858515739440918\n",
      "Iteration: 4010 loss: 0.0000092227 time: 0.18265461921691895\n",
      "Iteration: 4020 loss: 0.0000091685 time: 0.18616914749145508\n",
      "Iteration: 4030 loss: 0.0000091149 time: 0.18169379234313965\n",
      "Iteration: 4040 loss: 0.0000090618 time: 0.17816686630249023\n",
      "Iteration: 4050 loss: 0.0000090092 time: 0.17014241218566895\n",
      "Iteration: 4060 loss: 0.0000089570 time: 0.1882319450378418\n",
      "Iteration: 4070 loss: 0.0000089053 time: 0.18620061874389648\n",
      "Iteration: 4080 loss: 0.0000088540 time: 0.1854383945465088\n",
      "Iteration: 4090 loss: 0.0000088031 time: 0.18453621864318848\n",
      "Iteration: 4100 loss: 0.0000087526 time: 0.18556499481201172\n",
      "Iteration: 4110 loss: 0.0000087024 time: 0.18125581741333008\n",
      "Iteration: 4120 loss: 0.0000086526 time: 0.1767888069152832\n",
      "Iteration: 4130 loss: 0.0000086030 time: 0.18250417709350586\n",
      "Iteration: 4140 loss: 0.0000085537 time: 0.17841458320617676\n",
      "Iteration: 4150 loss: 0.0000085047 time: 0.18916726112365723\n",
      "Iteration: 4160 loss: 0.0000084559 time: 0.18552303314208984\n",
      "Iteration: 4170 loss: 0.0000084074 time: 0.18697047233581543\n",
      "Iteration: 4180 loss: 0.0000083591 time: 0.1780412197113037\n",
      "Iteration: 4190 loss: 0.0000083110 time: 0.17800617218017578\n",
      "Iteration: 4200 loss: 0.0000082630 time: 0.18683433532714844\n",
      "Iteration: 4210 loss: 0.0000082152 time: 0.1836555004119873\n",
      "Iteration: 4220 loss: 0.0000081676 time: 0.1869809627532959\n",
      "Iteration: 4230 loss: 0.0000081201 time: 0.18576383590698242\n",
      "Iteration: 4240 loss: 0.0000080728 time: 0.1824641227722168\n",
      "Iteration: 4250 loss: 0.0000080255 time: 0.18904423713684082\n",
      "Iteration: 4260 loss: 0.0000079784 time: 0.184403657913208\n",
      "Iteration: 4270 loss: 0.0000079313 time: 0.18709588050842285\n",
      "Iteration: 4280 loss: 0.0000078843 time: 0.17981600761413574\n",
      "Iteration: 4290 loss: 0.0000078374 time: 0.18135952949523926\n",
      "Iteration: 4300 loss: 0.0000077905 time: 0.18354487419128418\n",
      "Iteration: 4310 loss: 0.0000077437 time: 0.18725156784057617\n",
      "Iteration: 4320 loss: 0.0000076969 time: 0.18918776512145996\n",
      "Iteration: 4330 loss: 0.0000076502 time: 0.18872308731079102\n",
      "Iteration: 4340 loss: 0.0000076034 time: 0.18608641624450684\n",
      "Iteration: 4350 loss: 0.0000075567 time: 0.17937707901000977\n",
      "Iteration: 4360 loss: 0.0000075100 time: 0.18311500549316406\n",
      "Iteration: 4370 loss: 0.0000074633 time: 0.18137097358703613\n",
      "Iteration: 4380 loss: 0.0000074165 time: 0.1844010353088379\n",
      "Iteration: 4390 loss: 0.0000073698 time: 0.18945002555847168\n",
      "Iteration: 4400 loss: 0.0000073230 time: 0.1778559684753418\n",
      "Iteration: 4410 loss: 0.0000072762 time: 0.18674564361572266\n",
      "Iteration: 4420 loss: 0.0000072294 time: 0.18740105628967285\n",
      "Iteration: 4430 loss: 0.0000071825 time: 0.18789887428283691\n",
      "Iteration: 4440 loss: 0.0000071356 time: 0.18985199928283691\n",
      "Iteration: 4450 loss: 0.0000070886 time: 0.18883991241455078\n",
      "Iteration: 4460 loss: 0.0000070416 time: 0.19085264205932617\n",
      "Iteration: 4470 loss: 0.0000069945 time: 0.18555760383605957\n",
      "Iteration: 4480 loss: 0.0000069474 time: 0.17886710166931152\n",
      "Iteration: 4490 loss: 0.0000069002 time: 0.1910698413848877\n",
      "Iteration: 4500 loss: 0.0000068530 time: 0.18377447128295898\n",
      "Iteration: 4510 loss: 0.0000068056 time: 0.19001221656799316\n",
      "Iteration: 4520 loss: 0.0000067583 time: 0.18205761909484863\n",
      "Iteration: 4530 loss: 0.0000067108 time: 0.17861437797546387\n",
      "Iteration: 4540 loss: 0.0000066633 time: 0.18193960189819336\n",
      "Iteration: 4550 loss: 0.0000066157 time: 0.1892101764678955\n",
      "Iteration: 4560 loss: 0.0000065680 time: 0.18241095542907715\n",
      "Iteration: 4570 loss: 0.0000065202 time: 0.19031023979187012\n",
      "Iteration: 4580 loss: 0.0000064724 time: 0.17827057838439941\n",
      "Iteration: 4590 loss: 0.0000064245 time: 0.18385934829711914\n",
      "Iteration: 4600 loss: 0.0000063766 time: 0.1778857707977295\n",
      "Iteration: 4610 loss: 0.0000063285 time: 0.17996764183044434\n",
      "Iteration: 4620 loss: 0.0000062804 time: 0.18326520919799805\n",
      "Iteration: 4630 loss: 0.0000062323 time: 0.18344998359680176\n",
      "Iteration: 4640 loss: 0.0000061840 time: 0.17782020568847656\n",
      "Iteration: 4650 loss: 0.0000061357 time: 0.1807553768157959\n",
      "Iteration: 4660 loss: 0.0000060874 time: 0.19167685508728027\n",
      "Iteration: 4670 loss: 0.0000060389 time: 0.1873009204864502\n",
      "Iteration: 4680 loss: 0.0000059905 time: 0.19217967987060547\n",
      "Iteration: 4690 loss: 0.0000059419 time: 0.18792390823364258\n",
      "Iteration: 4700 loss: 0.0000058934 time: 0.17846107482910156\n",
      "Iteration: 4710 loss: 0.0000058448 time: 0.17798900604248047\n",
      "Iteration: 4720 loss: 0.0000057961 time: 0.18781471252441406\n",
      "Iteration: 4730 loss: 0.0000057474 time: 0.19002747535705566\n",
      "Iteration: 4740 loss: 0.0000056987 time: 0.18741297721862793\n",
      "Iteration: 4750 loss: 0.0000056500 time: 0.17537641525268555\n",
      "Iteration: 4760 loss: 0.0000056012 time: 0.18352413177490234\n",
      "Iteration: 4770 loss: 0.0000055525 time: 0.18932151794433594\n",
      "Iteration: 4780 loss: 0.0000055037 time: 0.19081377983093262\n",
      "Iteration: 4790 loss: 0.0000054549 time: 0.17907047271728516\n",
      "Iteration: 4800 loss: 0.0000054062 time: 0.18005800247192383\n",
      "Iteration: 4810 loss: 0.0000053574 time: 0.18036270141601562\n",
      "Iteration: 4820 loss: 0.0000053087 time: 0.18152809143066406\n",
      "Iteration: 4830 loss: 0.0000052600 time: 0.17741608619689941\n",
      "Iteration: 4840 loss: 0.0000052114 time: 0.17866802215576172\n",
      "Iteration: 4850 loss: 0.0000051628 time: 0.18276453018188477\n",
      "Iteration: 4860 loss: 0.0000051143 time: 0.1880178451538086\n",
      "Iteration: 4870 loss: 0.0000050658 time: 0.1831531524658203\n",
      "Iteration: 4880 loss: 0.0000050174 time: 0.18347573280334473\n",
      "Iteration: 4890 loss: 0.0000049691 time: 0.17861676216125488\n",
      "Iteration: 4900 loss: 0.0000049209 time: 0.18642306327819824\n",
      "Iteration: 4910 loss: 0.0000048727 time: 0.18413662910461426\n",
      "Iteration: 4920 loss: 0.0000048247 time: 0.17577433586120605\n",
      "Iteration: 4930 loss: 0.0000047768 time: 0.18453764915466309\n",
      "Iteration: 4940 loss: 0.0000047291 time: 0.18009281158447266\n",
      "Iteration: 4950 loss: 0.0000046815 time: 0.17800116539001465\n",
      "Iteration: 4960 loss: 0.0000046340 time: 0.17583990097045898\n",
      "Iteration: 4970 loss: 0.0000045867 time: 0.17858481407165527\n",
      "Iteration: 4980 loss: 0.0000045395 time: 0.1852254867553711\n",
      "Iteration: 4990 loss: 0.0000044926 time: 0.18901300430297852\n",
      "Iteration: 5000 loss: 0.0000044458 time: 0.1759343147277832\n",
      "Iteration: 5010 loss: 0.0000043993 time: 0.17633628845214844\n",
      "Iteration: 5020 loss: 0.0000043529 time: 0.17898154258728027\n",
      "Iteration: 5030 loss: 0.0000043068 time: 0.1830301284790039\n",
      "Iteration: 5040 loss: 0.0000042609 time: 0.18370485305786133\n",
      "Iteration: 5050 loss: 0.0000042153 time: 0.1814274787902832\n",
      "Iteration: 5060 loss: 0.0000041699 time: 0.1845076084136963\n",
      "Iteration: 5070 loss: 0.0000041248 time: 0.17446041107177734\n",
      "Iteration: 5080 loss: 0.0000040800 time: 0.1871337890625\n",
      "Iteration: 5090 loss: 0.0000040354 time: 0.17354583740234375\n",
      "Iteration: 5100 loss: 0.0000039912 time: 0.1823878288269043\n",
      "Iteration: 5110 loss: 0.0000039472 time: 0.18119597434997559\n",
      "Iteration: 5120 loss: 0.0000039036 time: 0.18084168434143066\n",
      "Iteration: 5130 loss: 0.0000038603 time: 0.18157005310058594\n",
      "Iteration: 5140 loss: 0.0000038173 time: 0.17426180839538574\n",
      "Iteration: 5150 loss: 0.0000037747 time: 0.18096160888671875\n",
      "Iteration: 5160 loss: 0.0000037325 time: 0.18201661109924316\n",
      "Iteration: 5170 loss: 0.0000036906 time: 0.18727350234985352\n",
      "Iteration: 5180 loss: 0.0000036491 time: 0.18639039993286133\n",
      "Iteration: 5190 loss: 0.0000036079 time: 0.18570709228515625\n",
      "Iteration: 5200 loss: 0.0000035672 time: 0.1880486011505127\n",
      "Iteration: 5210 loss: 0.0000035268 time: 0.18657803535461426\n",
      "Iteration: 5220 loss: 0.0000034869 time: 0.18355035781860352\n",
      "Iteration: 5230 loss: 0.0000034473 time: 0.19742679595947266\n",
      "Iteration: 5240 loss: 0.0000034082 time: 0.1847367286682129\n",
      "Iteration: 5250 loss: 0.0000033695 time: 0.1867365837097168\n",
      "Iteration: 5260 loss: 0.0000033313 time: 0.18662214279174805\n",
      "Iteration: 5270 loss: 0.0000032934 time: 0.18660187721252441\n",
      "Iteration: 5280 loss: 0.0000032560 time: 0.18543291091918945\n",
      "Iteration: 5290 loss: 0.0000032191 time: 0.1918625831604004\n",
      "Iteration: 5300 loss: 0.0000031826 time: 0.19349050521850586\n",
      "Iteration: 5310 loss: 0.0000031466 time: 0.1829068660736084\n",
      "Iteration: 5320 loss: 0.0000031110 time: 0.1868269443511963\n",
      "Iteration: 5330 loss: 0.0000030759 time: 0.18673229217529297\n",
      "Iteration: 5340 loss: 0.0000030412 time: 0.18631958961486816\n",
      "Iteration: 5350 loss: 0.0000030070 time: 0.18329238891601562\n",
      "Iteration: 5360 loss: 0.0000029733 time: 0.190812349319458\n",
      "Iteration: 5370 loss: 0.0000029400 time: 0.18716740608215332\n",
      "Iteration: 5380 loss: 0.0000029072 time: 0.18220973014831543\n",
      "Iteration: 5390 loss: 0.0000028749 time: 0.18665552139282227\n",
      "Iteration: 5400 loss: 0.0000028430 time: 0.18816757202148438\n",
      "Iteration: 5410 loss: 0.0000028116 time: 0.19317293167114258\n",
      "Iteration: 5420 loss: 0.0000027806 time: 0.1848595142364502\n",
      "Iteration: 5430 loss: 0.0000027501 time: 0.18533945083618164\n",
      "Iteration: 5440 loss: 0.0000027201 time: 0.18006372451782227\n",
      "Iteration: 5450 loss: 0.0000026905 time: 0.18283748626708984\n",
      "Iteration: 5460 loss: 0.0000026613 time: 0.17638850212097168\n",
      "Iteration: 5470 loss: 0.0000026327 time: 0.18249273300170898\n",
      "Iteration: 5480 loss: 0.0000026044 time: 0.18443083763122559\n",
      "Iteration: 5490 loss: 0.0000025766 time: 0.19127726554870605\n",
      "Iteration: 5500 loss: 0.0000025492 time: 0.17865204811096191\n",
      "Iteration: 5510 loss: 0.0000025222 time: 0.18630337715148926\n",
      "Iteration: 5520 loss: 0.0000024956 time: 0.18655800819396973\n",
      "Iteration: 5530 loss: 0.0000024695 time: 0.18782424926757812\n",
      "Iteration: 5540 loss: 0.0000024437 time: 0.19274353981018066\n",
      "Iteration: 5550 loss: 0.0000024183 time: 0.18647241592407227\n",
      "Iteration: 5560 loss: 0.0000023934 time: 0.18024802207946777\n",
      "Iteration: 5570 loss: 0.0000023688 time: 0.18411946296691895\n",
      "Iteration: 5580 loss: 0.0000023445 time: 0.17853808403015137\n",
      "Iteration: 5590 loss: 0.0000023206 time: 0.1873173713684082\n",
      "Iteration: 5600 loss: 0.0000022971 time: 0.18621516227722168\n",
      "Iteration: 5610 loss: 0.0000022739 time: 0.1842808723449707\n",
      "Iteration: 5620 loss: 0.0000022511 time: 0.1795670986175537\n",
      "Iteration: 5630 loss: 0.0000022285 time: 0.18146967887878418\n",
      "Iteration: 5640 loss: 0.0000022063 time: 0.1762380599975586\n",
      "Iteration: 5650 loss: 0.0000021844 time: 0.18601632118225098\n",
      "Iteration: 5660 loss: 0.0000021627 time: 0.18495392799377441\n",
      "Iteration: 5670 loss: 0.0000021413 time: 0.17895817756652832\n",
      "Iteration: 5680 loss: 0.0000021203 time: 0.18247389793395996\n",
      "Iteration: 5690 loss: 0.0000020994 time: 0.1829822063446045\n",
      "Iteration: 5700 loss: 0.0000020788 time: 0.17976999282836914\n",
      "Iteration: 5710 loss: 0.0000020585 time: 0.17409038543701172\n",
      "Iteration: 5720 loss: 0.0000020384 time: 0.18407797813415527\n",
      "Iteration: 5730 loss: 0.0000020185 time: 0.17814874649047852\n",
      "Iteration: 5740 loss: 0.0000019988 time: 0.17383575439453125\n",
      "Iteration: 5750 loss: 0.0000019793 time: 0.1853165626525879\n",
      "Iteration: 5760 loss: 0.0000019600 time: 0.17978692054748535\n",
      "Iteration: 5770 loss: 0.0000019408 time: 0.18594789505004883\n",
      "Iteration: 5780 loss: 0.0000019219 time: 0.17673206329345703\n",
      "Iteration: 5790 loss: 0.0000019031 time: 0.18180346488952637\n",
      "Iteration: 5800 loss: 0.0000018845 time: 0.17772674560546875\n",
      "Iteration: 5810 loss: 0.0000018660 time: 0.17938685417175293\n",
      "Iteration: 5820 loss: 0.0000018476 time: 0.17850899696350098\n",
      "Iteration: 5830 loss: 0.0000018294 time: 0.17240095138549805\n",
      "Iteration: 5840 loss: 0.0000018113 time: 0.17996883392333984\n",
      "Iteration: 5850 loss: 0.0000017933 time: 0.177870512008667\n",
      "Iteration: 5860 loss: 0.0000017754 time: 0.17939996719360352\n",
      "Iteration: 5870 loss: 0.0000017576 time: 0.1786494255065918\n",
      "Iteration: 5880 loss: 0.0000017399 time: 0.1864612102508545\n",
      "Iteration: 5890 loss: 0.0000017223 time: 0.1807844638824463\n",
      "Iteration: 5900 loss: 0.0000017048 time: 0.17841768264770508\n",
      "Iteration: 5910 loss: 0.0000016873 time: 0.18647360801696777\n",
      "Iteration: 5920 loss: 0.0000016700 time: 0.18577146530151367\n",
      "Iteration: 5930 loss: 0.0000016526 time: 0.18076205253601074\n",
      "Iteration: 5940 loss: 0.0000016354 time: 0.18175816535949707\n",
      "Iteration: 5950 loss: 0.0000016182 time: 0.1903829574584961\n",
      "Iteration: 5960 loss: 0.0000016011 time: 0.18820691108703613\n",
      "Iteration: 5970 loss: 0.0000015840 time: 0.18198204040527344\n",
      "Iteration: 5980 loss: 0.0000015669 time: 0.18610239028930664\n",
      "Iteration: 5990 loss: 0.0000015499 time: 0.18845105171203613\n",
      "Iteration: 6000 loss: 0.0000015330 time: 0.18522429466247559\n",
      "Iteration: 6010 loss: 0.0000015161 time: 0.18805861473083496\n",
      "Iteration: 6020 loss: 0.0000014992 time: 0.1736743450164795\n",
      "Iteration: 6030 loss: 0.0000014824 time: 0.1772761344909668\n",
      "Iteration: 6040 loss: 0.0000014655 time: 0.1888294219970703\n",
      "Iteration: 6050 loss: 0.0000014488 time: 0.18082356452941895\n",
      "Iteration: 6060 loss: 0.0000014320 time: 0.1749897003173828\n",
      "Iteration: 6070 loss: 0.0000014153 time: 0.16381001472473145\n",
      "Iteration: 6080 loss: 0.0000013986 time: 0.18056941032409668\n",
      "Iteration: 6090 loss: 0.0000013820 time: 0.1751394271850586\n",
      "Iteration: 6100 loss: 0.0000013653 time: 0.17320966720581055\n",
      "Iteration: 6110 loss: 0.0000013488 time: 0.17862558364868164\n",
      "Iteration: 6120 loss: 0.0000013322 time: 0.17723488807678223\n",
      "Iteration: 6130 loss: 0.0000013157 time: 0.17820978164672852\n",
      "Iteration: 6140 loss: 0.0000012992 time: 0.17916488647460938\n",
      "Iteration: 6150 loss: 0.0000012827 time: 0.1862635612487793\n",
      "Iteration: 6160 loss: 0.0000012663 time: 0.18593859672546387\n",
      "Iteration: 6170 loss: 0.0000012499 time: 0.174269437789917\n",
      "Iteration: 6180 loss: 0.0000012335 time: 0.17860746383666992\n",
      "Iteration: 6190 loss: 0.0000012172 time: 0.17984223365783691\n",
      "Iteration: 6200 loss: 0.0000012009 time: 0.17725110054016113\n",
      "Iteration: 6210 loss: 0.0000011847 time: 0.18171238899230957\n",
      "Iteration: 6220 loss: 0.0000011685 time: 0.18394780158996582\n",
      "Iteration: 6230 loss: 0.0000011524 time: 0.1825108528137207\n",
      "Iteration: 6240 loss: 0.0000011363 time: 0.17857050895690918\n",
      "Iteration: 6250 loss: 0.0000011203 time: 0.17806458473205566\n",
      "Iteration: 6260 loss: 0.0000011043 time: 0.17725491523742676\n",
      "Iteration: 6270 loss: 0.0000010884 time: 0.1895458698272705\n",
      "Iteration: 6280 loss: 0.0000010726 time: 0.1849358081817627\n",
      "Iteration: 6290 loss: 0.0000010568 time: 0.1843113899230957\n",
      "Iteration: 6300 loss: 0.0000010411 time: 0.18519330024719238\n",
      "Iteration: 6310 loss: 0.0000010255 time: 0.17414331436157227\n",
      "Iteration: 6320 loss: 0.0000010099 time: 0.1772770881652832\n",
      "Iteration: 6330 loss: 0.0000009945 time: 0.17754101753234863\n",
      "Iteration: 6340 loss: 0.0000009791 time: 0.1840958595275879\n",
      "Iteration: 6350 loss: 0.0000009638 time: 0.17502117156982422\n",
      "Iteration: 6360 loss: 0.0000009486 time: 0.17918610572814941\n",
      "Iteration: 6370 loss: 0.0000009335 time: 0.18680667877197266\n",
      "Iteration: 6380 loss: 0.0000009185 time: 0.1874682903289795\n",
      "Iteration: 6390 loss: 0.0000009036 time: 0.1815781593322754\n",
      "Iteration: 6400 loss: 0.0000008888 time: 0.18268942832946777\n",
      "Iteration: 6410 loss: 0.0000008741 time: 0.18404865264892578\n",
      "Iteration: 6420 loss: 0.0000008595 time: 0.1800529956817627\n",
      "Iteration: 6430 loss: 0.0000008451 time: 0.17921161651611328\n",
      "Iteration: 6440 loss: 0.0000008308 time: 0.17828154563903809\n",
      "Iteration: 6450 loss: 0.0000008166 time: 0.1795051097869873\n",
      "Iteration: 6460 loss: 0.0000008025 time: 0.18165850639343262\n",
      "Iteration: 6470 loss: 0.0000007886 time: 0.17381978034973145\n",
      "Iteration: 6480 loss: 0.0000007749 time: 0.17649245262145996\n",
      "Iteration: 6490 loss: 0.0000007612 time: 0.1747143268585205\n",
      "Iteration: 6500 loss: 0.0000007478 time: 0.18013787269592285\n",
      "Iteration: 6510 loss: 0.0000007344 time: 0.17639517784118652\n",
      "Iteration: 6520 loss: 0.0000007213 time: 0.18150019645690918\n",
      "Iteration: 6530 loss: 0.0000007083 time: 0.18298935890197754\n",
      "Iteration: 6540 loss: 0.0000006954 time: 0.18310809135437012\n",
      "Iteration: 6550 loss: 0.0000006827 time: 0.18636679649353027\n",
      "Iteration: 6560 loss: 0.0000006702 time: 0.1732807159423828\n",
      "Iteration: 6570 loss: 0.0000006579 time: 0.1751260757446289\n",
      "Iteration: 6580 loss: 0.0000006457 time: 0.17807292938232422\n",
      "Iteration: 6590 loss: 0.0000006337 time: 0.18314051628112793\n",
      "Iteration: 6600 loss: 0.0000006219 time: 0.1797330379486084\n",
      "Iteration: 6610 loss: 0.0000006103 time: 0.1806182861328125\n",
      "Iteration: 6620 loss: 0.0000005989 time: 0.1898667812347412\n",
      "Iteration: 6630 loss: 0.0000005877 time: 0.17663359642028809\n",
      "Iteration: 6640 loss: 0.0000005766 time: 0.19960427284240723\n",
      "Iteration: 6650 loss: 0.0000005658 time: 0.18840622901916504\n",
      "Iteration: 6660 loss: 0.0000005551 time: 0.18763160705566406\n",
      "Iteration: 6670 loss: 0.0000005446 time: 0.18142080307006836\n",
      "Iteration: 6680 loss: 0.0000005344 time: 0.1847991943359375\n",
      "Iteration: 6690 loss: 0.0000005243 time: 0.1774134635925293\n",
      "Iteration: 6700 loss: 0.0000005144 time: 0.18106436729431152\n",
      "Iteration: 6710 loss: 0.0000005047 time: 0.18477320671081543\n",
      "Iteration: 6720 loss: 0.0000004952 time: 0.19155478477478027\n",
      "Iteration: 6730 loss: 0.0000004859 time: 0.180708646774292\n",
      "Iteration: 6740 loss: 0.0000004769 time: 0.18097805976867676\n",
      "Iteration: 6750 loss: 0.0000004680 time: 0.18488001823425293\n",
      "Iteration: 6760 loss: 0.0000004593 time: 0.18960189819335938\n",
      "Iteration: 6770 loss: 0.0000004508 time: 0.18736743927001953\n",
      "Iteration: 6780 loss: 0.0000004425 time: 0.1792750358581543\n",
      "Iteration: 6790 loss: 0.0000004343 time: 0.18262815475463867\n",
      "Iteration: 6800 loss: 0.0000004264 time: 0.18924999237060547\n",
      "Iteration: 6810 loss: 0.0000004187 time: 0.18187379837036133\n",
      "Iteration: 6820 loss: 0.0000004111 time: 0.17787647247314453\n",
      "Iteration: 6830 loss: 0.0000004038 time: 0.1814413070678711\n",
      "Iteration: 6840 loss: 0.0000003966 time: 0.1835649013519287\n",
      "Iteration: 6850 loss: 0.0000003896 time: 0.17883849143981934\n",
      "Iteration: 6860 loss: 0.0000003828 time: 0.19535231590270996\n",
      "Iteration: 6870 loss: 0.0000003762 time: 0.17994141578674316\n",
      "Iteration: 6880 loss: 0.0000003697 time: 0.1841294765472412\n",
      "Iteration: 6890 loss: 0.0000003634 time: 0.17832279205322266\n",
      "Iteration: 6900 loss: 0.0000003573 time: 0.1796722412109375\n",
      "Iteration: 6910 loss: 0.0000003514 time: 0.18568706512451172\n",
      "Iteration: 6920 loss: 0.0000003456 time: 0.18335747718811035\n",
      "Iteration: 6930 loss: 0.0000003400 time: 0.18426156044006348\n",
      "Iteration: 6940 loss: 0.0000003345 time: 0.18772435188293457\n",
      "Iteration: 6950 loss: 0.0000003292 time: 0.1844346523284912\n",
      "Iteration: 6960 loss: 0.0000003240 time: 0.1870100498199463\n",
      "Iteration: 6970 loss: 0.0000003190 time: 0.18537449836730957\n",
      "Iteration: 6980 loss: 0.0000003141 time: 0.1888103485107422\n",
      "Iteration: 6990 loss: 0.0000003094 time: 0.18073439598083496\n",
      "Iteration: 7000 loss: 0.0000003048 time: 0.18376874923706055\n",
      "Iteration: 7010 loss: 0.0000003003 time: 0.18157720565795898\n",
      "Iteration: 7020 loss: 0.0000002960 time: 0.18304204940795898\n",
      "Iteration: 7030 loss: 0.0000002918 time: 0.1837928295135498\n",
      "Iteration: 7040 loss: 0.0000002877 time: 0.18949174880981445\n",
      "Iteration: 7050 loss: 0.0000002837 time: 0.18954825401306152\n",
      "Iteration: 7060 loss: 0.0000002799 time: 0.19035959243774414\n",
      "Iteration: 7070 loss: 0.0000002761 time: 0.18117356300354004\n",
      "Iteration: 7080 loss: 0.0000002725 time: 0.17633652687072754\n",
      "Iteration: 7090 loss: 0.0000002690 time: 0.18254876136779785\n",
      "Iteration: 7100 loss: 0.0000002656 time: 0.19057059288024902\n",
      "Iteration: 7110 loss: 0.0000002623 time: 0.1861734390258789\n",
      "Iteration: 7120 loss: 0.0000002591 time: 0.18212199211120605\n",
      "Iteration: 7130 loss: 0.0000002560 time: 0.17828607559204102\n",
      "Iteration: 7140 loss: 0.0000002529 time: 0.18697690963745117\n",
      "Iteration: 7150 loss: 0.0000002500 time: 0.1777489185333252\n",
      "Iteration: 7160 loss: 0.0000002472 time: 0.1822044849395752\n",
      "Iteration: 7170 loss: 0.0000002444 time: 0.17137837409973145\n",
      "Iteration: 7180 loss: 0.0000002417 time: 0.18714118003845215\n",
      "Iteration: 7190 loss: 0.0000002391 time: 0.18475103378295898\n",
      "Iteration: 7200 loss: 0.0000002366 time: 0.17681312561035156\n",
      "Iteration: 7210 loss: 0.0000002341 time: 0.18106913566589355\n",
      "Iteration: 7220 loss: 0.0000002317 time: 0.17502140998840332\n",
      "Iteration: 7230 loss: 0.0000002294 time: 0.18006205558776855\n",
      "Iteration: 7240 loss: 0.0000002272 time: 0.18218493461608887\n",
      "Iteration: 7250 loss: 0.0000002250 time: 0.1992502212524414\n",
      "Iteration: 7260 loss: 0.0000002229 time: 0.1839158535003662\n",
      "Iteration: 7270 loss: 0.0000002208 time: 0.17906594276428223\n",
      "Iteration: 7280 loss: 0.0000002188 time: 0.17746758460998535\n",
      "Iteration: 7290 loss: 0.0000002168 time: 0.18265151977539062\n",
      "Iteration: 7300 loss: 0.0000002149 time: 0.1810297966003418\n",
      "Iteration: 7310 loss: 0.0000002131 time: 0.18167996406555176\n",
      "Iteration: 7320 loss: 0.0000002113 time: 0.17920207977294922\n",
      "Iteration: 7330 loss: 0.0000002095 time: 0.18526482582092285\n",
      "Iteration: 7340 loss: 0.0000002078 time: 0.18279647827148438\n",
      "Iteration: 7350 loss: 0.0000002062 time: 0.1772139072418213\n",
      "Iteration: 7360 loss: 0.0000002046 time: 0.18121552467346191\n",
      "Iteration: 7370 loss: 0.0000002030 time: 0.1726670265197754\n",
      "Iteration: 7380 loss: 0.0000002015 time: 0.17868924140930176\n",
      "Iteration: 7390 loss: 0.0000002000 time: 0.18668246269226074\n",
      "Iteration: 7400 loss: 0.0000001985 time: 0.17905282974243164\n",
      "Iteration: 7410 loss: 0.0000001971 time: 0.18553614616394043\n",
      "Iteration: 7420 loss: 0.0000001957 time: 0.18108320236206055\n",
      "Iteration: 7430 loss: 0.0000001943 time: 0.1785736083984375\n",
      "Iteration: 7440 loss: 0.0000001930 time: 0.18227362632751465\n",
      "Iteration: 7450 loss: 0.0000001917 time: 0.1766979694366455\n",
      "Iteration: 7460 loss: 0.0000001904 time: 0.18416810035705566\n",
      "Iteration: 7470 loss: 0.0000001892 time: 0.17979097366333008\n",
      "Iteration: 7480 loss: 0.0000001880 time: 0.17987537384033203\n",
      "Iteration: 7490 loss: 0.0000001868 time: 0.19405245780944824\n",
      "Iteration: 7500 loss: 0.0000001857 time: 0.18358826637268066\n",
      "Iteration: 7510 loss: 0.0000001845 time: 0.18202900886535645\n",
      "Iteration: 7520 loss: 0.0000001834 time: 0.1861402988433838\n",
      "Iteration: 7530 loss: 0.0000001823 time: 0.18317031860351562\n",
      "Iteration: 7540 loss: 0.0000001813 time: 0.1862187385559082\n",
      "Iteration: 7550 loss: 0.0000001802 time: 0.17716264724731445\n",
      "Iteration: 7560 loss: 0.0000001792 time: 0.18411827087402344\n",
      "Iteration: 7570 loss: 0.0000001782 time: 0.18074607849121094\n",
      "Iteration: 7580 loss: 0.0000001772 time: 0.1848435401916504\n",
      "Iteration: 7590 loss: 0.0000001762 time: 0.18078851699829102\n",
      "Iteration: 7600 loss: 0.0000001753 time: 0.18821239471435547\n",
      "Iteration: 7610 loss: 0.0000001744 time: 0.1875016689300537\n",
      "Iteration: 7620 loss: 0.0000001734 time: 0.17583465576171875\n",
      "Iteration: 7630 loss: 0.0000001725 time: 0.18448209762573242\n",
      "Iteration: 7640 loss: 0.0000001717 time: 0.18717169761657715\n",
      "Iteration: 7650 loss: 0.0000001708 time: 0.18761372566223145\n",
      "Iteration: 7660 loss: 0.0000001699 time: 0.1890730857849121\n",
      "Iteration: 7670 loss: 0.0000001691 time: 0.17998886108398438\n",
      "Iteration: 7680 loss: 0.0000001682 time: 0.1821432113647461\n",
      "Iteration: 7690 loss: 0.0000001674 time: 0.1913893222808838\n",
      "Iteration: 7700 loss: 0.0000001666 time: 0.18709301948547363\n",
      "Iteration: 7710 loss: 0.0000001658 time: 0.17507290840148926\n",
      "Iteration: 7720 loss: 0.0000001650 time: 0.179548978805542\n",
      "Iteration: 7730 loss: 0.0000001643 time: 0.18265676498413086\n",
      "Iteration: 7740 loss: 0.0000001635 time: 0.1901097297668457\n",
      "Iteration: 7750 loss: 0.0000001627 time: 0.18914175033569336\n",
      "Iteration: 7760 loss: 0.0000001620 time: 0.18458914756774902\n",
      "Iteration: 7770 loss: 0.0000001613 time: 0.1849045753479004\n",
      "Iteration: 7780 loss: 0.0000001605 time: 0.1876819133758545\n",
      "Iteration: 7790 loss: 0.0000001598 time: 0.18248939514160156\n",
      "Iteration: 7800 loss: 0.0000001591 time: 0.1763465404510498\n",
      "Iteration: 7810 loss: 0.0000001584 time: 0.18773317337036133\n",
      "Iteration: 7820 loss: 0.0000001577 time: 0.18218636512756348\n",
      "Iteration: 7830 loss: 0.0000001570 time: 0.18114423751831055\n",
      "Iteration: 7840 loss: 0.0000001563 time: 0.17649412155151367\n",
      "Iteration: 7850 loss: 0.0000001557 time: 0.18427157402038574\n",
      "Iteration: 7860 loss: 0.0000001550 time: 0.18362808227539062\n",
      "Iteration: 7870 loss: 0.0000001543 time: 0.18953752517700195\n",
      "Iteration: 7880 loss: 0.0000001537 time: 0.18208742141723633\n",
      "Iteration: 7890 loss: 0.0000001530 time: 0.17650437355041504\n",
      "Iteration: 7900 loss: 0.0000001524 time: 0.18788433074951172\n",
      "Iteration: 7910 loss: 0.0000001517 time: 0.18817734718322754\n",
      "Iteration: 7920 loss: 0.0000001511 time: 0.1837937831878662\n",
      "Iteration: 7930 loss: 0.0000001505 time: 0.1835780143737793\n",
      "Iteration: 7940 loss: 0.0000001499 time: 0.18893694877624512\n",
      "Iteration: 7950 loss: 0.0000001493 time: 0.18578624725341797\n",
      "Iteration: 7960 loss: 0.0000001486 time: 0.17946195602416992\n",
      "Iteration: 7970 loss: 0.0000001480 time: 0.18197417259216309\n",
      "Iteration: 7980 loss: 0.0000001474 time: 0.1845715045928955\n",
      "Iteration: 7990 loss: 0.0000001468 time: 0.18175005912780762\n",
      "Iteration: 8000 loss: 0.0000001462 time: 0.1877002716064453\n",
      "Iteration: 8010 loss: 0.0000001456 time: 0.1921837329864502\n",
      "Iteration: 8020 loss: 0.0000001451 time: 0.19012451171875\n",
      "Iteration: 8030 loss: 0.0000001445 time: 0.1751880645751953\n",
      "Iteration: 8040 loss: 0.0000001439 time: 0.18354153633117676\n",
      "Iteration: 8050 loss: 0.0000001433 time: 0.18864822387695312\n",
      "Iteration: 8060 loss: 0.0000001427 time: 0.1902320384979248\n",
      "Iteration: 8070 loss: 0.0000001422 time: 0.18433260917663574\n",
      "Iteration: 8080 loss: 0.0000001416 time: 0.1812453269958496\n",
      "Iteration: 8090 loss: 0.0000001411 time: 0.18567943572998047\n",
      "Iteration: 8100 loss: 0.0000001405 time: 0.18420815467834473\n",
      "Iteration: 8110 loss: 0.0000001399 time: 0.18621301651000977\n",
      "Iteration: 8120 loss: 0.0000001394 time: 0.18990373611450195\n",
      "Iteration: 8130 loss: 0.0000001388 time: 0.18639540672302246\n",
      "Iteration: 8140 loss: 0.0000001383 time: 0.18866467475891113\n",
      "Iteration: 8150 loss: 0.0000001377 time: 0.18261265754699707\n",
      "Iteration: 8160 loss: 0.0000001372 time: 0.18040728569030762\n",
      "Iteration: 8170 loss: 0.0000001367 time: 0.186201810836792\n",
      "Iteration: 8180 loss: 0.0000001361 time: 0.18833589553833008\n",
      "Iteration: 8190 loss: 0.0000001356 time: 0.18396830558776855\n",
      "Iteration: 8200 loss: 0.0000001351 time: 0.1830906867980957\n",
      "Iteration: 8210 loss: 0.0000001345 time: 0.1904890537261963\n",
      "Iteration: 8220 loss: 0.0000001340 time: 0.18808579444885254\n",
      "Iteration: 8230 loss: 0.0000001335 time: 0.18323564529418945\n",
      "Iteration: 8240 loss: 0.0000001329 time: 0.18519973754882812\n",
      "Iteration: 8250 loss: 0.0000001324 time: 0.18609404563903809\n",
      "Iteration: 8260 loss: 0.0000001319 time: 0.1871781349182129\n",
      "Iteration: 8270 loss: 0.0000001314 time: 0.1800394058227539\n",
      "Iteration: 8280 loss: 0.0000001309 time: 0.18648457527160645\n",
      "Iteration: 8290 loss: 0.0000001304 time: 0.18813252449035645\n",
      "Iteration: 8300 loss: 0.0000001298 time: 0.18251943588256836\n",
      "Iteration: 8310 loss: 0.0000001293 time: 0.18996572494506836\n",
      "Iteration: 8320 loss: 0.0000001288 time: 0.18651747703552246\n",
      "Iteration: 8330 loss: 0.0000001283 time: 0.18121862411499023\n",
      "Iteration: 8340 loss: 0.0000001278 time: 0.18082594871520996\n",
      "Iteration: 8350 loss: 0.0000001273 time: 0.1855180263519287\n",
      "Iteration: 8360 loss: 0.0000001268 time: 0.18825149536132812\n",
      "Iteration: 8370 loss: 0.0000001263 time: 0.1784687042236328\n",
      "Iteration: 8380 loss: 0.0000001258 time: 0.17446160316467285\n",
      "Iteration: 8390 loss: 0.0000001253 time: 0.17950057983398438\n",
      "Iteration: 8400 loss: 0.0000001248 time: 0.1787550449371338\n",
      "Iteration: 8410 loss: 0.0000001243 time: 0.18222427368164062\n",
      "Iteration: 8420 loss: 0.0000001238 time: 0.18625569343566895\n",
      "Iteration: 8430 loss: 0.0000001234 time: 0.18215107917785645\n",
      "Iteration: 8440 loss: 0.0000001229 time: 0.18790674209594727\n",
      "Iteration: 8450 loss: 0.0000001224 time: 0.1852128505706787\n",
      "Iteration: 8460 loss: 0.0000001219 time: 0.18087172508239746\n",
      "Iteration: 8470 loss: 0.0000001214 time: 0.1767261028289795\n",
      "Iteration: 8480 loss: 0.0000001209 time: 0.17928004264831543\n",
      "Iteration: 8490 loss: 0.0000001204 time: 0.18601131439208984\n",
      "Iteration: 8500 loss: 0.0000001200 time: 0.189314603805542\n",
      "Iteration: 8510 loss: 0.0000001195 time: 0.18488121032714844\n",
      "Iteration: 8520 loss: 0.0000001190 time: 0.1869187355041504\n",
      "Iteration: 8530 loss: 0.0000001185 time: 0.1937875747680664\n",
      "Iteration: 8540 loss: 0.0000001181 time: 0.18886971473693848\n",
      "Iteration: 8550 loss: 0.0000001176 time: 0.18969964981079102\n",
      "Iteration: 8560 loss: 0.0000001171 time: 0.18333673477172852\n",
      "Iteration: 8570 loss: 0.0000001166 time: 0.18940472602844238\n",
      "Iteration: 8580 loss: 0.0000001162 time: 0.1866743564605713\n",
      "Iteration: 8590 loss: 0.0000001157 time: 0.18563389778137207\n",
      "Iteration: 8600 loss: 0.0000001152 time: 0.18664836883544922\n",
      "Iteration: 8610 loss: 0.0000001148 time: 0.18011188507080078\n",
      "Iteration: 8620 loss: 0.0000001143 time: 0.1835012435913086\n",
      "Iteration: 8630 loss: 0.0000001138 time: 0.18646669387817383\n",
      "Iteration: 8640 loss: 0.0000001134 time: 0.18505382537841797\n",
      "Iteration: 8650 loss: 0.0000001129 time: 0.17757678031921387\n",
      "Iteration: 8660 loss: 0.0000001125 time: 0.18817400932312012\n",
      "Iteration: 8670 loss: 0.0000001120 time: 0.1843278408050537\n",
      "Iteration: 8680 loss: 0.0000001115 time: 0.18907690048217773\n",
      "Iteration: 8690 loss: 0.0000001111 time: 0.19221901893615723\n",
      "Iteration: 8700 loss: 0.0000001106 time: 0.17531824111938477\n",
      "Iteration: 8710 loss: 0.0000001102 time: 0.18472933769226074\n",
      "Iteration: 8720 loss: 0.0000001097 time: 0.18994593620300293\n",
      "Iteration: 8730 loss: 0.0000001093 time: 0.18564796447753906\n",
      "Iteration: 8740 loss: 0.0000001088 time: 0.18139910697937012\n",
      "Iteration: 8750 loss: 0.0000001084 time: 0.18790507316589355\n",
      "Iteration: 8760 loss: 0.0000001079 time: 0.18962812423706055\n",
      "Iteration: 8770 loss: 0.0000001075 time: 0.1833641529083252\n",
      "Iteration: 8780 loss: 0.0000001070 time: 0.18271875381469727\n",
      "Iteration: 8790 loss: 0.0000001066 time: 0.18598055839538574\n",
      "Iteration: 8800 loss: 0.0000001061 time: 0.17467498779296875\n",
      "Iteration: 8810 loss: 0.0000001057 time: 0.18177580833435059\n",
      "Iteration: 8820 loss: 0.0000001052 time: 0.18185091018676758\n",
      "Iteration: 8830 loss: 0.0000001048 time: 0.1787259578704834\n",
      "Iteration: 8840 loss: 0.0000001044 time: 0.18990659713745117\n",
      "Iteration: 8850 loss: 0.0000001039 time: 0.1871321201324463\n",
      "Iteration: 8860 loss: 0.0000001035 time: 0.18359136581420898\n",
      "Iteration: 8870 loss: 0.0000001030 time: 0.18504905700683594\n",
      "Iteration: 8880 loss: 0.0000001026 time: 0.19364452362060547\n",
      "Iteration: 8890 loss: 0.0000001021 time: 0.1832294464111328\n",
      "Iteration: 8900 loss: 0.0000001017 time: 0.18385601043701172\n",
      "Iteration: 8910 loss: 0.0000001013 time: 0.17056965827941895\n",
      "Iteration: 8920 loss: 0.0000001008 time: 0.1823577880859375\n",
      "Iteration: 8930 loss: 0.0000001004 time: 0.1804797649383545\n",
      "Iteration: 8940 loss: 0.0000001000 time: 0.18226003646850586\n",
      "Iteration: 8950 loss: 0.0000000995 time: 0.18138957023620605\n",
      "Iteration: 8960 loss: 0.0000000991 time: 0.18111824989318848\n",
      "Iteration: 8970 loss: 0.0000000987 time: 0.181718111038208\n",
      "Iteration: 8980 loss: 0.0000000982 time: 0.18819117546081543\n",
      "Iteration: 8990 loss: 0.0000000978 time: 0.19002914428710938\n",
      "Iteration: 9000 loss: 0.0000000974 time: 0.18930816650390625\n",
      "Iteration: 9010 loss: 0.0000000969 time: 0.19581890106201172\n",
      "Iteration: 9020 loss: 0.0000000965 time: 0.1921694278717041\n",
      "Iteration: 9030 loss: 0.0000000961 time: 0.18241143226623535\n",
      "Iteration: 9040 loss: 0.0000000956 time: 0.18787813186645508\n",
      "Iteration: 9050 loss: 0.0000000952 time: 0.19016528129577637\n",
      "Iteration: 9060 loss: 0.0000000948 time: 0.1954030990600586\n",
      "Iteration: 9070 loss: 0.0000000944 time: 0.18506836891174316\n",
      "Iteration: 9080 loss: 0.0000000939 time: 0.18802714347839355\n",
      "Iteration: 9090 loss: 0.0000000935 time: 0.17890381813049316\n",
      "Iteration: 9100 loss: 0.0000000931 time: 0.18564128875732422\n",
      "Iteration: 9110 loss: 0.0000000927 time: 0.186417818069458\n",
      "Iteration: 9120 loss: 0.0000000922 time: 0.1870121955871582\n",
      "Iteration: 9130 loss: 0.0000000918 time: 0.1822645664215088\n",
      "Iteration: 9140 loss: 0.0000000914 time: 0.18469810485839844\n",
      "Iteration: 9150 loss: 0.0000000910 time: 0.19040751457214355\n",
      "Iteration: 9160 loss: 0.0000000905 time: 0.18571043014526367\n",
      "Iteration: 9170 loss: 0.0000000901 time: 0.18475627899169922\n",
      "Iteration: 9180 loss: 0.0000000897 time: 0.18184828758239746\n",
      "Iteration: 9190 loss: 0.0000000893 time: 0.17792129516601562\n",
      "Iteration: 9200 loss: 0.0000000888 time: 0.18593382835388184\n",
      "Iteration: 9210 loss: 0.0000000884 time: 0.18575763702392578\n",
      "Iteration: 9220 loss: 0.0000000880 time: 0.1875009536743164\n",
      "Iteration: 9230 loss: 0.0000000876 time: 0.19019055366516113\n",
      "Iteration: 9240 loss: 0.0000000872 time: 0.19611430168151855\n",
      "Iteration: 9250 loss: 0.0000000867 time: 0.1929633617401123\n",
      "Iteration: 9260 loss: 0.0000000863 time: 0.19234466552734375\n",
      "Iteration: 9270 loss: 0.0000000859 time: 0.18250179290771484\n",
      "Iteration: 9280 loss: 0.0000000855 time: 0.18578386306762695\n",
      "Iteration: 9290 loss: 0.0000000851 time: 0.18648815155029297\n",
      "Iteration: 9300 loss: 0.0000000847 time: 0.18839192390441895\n",
      "Iteration: 9310 loss: 0.0000000842 time: 0.18319416046142578\n",
      "Iteration: 9320 loss: 0.0000000838 time: 0.1841573715209961\n",
      "Iteration: 9330 loss: 0.0000000834 time: 0.18177533149719238\n",
      "Iteration: 9340 loss: 0.0000000830 time: 0.17727065086364746\n",
      "Iteration: 9350 loss: 0.0000000826 time: 0.18501973152160645\n",
      "Iteration: 9360 loss: 0.0000000822 time: 0.18642354011535645\n",
      "Iteration: 9370 loss: 0.0000000817 time: 0.1843273639678955\n",
      "Iteration: 9380 loss: 0.0000000813 time: 0.18464159965515137\n",
      "Iteration: 9390 loss: 0.0000000809 time: 0.17642951011657715\n",
      "Iteration: 9400 loss: 0.0000000805 time: 0.18865132331848145\n",
      "Iteration: 9410 loss: 0.0000000801 time: 0.1870417594909668\n",
      "Iteration: 9420 loss: 0.0000000797 time: 0.1851198673248291\n",
      "Iteration: 9430 loss: 0.0000000792 time: 0.18965458869934082\n",
      "Iteration: 9440 loss: 0.0000000788 time: 0.18829989433288574\n",
      "Iteration: 9450 loss: 0.0000000784 time: 0.18494725227355957\n",
      "Iteration: 9460 loss: 0.0000000780 time: 0.18521928787231445\n",
      "Iteration: 9470 loss: 0.0000000776 time: 0.18355417251586914\n",
      "Iteration: 9480 loss: 0.0000000772 time: 0.1836094856262207\n",
      "Iteration: 9490 loss: 0.0000000768 time: 0.18446707725524902\n",
      "Iteration: 9500 loss: 0.0000000763 time: 0.1827681064605713\n",
      "Iteration: 9510 loss: 0.0000000759 time: 0.18034124374389648\n",
      "Iteration: 9520 loss: 0.0000000755 time: 0.18217086791992188\n",
      "Iteration: 9530 loss: 0.0000000751 time: 0.1791069507598877\n",
      "Iteration: 9540 loss: 0.0000000747 time: 0.17557501792907715\n",
      "Iteration: 9550 loss: 0.0000000743 time: 0.18787884712219238\n",
      "Iteration: 9560 loss: 0.0000000739 time: 0.18052196502685547\n",
      "Iteration: 9570 loss: 0.0000000735 time: 0.17731976509094238\n",
      "Iteration: 9580 loss: 0.0000000730 time: 0.18599581718444824\n",
      "Iteration: 9590 loss: 0.0000000726 time: 0.1883394718170166\n",
      "Iteration: 9600 loss: 0.0000000722 time: 0.1778125762939453\n",
      "Iteration: 9610 loss: 0.0000000718 time: 0.18455266952514648\n",
      "Iteration: 9620 loss: 0.0000000714 time: 0.18316864967346191\n",
      "Iteration: 9630 loss: 0.0000000710 time: 0.1813499927520752\n",
      "Iteration: 9640 loss: 0.0000000706 time: 0.1828300952911377\n",
      "Iteration: 9650 loss: 0.0000000701 time: 0.1819443702697754\n",
      "Iteration: 9660 loss: 0.0000000697 time: 0.18516230583190918\n",
      "Iteration: 9670 loss: 0.0000000693 time: 0.18818283081054688\n",
      "Iteration: 9680 loss: 0.0000000689 time: 0.1769094467163086\n",
      "Iteration: 9690 loss: 0.0000000685 time: 0.18078923225402832\n",
      "Iteration: 9700 loss: 0.0000000681 time: 0.18394804000854492\n",
      "Iteration: 9710 loss: 0.0000000677 time: 0.185089111328125\n",
      "Iteration: 9720 loss: 0.0000000673 time: 0.1823101043701172\n",
      "Iteration: 9730 loss: 0.0000000668 time: 0.17748332023620605\n",
      "Iteration: 9740 loss: 0.0000000664 time: 0.18086934089660645\n",
      "Iteration: 9750 loss: 0.0000000660 time: 0.1847543716430664\n",
      "Iteration: 9760 loss: 0.0000000656 time: 0.1902003288269043\n",
      "Iteration: 9770 loss: 0.0000000652 time: 0.18798375129699707\n",
      "Iteration: 9780 loss: 0.0000000648 time: 0.1842803955078125\n",
      "Iteration: 9790 loss: 0.0000000644 time: 0.18669509887695312\n",
      "Iteration: 9800 loss: 0.0000000639 time: 0.17760229110717773\n",
      "Iteration: 9810 loss: 0.0000000635 time: 0.18193364143371582\n",
      "Iteration: 9820 loss: 0.0000000631 time: 0.1918349266052246\n",
      "Iteration: 9830 loss: 0.0000000627 time: 0.18108272552490234\n",
      "Iteration: 9840 loss: 0.0000000623 time: 0.18821930885314941\n",
      "Iteration: 9850 loss: 0.0000000619 time: 0.18812870979309082\n",
      "Iteration: 9860 loss: 0.0000000615 time: 0.18084144592285156\n",
      "Iteration: 9870 loss: 0.0000000610 time: 0.18605780601501465\n",
      "Iteration: 9880 loss: 0.0000000606 time: 0.18683671951293945\n",
      "Iteration: 9890 loss: 0.0000000602 time: 0.18185925483703613\n",
      "Iteration: 9900 loss: 0.0000000598 time: 0.18979549407958984\n",
      "Iteration: 9910 loss: 0.0000000594 time: 0.18416452407836914\n",
      "Iteration: 9920 loss: 0.0000000590 time: 0.18842244148254395\n",
      "Iteration: 9930 loss: 0.0000000586 time: 0.17849445343017578\n",
      "Iteration: 9940 loss: 0.0000000582 time: 0.18631887435913086\n",
      "Iteration: 9950 loss: 0.0000000577 time: 0.1946871280670166\n",
      "Iteration: 9960 loss: 0.0000000718 time: 0.17753386497497559\n",
      "Iteration: 9970 loss: 0.0000000596 time: 0.18280243873596191\n",
      "Iteration: 9980 loss: 0.0000000585 time: 0.18894124031066895\n",
      "Iteration: 9990 loss: 0.0000000563 time: 0.19383788108825684\n",
      "Iteration: 10000 loss: 0.0000000559 time: 0.184157133102417\n",
      "Iteration: 10010 loss: 0.0000000555 time: 0.18742108345031738\n",
      "Iteration: 10020 loss: 0.0000000550 time: 0.1878373622894287\n",
      "Iteration: 10030 loss: 0.0000000546 time: 0.18991732597351074\n",
      "Iteration: 10040 loss: 0.0000000543 time: 0.182175874710083\n",
      "Iteration: 10050 loss: 0.0000000539 time: 0.1810131072998047\n",
      "Iteration: 10060 loss: 0.0000000535 time: 0.18805789947509766\n",
      "Iteration: 10070 loss: 0.0000000531 time: 0.19240427017211914\n",
      "Iteration: 10080 loss: 0.0000000527 time: 0.18067073822021484\n",
      "Iteration: 10090 loss: 0.0000000523 time: 0.18565726280212402\n",
      "Iteration: 10100 loss: 0.0000000520 time: 0.18218278884887695\n",
      "Iteration: 10110 loss: 0.0000000516 time: 0.18484115600585938\n",
      "Iteration: 10120 loss: 0.0000000512 time: 0.1903071403503418\n",
      "Iteration: 10130 loss: 0.0000000508 time: 0.1834397315979004\n",
      "Iteration: 10140 loss: 0.0000000504 time: 0.18323707580566406\n",
      "Iteration: 10150 loss: 0.0000000501 time: 0.185502290725708\n",
      "Iteration: 10160 loss: 0.0000000497 time: 0.1826784610748291\n",
      "Iteration: 10170 loss: 0.0000000493 time: 0.18174219131469727\n",
      "Iteration: 10180 loss: 0.0000000489 time: 0.1840827465057373\n",
      "Iteration: 10190 loss: 0.0000000485 time: 0.190230131149292\n",
      "Iteration: 10200 loss: 0.0000000481 time: 0.1959083080291748\n",
      "Iteration: 10210 loss: 0.0000000478 time: 0.18460392951965332\n",
      "Iteration: 10220 loss: 0.0000000474 time: 0.18235421180725098\n",
      "Iteration: 10230 loss: 0.0000000470 time: 0.18372249603271484\n",
      "Iteration: 10240 loss: 0.0000000466 time: 0.18605589866638184\n",
      "Iteration: 10250 loss: 0.0000000462 time: 0.18397736549377441\n",
      "Iteration: 10260 loss: 0.0000000459 time: 0.1911153793334961\n",
      "Iteration: 10270 loss: 0.0000000455 time: 0.17940425872802734\n",
      "Iteration: 10280 loss: 0.0000000451 time: 0.1886281967163086\n",
      "Iteration: 10290 loss: 0.0000000447 time: 0.18534350395202637\n",
      "Iteration: 10300 loss: 0.0000000443 time: 0.18689942359924316\n",
      "Iteration: 10310 loss: 0.0000000439 time: 0.19327068328857422\n",
      "Iteration: 10320 loss: 0.0000000436 time: 0.18579769134521484\n",
      "Iteration: 10330 loss: 0.0000000432 time: 0.1911144256591797\n",
      "Iteration: 10340 loss: 0.0000000430 time: 0.19150829315185547\n",
      "Iteration: 10350 loss: 0.0000000630 time: 0.18223357200622559\n",
      "Iteration: 10360 loss: 0.0000000482 time: 0.18718647956848145\n",
      "Iteration: 10370 loss: 0.0000000421 time: 0.19095563888549805\n",
      "Iteration: 10380 loss: 0.0000000416 time: 0.18733859062194824\n",
      "Iteration: 10390 loss: 0.0000000413 time: 0.1876845359802246\n",
      "Iteration: 10400 loss: 0.0000000408 time: 0.18888139724731445\n",
      "Iteration: 10410 loss: 0.0000000404 time: 0.18436145782470703\n",
      "Iteration: 10420 loss: 0.0000000400 time: 0.1920480728149414\n",
      "Iteration: 10430 loss: 0.0000000397 time: 0.18585920333862305\n",
      "Iteration: 10440 loss: 0.0000000393 time: 0.1923537254333496\n",
      "Iteration: 10450 loss: 0.0000000390 time: 0.1755521297454834\n",
      "Iteration: 10460 loss: 0.0000000387 time: 0.17775654792785645\n",
      "Iteration: 10470 loss: 0.0000000383 time: 0.17948460578918457\n",
      "Iteration: 10480 loss: 0.0000000380 time: 0.19160938262939453\n",
      "Iteration: 10490 loss: 0.0000000376 time: 0.18781518936157227\n",
      "Iteration: 10500 loss: 0.0000000373 time: 0.18053746223449707\n",
      "Iteration: 10510 loss: 0.0000000370 time: 0.18067502975463867\n",
      "Iteration: 10520 loss: 0.0000000366 time: 0.18451714515686035\n",
      "Iteration: 10530 loss: 0.0000000363 time: 0.18548154830932617\n",
      "Iteration: 10540 loss: 0.0000000359 time: 0.19075250625610352\n",
      "Iteration: 10550 loss: 0.0000000356 time: 0.1884608268737793\n",
      "Iteration: 10560 loss: 0.0000000353 time: 0.1828141212463379\n",
      "Iteration: 10570 loss: 0.0000000349 time: 0.18452930450439453\n",
      "Iteration: 10580 loss: 0.0000000346 time: 0.18416929244995117\n",
      "Iteration: 10590 loss: 0.0000000343 time: 0.1902754306793213\n",
      "Iteration: 10600 loss: 0.0000000343 time: 0.18572235107421875\n",
      "Iteration: 10610 loss: 0.0000000372 time: 0.1928730010986328\n",
      "Iteration: 10620 loss: 0.0000000352 time: 0.18498539924621582\n",
      "Iteration: 10630 loss: 0.0000000331 time: 0.19036412239074707\n",
      "Iteration: 10640 loss: 0.0000000327 time: 0.17375922203063965\n",
      "Iteration: 10650 loss: 0.0000000324 time: 0.18797540664672852\n",
      "Iteration: 10660 loss: 0.0000000320 time: 0.1865522861480713\n",
      "Iteration: 10670 loss: 0.0000000317 time: 0.1919398307800293\n",
      "Iteration: 10680 loss: 0.0000000314 time: 0.1900043487548828\n",
      "Iteration: 10690 loss: 0.0000000311 time: 0.18132734298706055\n",
      "Iteration: 10700 loss: 0.0000000308 time: 0.18592190742492676\n",
      "Iteration: 10710 loss: 0.0000000304 time: 0.18327856063842773\n",
      "Iteration: 10720 loss: 0.0000000301 time: 0.1929173469543457\n",
      "Iteration: 10730 loss: 0.0000000303 time: 0.1891651153564453\n",
      "Iteration: 10740 loss: 0.0000000422 time: 0.18791985511779785\n",
      "Iteration: 10750 loss: 0.0000000293 time: 0.1802077293395996\n",
      "Iteration: 10760 loss: 0.0000000299 time: 0.18446135520935059\n",
      "Iteration: 10770 loss: 0.0000000291 time: 0.19480347633361816\n",
      "Iteration: 10780 loss: 0.0000000284 time: 0.1845395565032959\n",
      "Iteration: 10790 loss: 0.0000000281 time: 0.18884778022766113\n",
      "Iteration: 10800 loss: 0.0000000278 time: 0.18609404563903809\n",
      "Iteration: 10810 loss: 0.0000000275 time: 0.17491984367370605\n",
      "Iteration: 10820 loss: 0.0000000272 time: 0.18192839622497559\n",
      "Iteration: 10830 loss: 0.0000000269 time: 0.18877005577087402\n",
      "Iteration: 10840 loss: 0.0000000266 time: 0.18197202682495117\n",
      "Iteration: 10850 loss: 0.0000000264 time: 0.1856851577758789\n",
      "Iteration: 10860 loss: 0.0000000261 time: 0.18857264518737793\n",
      "Iteration: 10870 loss: 0.0000000258 time: 0.18454670906066895\n",
      "Iteration: 10880 loss: 0.0000000255 time: 0.178208589553833\n",
      "Iteration: 10890 loss: 0.0000000252 time: 0.18807697296142578\n",
      "Iteration: 10900 loss: 0.0000000251 time: 0.18489313125610352\n",
      "Iteration: 10910 loss: 0.0000000341 time: 0.17862391471862793\n",
      "Iteration: 10920 loss: 0.0000000272 time: 0.1775989532470703\n",
      "Iteration: 10930 loss: 0.0000000249 time: 0.18811774253845215\n",
      "Iteration: 10940 loss: 0.0000000239 time: 0.17904973030090332\n",
      "Iteration: 10950 loss: 0.0000000238 time: 0.17915678024291992\n",
      "Iteration: 10960 loss: 0.0000000234 time: 0.18663859367370605\n",
      "Iteration: 10970 loss: 0.0000000233 time: 0.18645381927490234\n",
      "Iteration: 10980 loss: 0.0000000256 time: 0.18331527709960938\n",
      "Iteration: 10990 loss: 0.0000000230 time: 0.1858682632446289\n",
      "Iteration: 11000 loss: 0.0000000224 time: 0.1845712661743164\n",
      "Iteration: 11010 loss: 0.0000000221 time: 0.19171619415283203\n",
      "Iteration: 11020 loss: 0.0000000219 time: 0.1883563995361328\n",
      "Iteration: 11030 loss: 0.0000000216 time: 0.18848896026611328\n",
      "Iteration: 11040 loss: 0.0000000214 time: 0.18753576278686523\n",
      "Iteration: 11050 loss: 0.0000000212 time: 0.18945741653442383\n",
      "Iteration: 11060 loss: 0.0000000209 time: 0.18596839904785156\n",
      "Iteration: 11070 loss: 0.0000000207 time: 0.18383145332336426\n",
      "Iteration: 11080 loss: 0.0000000204 time: 0.17849016189575195\n",
      "Iteration: 11090 loss: 0.0000000203 time: 0.18253469467163086\n",
      "Iteration: 11100 loss: 0.0000000224 time: 0.1896684169769287\n",
      "Iteration: 11110 loss: 0.0000000210 time: 0.18730711936950684\n",
      "Iteration: 11120 loss: 0.0000000195 time: 0.18465185165405273\n",
      "Iteration: 11130 loss: 0.0000000196 time: 0.1796858310699463\n",
      "Iteration: 11140 loss: 0.0000000193 time: 0.1886899471282959\n",
      "Iteration: 11150 loss: 0.0000000189 time: 0.18281173706054688\n",
      "Iteration: 11160 loss: 0.0000000187 time: 0.18642520904541016\n",
      "Iteration: 11170 loss: 0.0000000189 time: 0.18830299377441406\n",
      "Iteration: 11180 loss: 0.0000000240 time: 0.1829833984375\n",
      "Iteration: 11190 loss: 0.0000000199 time: 0.18953990936279297\n",
      "Iteration: 11200 loss: 0.0000000183 time: 0.18587565422058105\n",
      "Iteration: 11210 loss: 0.0000000177 time: 0.18996572494506836\n",
      "Iteration: 11220 loss: 0.0000000174 time: 0.18594813346862793\n",
      "Iteration: 11230 loss: 0.0000000171 time: 0.17909002304077148\n",
      "Iteration: 11240 loss: 0.0000000169 time: 0.18526101112365723\n",
      "Iteration: 11250 loss: 0.0000000167 time: 0.19179654121398926\n",
      "Iteration: 11260 loss: 0.0000000167 time: 0.18418240547180176\n",
      "Iteration: 11270 loss: 0.0000000249 time: 0.17792201042175293\n",
      "Iteration: 11280 loss: 0.0000000199 time: 0.18258142471313477\n",
      "Iteration: 11290 loss: 0.0000000171 time: 0.187300443649292\n",
      "Iteration: 11300 loss: 0.0000000157 time: 0.18699407577514648\n",
      "Iteration: 11310 loss: 0.0000000157 time: 0.1774580478668213\n",
      "Iteration: 11320 loss: 0.0000000154 time: 0.18682527542114258\n",
      "Iteration: 11330 loss: 0.0000000151 time: 0.18638825416564941\n",
      "Iteration: 11340 loss: 0.0000000150 time: 0.18876028060913086\n",
      "Iteration: 11350 loss: 0.0000000148 time: 0.18733644485473633\n",
      "Iteration: 11360 loss: 0.0000000162 time: 0.18969178199768066\n",
      "Iteration: 11370 loss: 0.0000000161 time: 0.1869525909423828\n",
      "Iteration: 11380 loss: 0.0000000163 time: 0.18363571166992188\n",
      "Iteration: 11390 loss: 0.0000000142 time: 0.18695282936096191\n",
      "Iteration: 11400 loss: 0.0000000142 time: 0.18499302864074707\n",
      "Iteration: 11410 loss: 0.0000000137 time: 0.17702889442443848\n",
      "Iteration: 11420 loss: 0.0000000136 time: 0.17979669570922852\n",
      "Iteration: 11430 loss: 0.0000000134 time: 0.18554973602294922\n",
      "Iteration: 11440 loss: 0.0000000132 time: 0.1870412826538086\n",
      "Iteration: 11450 loss: 0.0000000130 time: 0.18303608894348145\n",
      "Iteration: 11460 loss: 0.0000000129 time: 0.189406156539917\n",
      "Iteration: 11470 loss: 0.0000000130 time: 0.18369412422180176\n",
      "Iteration: 11480 loss: 0.0000000247 time: 0.18149685859680176\n",
      "Iteration: 11490 loss: 0.0000000183 time: 0.19297575950622559\n",
      "Iteration: 11500 loss: 0.0000000123 time: 0.18222403526306152\n",
      "Iteration: 11510 loss: 0.0000000128 time: 0.18712925910949707\n",
      "Iteration: 11520 loss: 0.0000000119 time: 0.18606090545654297\n",
      "Iteration: 11530 loss: 0.0000000119 time: 0.1822657585144043\n",
      "Iteration: 11540 loss: 0.0000000117 time: 0.18114686012268066\n",
      "Iteration: 11550 loss: 0.0000000115 time: 0.18636131286621094\n",
      "Iteration: 11560 loss: 0.0000000114 time: 0.18645668029785156\n",
      "Iteration: 11570 loss: 0.0000000112 time: 0.18262195587158203\n",
      "Iteration: 11580 loss: 0.0000000111 time: 0.18792486190795898\n",
      "Iteration: 11590 loss: 0.0000000109 time: 0.18613338470458984\n",
      "Iteration: 11600 loss: 0.0000000108 time: 0.19179773330688477\n",
      "Iteration: 11610 loss: 0.0000000114 time: 0.17945241928100586\n",
      "Iteration: 11620 loss: 0.0000000212 time: 0.19391131401062012\n",
      "Iteration: 11630 loss: 0.0000000105 time: 0.18343687057495117\n",
      "Iteration: 11640 loss: 0.0000000115 time: 0.18018388748168945\n",
      "Iteration: 11650 loss: 0.0000000101 time: 0.18801093101501465\n",
      "Iteration: 11660 loss: 0.0000000101 time: 0.1820087432861328\n",
      "Iteration: 11670 loss: 0.0000000099 time: 0.17664361000061035\n",
      "Iteration: 11680 loss: 0.0000000097 time: 0.1830897331237793\n",
      "Iteration: 11690 loss: 0.0000000097 time: 0.1770951747894287\n",
      "Iteration: 11700 loss: 0.0000000136 time: 0.17642641067504883\n",
      "Iteration: 11710 loss: 0.0000000094 time: 0.18557405471801758\n",
      "Iteration: 11720 loss: 0.0000000097 time: 0.19603443145751953\n",
      "Iteration: 11730 loss: 0.0000000097 time: 0.18319344520568848\n",
      "Iteration: 11740 loss: 0.0000000092 time: 0.18706870079040527\n",
      "Iteration: 11750 loss: 0.0000000089 time: 0.1861429214477539\n",
      "Iteration: 11760 loss: 0.0000000088 time: 0.18964600563049316\n",
      "Iteration: 11770 loss: 0.0000000086 time: 0.18731045722961426\n",
      "Iteration: 11780 loss: 0.0000000085 time: 0.18587017059326172\n",
      "Iteration: 11790 loss: 0.0000000084 time: 0.18085455894470215\n",
      "Iteration: 11800 loss: 0.0000000083 time: 0.17973566055297852\n",
      "Iteration: 11810 loss: 0.0000000082 time: 0.18466663360595703\n",
      "Iteration: 11820 loss: 0.0000000081 time: 0.1763451099395752\n",
      "Iteration: 11830 loss: 0.0000000082 time: 0.18346023559570312\n",
      "Iteration: 11840 loss: 0.0000000171 time: 0.1888573169708252\n",
      "Iteration: 11850 loss: 0.0000000151 time: 0.18444323539733887\n",
      "Iteration: 11860 loss: 0.0000000083 time: 0.18966436386108398\n",
      "Iteration: 11870 loss: 0.0000000083 time: 0.1833972930908203\n",
      "Iteration: 11880 loss: 0.0000000076 time: 0.18567800521850586\n",
      "Iteration: 11890 loss: 0.0000000075 time: 0.18446969985961914\n",
      "Iteration: 11900 loss: 0.0000000073 time: 0.18227410316467285\n",
      "Iteration: 11910 loss: 0.0000000072 time: 0.1781003475189209\n",
      "Iteration: 11920 loss: 0.0000000073 time: 0.17854690551757812\n",
      "Iteration: 11930 loss: 0.0000000136 time: 0.1790790557861328\n",
      "Iteration: 11940 loss: 0.0000000081 time: 0.18298768997192383\n",
      "Iteration: 11950 loss: 0.0000000087 time: 0.18026471138000488\n",
      "Iteration: 11960 loss: 0.0000000071 time: 0.18203139305114746\n",
      "Iteration: 11970 loss: 0.0000000066 time: 0.18042755126953125\n",
      "Iteration: 11980 loss: 0.0000000065 time: 0.17695331573486328\n",
      "Iteration: 11990 loss: 0.0000000065 time: 0.18504953384399414\n",
      "Iteration: 12000 loss: 0.0000000064 time: 0.1838235855102539\n",
      "Iteration: 12010 loss: 0.0000000063 time: 0.17945265769958496\n",
      "Iteration: 12020 loss: 0.0000000062 time: 0.1823902130126953\n",
      "Iteration: 12030 loss: 0.0000000061 time: 0.1759016513824463\n",
      "Iteration: 12040 loss: 0.0000000060 time: 0.18222260475158691\n",
      "Iteration: 12050 loss: 0.0000000059 time: 0.19016623497009277\n",
      "Iteration: 12060 loss: 0.0000000059 time: 0.17856431007385254\n",
      "Iteration: 12070 loss: 0.0000000058 time: 0.18363738059997559\n",
      "Iteration: 12080 loss: 0.0000000064 time: 0.18727469444274902\n",
      "Iteration: 12090 loss: 0.0000000218 time: 0.19009780883789062\n",
      "Iteration: 12100 loss: 0.0000000141 time: 0.1902921199798584\n",
      "Iteration: 12110 loss: 0.0000000075 time: 0.18267273902893066\n",
      "Iteration: 12120 loss: 0.0000000058 time: 0.18413352966308594\n",
      "Iteration: 12130 loss: 0.0000000057 time: 0.1866319179534912\n",
      "Iteration: 12140 loss: 0.0000000053 time: 0.18560218811035156\n",
      "Iteration: 12150 loss: 0.0000000052 time: 0.190032958984375\n",
      "Iteration: 12160 loss: 0.0000000051 time: 0.18215489387512207\n",
      "Iteration: 12170 loss: 0.0000000051 time: 0.1862781047821045\n",
      "Iteration: 12180 loss: 0.0000000050 time: 0.18241000175476074\n",
      "Iteration: 12190 loss: 0.0000000049 time: 0.19105911254882812\n",
      "Iteration: 12200 loss: 0.0000000050 time: 0.18819499015808105\n",
      "Iteration: 12210 loss: 0.0000000089 time: 0.18510842323303223\n",
      "Iteration: 12220 loss: 0.0000000048 time: 0.1896648406982422\n",
      "Iteration: 12230 loss: 0.0000000049 time: 0.18634581565856934\n",
      "Iteration: 12240 loss: 0.0000000050 time: 0.1855485439300537\n",
      "Iteration: 12250 loss: 0.0000000048 time: 0.18329381942749023\n",
      "Iteration: 12260 loss: 0.0000000046 time: 0.18902134895324707\n",
      "Iteration: 12270 loss: 0.0000000045 time: 0.1822645664215088\n",
      "Iteration: 12280 loss: 0.0000000044 time: 0.17960286140441895\n",
      "Iteration: 12290 loss: 0.0000000043 time: 0.1864001750946045\n",
      "Iteration: 12300 loss: 0.0000000043 time: 0.1845235824584961\n",
      "Iteration: 12310 loss: 0.0000000044 time: 0.1887216567993164\n",
      "Iteration: 12320 loss: 0.0000000122 time: 0.1830441951751709\n",
      "Iteration: 12330 loss: 0.0000000067 time: 0.18693876266479492\n",
      "Iteration: 12340 loss: 0.0000000064 time: 0.18269658088684082\n",
      "Iteration: 12350 loss: 0.0000000040 time: 0.1846141815185547\n",
      "Iteration: 12360 loss: 0.0000000041 time: 0.17913269996643066\n",
      "Iteration: 12370 loss: 0.0000000040 time: 0.18072795867919922\n",
      "Iteration: 12380 loss: 0.0000000039 time: 0.18636727333068848\n",
      "Iteration: 12390 loss: 0.0000000045 time: 0.1906125545501709\n",
      "Iteration: 12400 loss: 0.0000000149 time: 0.18282222747802734\n",
      "Iteration: 12410 loss: 0.0000000066 time: 0.1871039867401123\n",
      "Iteration: 12420 loss: 0.0000000037 time: 0.19004344940185547\n",
      "Iteration: 12430 loss: 0.0000000038 time: 0.1985914707183838\n",
      "Iteration: 12440 loss: 0.0000000037 time: 0.19525790214538574\n",
      "Iteration: 12450 loss: 0.0000000035 time: 0.19228267669677734\n",
      "Iteration: 12460 loss: 0.0000000035 time: 0.1778719425201416\n",
      "Iteration: 12470 loss: 0.0000000034 time: 0.17685532569885254\n",
      "Iteration: 12480 loss: 0.0000000034 time: 0.18706274032592773\n",
      "Iteration: 12490 loss: 0.0000000033 time: 0.17412209510803223\n",
      "Iteration: 12500 loss: 0.0000000033 time: 0.17533183097839355\n",
      "Iteration: 12510 loss: 0.0000000032 time: 0.18046903610229492\n",
      "Iteration: 12520 loss: 0.0000000038 time: 0.18693304061889648\n",
      "Iteration: 12530 loss: 0.0000000219 time: 0.18372321128845215\n",
      "Iteration: 12540 loss: 0.0000000059 time: 0.18360233306884766\n",
      "Iteration: 12550 loss: 0.0000000043 time: 0.17529058456420898\n",
      "Iteration: 12560 loss: 0.0000000033 time: 0.1911618709564209\n",
      "Iteration: 12570 loss: 0.0000000032 time: 0.1841118335723877\n",
      "Iteration: 12580 loss: 0.0000000030 time: 0.18802666664123535\n",
      "Iteration: 12590 loss: 0.0000000031 time: 0.19221091270446777\n",
      "Iteration: 12600 loss: 0.0000000060 time: 0.1905055046081543\n",
      "Iteration: 12610 loss: 0.0000000039 time: 0.18577790260314941\n",
      "Iteration: 12620 loss: 0.0000000028 time: 0.1863696575164795\n",
      "Iteration: 12630 loss: 0.0000000029 time: 0.17881226539611816\n",
      "Iteration: 12640 loss: 0.0000000028 time: 0.19154620170593262\n",
      "Iteration: 12650 loss: 0.0000000028 time: 0.17753148078918457\n",
      "Iteration: 12660 loss: 0.0000000027 time: 0.1845989227294922\n",
      "Iteration: 12670 loss: 0.0000000026 time: 0.1858208179473877\n",
      "Iteration: 12680 loss: 0.0000000026 time: 0.18501067161560059\n",
      "Iteration: 12690 loss: 0.0000000026 time: 0.18883538246154785\n",
      "Iteration: 12700 loss: 0.0000000026 time: 0.19104886054992676\n",
      "Iteration: 12710 loss: 0.0000000025 time: 0.17463159561157227\n",
      "Iteration: 12720 loss: 0.0000000025 time: 0.1895902156829834\n",
      "Iteration: 12730 loss: 0.0000000042 time: 0.1890556812286377\n",
      "Iteration: 12740 loss: 0.0000000102 time: 0.1823892593383789\n",
      "Iteration: 12750 loss: 0.0000000041 time: 0.1874396800994873\n",
      "Iteration: 12760 loss: 0.0000000031 time: 0.1834549903869629\n",
      "Iteration: 12770 loss: 0.0000000029 time: 0.18891072273254395\n",
      "Iteration: 12780 loss: 0.0000000026 time: 0.18874597549438477\n",
      "Iteration: 12790 loss: 0.0000000032 time: 0.18967866897583008\n",
      "Iteration: 12800 loss: 0.0000000053 time: 0.18921494483947754\n",
      "Iteration: 12810 loss: 0.0000000024 time: 0.18448638916015625\n",
      "Iteration: 12820 loss: 0.0000000029 time: 0.18590712547302246\n",
      "Iteration: 12830 loss: 0.0000000022 time: 0.1859910488128662\n",
      "Iteration: 12840 loss: 0.0000000023 time: 0.18655896186828613\n",
      "Iteration: 12850 loss: 0.0000000025 time: 0.17791128158569336\n",
      "Iteration: 12860 loss: 0.0000000044 time: 0.18213605880737305\n",
      "Iteration: 12870 loss: 0.0000000046 time: 0.17846107482910156\n",
      "Iteration: 12880 loss: 0.0000000032 time: 0.18346548080444336\n",
      "Iteration: 12890 loss: 0.0000000021 time: 0.18473529815673828\n",
      "Iteration: 12900 loss: 0.0000000022 time: 0.18563055992126465\n",
      "Iteration: 12910 loss: 0.0000000020 time: 0.187164306640625\n",
      "Iteration: 12920 loss: 0.0000000021 time: 0.18579816818237305\n",
      "Iteration: 12930 loss: 0.0000000023 time: 0.18033242225646973\n",
      "Iteration: 12940 loss: 0.0000000052 time: 0.18569016456604004\n",
      "Iteration: 12950 loss: 0.0000000046 time: 0.1787104606628418\n",
      "Iteration: 12960 loss: 0.0000000125 time: 0.18198776245117188\n",
      "Iteration: 12970 loss: 0.0000000037 time: 0.1779952049255371\n",
      "Iteration: 12980 loss: 0.0000000021 time: 0.18130970001220703\n",
      "Iteration: 12990 loss: 0.0000000022 time: 0.1742088794708252\n",
      "Iteration: 13000 loss: 0.0000000020 time: 0.18709921836853027\n",
      "Iteration: 13010 loss: 0.0000000018 time: 0.17942118644714355\n",
      "Iteration: 13020 loss: 0.0000000018 time: 0.18485069274902344\n",
      "Iteration: 13030 loss: 0.0000000017 time: 0.1873643398284912\n",
      "Iteration: 13040 loss: 0.0000000017 time: 0.17945265769958496\n",
      "Iteration: 13050 loss: 0.0000000017 time: 0.18473577499389648\n",
      "Iteration: 13060 loss: 0.0000000017 time: 0.19095897674560547\n",
      "Iteration: 13070 loss: 0.0000000018 time: 0.18532228469848633\n",
      "Iteration: 13080 loss: 0.0000000054 time: 0.19327998161315918\n",
      "Iteration: 13090 loss: 0.0000000027 time: 0.18416714668273926\n",
      "Iteration: 13100 loss: 0.0000000028 time: 0.18127822875976562\n",
      "Iteration: 13110 loss: 0.0000000027 time: 0.19806861877441406\n",
      "Iteration: 13120 loss: 0.0000000016 time: 0.18973112106323242\n",
      "Iteration: 13130 loss: 0.0000000016 time: 0.1863396167755127\n",
      "Iteration: 13140 loss: 0.0000000016 time: 0.19340157508850098\n",
      "Iteration: 13150 loss: 0.0000000015 time: 0.19663071632385254\n",
      "Iteration: 13160 loss: 0.0000000016 time: 0.18355798721313477\n",
      "Iteration: 13170 loss: 0.0000000043 time: 0.18015551567077637\n",
      "Iteration: 13180 loss: 0.0000000070 time: 0.1756610870361328\n",
      "Iteration: 13190 loss: 0.0000000023 time: 0.18135595321655273\n",
      "Iteration: 13200 loss: 0.0000000029 time: 0.18021273612976074\n",
      "Iteration: 13210 loss: 0.0000000015 time: 0.1747729778289795\n",
      "Iteration: 13220 loss: 0.0000000016 time: 0.19182538986206055\n",
      "Iteration: 13230 loss: 0.0000000014 time: 0.18463826179504395\n",
      "Iteration: 13240 loss: 0.0000000014 time: 0.18620872497558594\n",
      "Iteration: 13250 loss: 0.0000000014 time: 0.18350601196289062\n",
      "Iteration: 13260 loss: 0.0000000014 time: 0.18995070457458496\n",
      "Iteration: 13270 loss: 0.0000000014 time: 0.192138671875\n",
      "Iteration: 13280 loss: 0.0000000013 time: 0.1901397705078125\n",
      "Iteration: 13290 loss: 0.0000000013 time: 0.18714237213134766\n",
      "Iteration: 13300 loss: 0.0000000013 time: 0.19057822227478027\n",
      "Iteration: 13310 loss: 0.0000000013 time: 0.18099761009216309\n",
      "Iteration: 13320 loss: 0.0000000013 time: 0.1881699562072754\n",
      "Iteration: 13330 loss: 0.0000000018 time: 0.1934962272644043\n",
      "Iteration: 13340 loss: 0.0000000248 time: 0.1743624210357666\n",
      "Iteration: 13350 loss: 0.0000000018 time: 0.19006133079528809\n",
      "Iteration: 13360 loss: 0.0000000037 time: 0.18623971939086914\n",
      "Iteration: 13370 loss: 0.0000000018 time: 0.18811631202697754\n",
      "Iteration: 13380 loss: 0.0000000012 time: 0.1875629425048828\n",
      "Iteration: 13390 loss: 0.0000000013 time: 0.1836225986480713\n",
      "Iteration: 13400 loss: 0.0000000012 time: 0.18785357475280762\n",
      "Iteration: 13410 loss: 0.0000000012 time: 0.1874232292175293\n",
      "Iteration: 13420 loss: 0.0000000014 time: 0.19050288200378418\n",
      "Iteration: 13430 loss: 0.0000000088 time: 0.17986607551574707\n",
      "Iteration: 13440 loss: 0.0000000021 time: 0.18568801879882812\n",
      "Iteration: 13450 loss: 0.0000000033 time: 0.18976044654846191\n",
      "Iteration: 13460 loss: 0.0000000018 time: 0.18559026718139648\n",
      "Iteration: 13470 loss: 0.0000000012 time: 0.18236327171325684\n",
      "Iteration: 13480 loss: 0.0000000011 time: 0.18619394302368164\n",
      "Iteration: 13490 loss: 0.0000000011 time: 0.18462276458740234\n",
      "Iteration: 13500 loss: 0.0000000011 time: 0.18819856643676758\n",
      "Iteration: 13510 loss: 0.0000000011 time: 0.18236517906188965\n",
      "Iteration: 13520 loss: 0.0000000011 time: 0.19165539741516113\n",
      "Iteration: 13530 loss: 0.0000000011 time: 0.18961548805236816\n",
      "Iteration: 13540 loss: 0.0000000011 time: 0.1810154914855957\n",
      "Iteration: 13550 loss: 0.0000000010 time: 0.17877864837646484\n",
      "Iteration: 13560 loss: 0.0000000010 time: 0.1814098358154297\n",
      "Iteration: 13570 loss: 0.0000000010 time: 0.18177390098571777\n",
      "Iteration: 13580 loss: 0.0000000016 time: 0.1763145923614502\n",
      "Iteration: 13590 loss: 0.0000000217 time: 0.18199825286865234\n",
      "Iteration: 13600 loss: 0.0000000075 time: 0.16755938529968262\n",
      "Iteration: 13610 loss: 0.0000000014 time: 0.17784404754638672\n",
      "Iteration: 13620 loss: 0.0000000018 time: 0.18241262435913086\n",
      "Iteration: 13630 loss: 0.0000000010 time: 0.17705678939819336\n",
      "Iteration: 13640 loss: 0.0000000011 time: 0.18265700340270996\n",
      "Iteration: 13650 loss: 0.0000000010 time: 0.17970633506774902\n",
      "Iteration: 13660 loss: 0.0000000010 time: 0.17807936668395996\n",
      "Iteration: 13670 loss: 0.0000000010 time: 0.1846158504486084\n",
      "Iteration: 13680 loss: 0.0000000015 time: 0.19053006172180176\n",
      "Iteration: 13690 loss: 0.0000000146 time: 0.17892885208129883\n",
      "Iteration: 13700 loss: 0.0000000061 time: 0.187835693359375\n",
      "Iteration: 13710 loss: 0.0000000014 time: 0.17692804336547852\n",
      "Iteration: 13720 loss: 0.0000000012 time: 0.19225716590881348\n",
      "Iteration: 13730 loss: 0.0000000011 time: 0.1880817413330078\n",
      "Iteration: 13740 loss: 0.0000000009 time: 0.18007111549377441\n",
      "Iteration: 13750 loss: 0.0000000009 time: 0.18358182907104492\n",
      "Iteration: 13760 loss: 0.0000000009 time: 0.1805250644683838\n",
      "Iteration: 13770 loss: 0.0000000009 time: 0.18073749542236328\n",
      "Iteration: 13780 loss: 0.0000000009 time: 0.18619513511657715\n",
      "Iteration: 13790 loss: 0.0000000009 time: 0.1859607696533203\n",
      "Iteration: 13800 loss: 0.0000000008 time: 0.18707275390625\n",
      "Iteration: 13810 loss: 0.0000000008 time: 0.18535423278808594\n",
      "Iteration: 13820 loss: 0.0000000009 time: 0.1843099594116211\n",
      "Iteration: 13830 loss: 0.0000000077 time: 0.1811079978942871\n",
      "Iteration: 13840 loss: 0.0000000027 time: 0.18592548370361328\n",
      "Iteration: 13850 loss: 0.0000000041 time: 0.18322229385375977\n",
      "Iteration: 13860 loss: 0.0000000011 time: 0.1820998191833496\n",
      "Iteration: 13870 loss: 0.0000000012 time: 0.18665838241577148\n",
      "Iteration: 13880 loss: 0.0000000008 time: 0.18749713897705078\n",
      "Iteration: 13890 loss: 0.0000000009 time: 0.18128490447998047\n",
      "Iteration: 13900 loss: 0.0000000015 time: 0.1820211410522461\n",
      "Iteration: 13910 loss: 0.0000000118 time: 0.17802190780639648\n",
      "Iteration: 13920 loss: 0.0000000046 time: 0.18358445167541504\n",
      "Iteration: 13930 loss: 0.0000000018 time: 0.1810131072998047\n",
      "Iteration: 13940 loss: 0.0000000008 time: 0.17534875869750977\n",
      "Iteration: 13950 loss: 0.0000000008 time: 0.18596506118774414\n",
      "Iteration: 13960 loss: 0.0000000008 time: 0.18268847465515137\n",
      "Iteration: 13970 loss: 0.0000000007 time: 0.1864302158355713\n",
      "Iteration: 13980 loss: 0.0000000007 time: 0.17915797233581543\n",
      "Iteration: 13990 loss: 0.0000000007 time: 0.18587636947631836\n",
      "Iteration: 14000 loss: 0.0000000007 time: 0.18112969398498535\n",
      "Iteration: 14010 loss: 0.0000000007 time: 0.1809530258178711\n",
      "Iteration: 14020 loss: 0.0000000007 time: 0.18120956420898438\n",
      "Iteration: 14030 loss: 0.0000000007 time: 0.1820387840270996\n",
      "Iteration: 14040 loss: 0.0000000008 time: 0.18176555633544922\n",
      "Iteration: 14050 loss: 0.0000000039 time: 0.18989324569702148\n",
      "Iteration: 14060 loss: 0.0000000037 time: 0.18427300453186035\n",
      "Iteration: 14070 loss: 0.0000000035 time: 0.18224525451660156\n",
      "Iteration: 14080 loss: 0.0000000014 time: 0.1850283145904541\n",
      "Iteration: 14090 loss: 0.0000000011 time: 0.19089889526367188\n",
      "Iteration: 14100 loss: 0.0000000009 time: 0.18202757835388184\n",
      "Iteration: 14110 loss: 0.0000000010 time: 0.1901383399963379\n",
      "Iteration: 14120 loss: 0.0000000035 time: 0.17732548713684082\n",
      "Iteration: 14130 loss: 0.0000000031 time: 0.18301987648010254\n",
      "Iteration: 14140 loss: 0.0000000021 time: 0.17815732955932617\n",
      "Iteration: 14150 loss: 0.0000000012 time: 0.17048287391662598\n",
      "Iteration: 14160 loss: 0.0000000007 time: 0.1840510368347168\n",
      "Iteration: 14170 loss: 0.0000000007 time: 0.17829442024230957\n",
      "Iteration: 14180 loss: 0.0000000007 time: 0.18227219581604004\n",
      "Iteration: 14190 loss: 0.0000000007 time: 0.18362832069396973\n",
      "Iteration: 14200 loss: 0.0000000008 time: 0.18304157257080078\n",
      "Iteration: 14210 loss: 0.0000000026 time: 0.18800926208496094\n",
      "Iteration: 14220 loss: 0.0000000063 time: 0.18474841117858887\n",
      "Iteration: 14230 loss: 0.0000000028 time: 0.17894482612609863\n",
      "Iteration: 14240 loss: 0.0000000014 time: 0.1858687400817871\n",
      "Iteration: 14250 loss: 0.0000000009 time: 0.185194730758667\n",
      "Iteration: 14260 loss: 0.0000000006 time: 0.17757320404052734\n",
      "Iteration: 14270 loss: 0.0000000006 time: 0.1849966049194336\n",
      "Iteration: 14280 loss: 0.0000000006 time: 0.19252920150756836\n",
      "Iteration: 14290 loss: 0.0000000007 time: 0.18222260475158691\n",
      "Iteration: 14300 loss: 0.0000000070 time: 0.1842966079711914\n",
      "Iteration: 14310 loss: 0.0000000031 time: 0.1743638515472412\n",
      "Iteration: 14320 loss: 0.0000000034 time: 0.18686723709106445\n",
      "Iteration: 14330 loss: 0.0000000012 time: 0.17983412742614746\n",
      "Iteration: 14340 loss: 0.0000000007 time: 0.18795108795166016\n",
      "Iteration: 14350 loss: 0.0000000007 time: 0.1939389705657959\n",
      "Iteration: 14360 loss: 0.0000000006 time: 0.18119096755981445\n",
      "Iteration: 14370 loss: 0.0000000009 time: 0.18656039237976074\n",
      "Iteration: 14380 loss: 0.0000000073 time: 0.18750691413879395\n",
      "Iteration: 14390 loss: 0.0000000008 time: 0.18432068824768066\n",
      "Iteration: 14400 loss: 0.0000000010 time: 0.18429780006408691\n",
      "Iteration: 14410 loss: 0.0000000009 time: 0.18763399124145508\n",
      "Iteration: 14420 loss: 0.0000000007 time: 0.18870258331298828\n",
      "Iteration: 14430 loss: 0.0000000006 time: 0.18512916564941406\n",
      "Iteration: 14440 loss: 0.0000000005 time: 0.1882767677307129\n",
      "Iteration: 14450 loss: 0.0000000005 time: 0.18564081192016602\n",
      "Iteration: 14460 loss: 0.0000000005 time: 0.19013690948486328\n",
      "Iteration: 14470 loss: 0.0000000005 time: 0.18540453910827637\n",
      "Iteration: 14480 loss: 0.0000000005 time: 0.17955398559570312\n",
      "Iteration: 14490 loss: 0.0000000005 time: 0.175856351852417\n",
      "Iteration: 14500 loss: 0.0000000005 time: 0.18469929695129395\n",
      "Iteration: 14510 loss: 0.0000000024 time: 0.18121004104614258\n",
      "Iteration: 14520 loss: 0.0000000109 time: 0.19244813919067383\n",
      "Iteration: 14530 loss: 0.0000000007 time: 0.1836559772491455\n",
      "Iteration: 14540 loss: 0.0000000022 time: 0.1789388656616211\n",
      "Iteration: 14550 loss: 0.0000000005 time: 0.17957615852355957\n",
      "Iteration: 14560 loss: 0.0000000007 time: 0.17952179908752441\n",
      "Iteration: 14570 loss: 0.0000000005 time: 0.1874549388885498\n",
      "Iteration: 14580 loss: 0.0000000012 time: 0.18186688423156738\n",
      "Iteration: 14590 loss: 0.0000000123 time: 0.18053245544433594\n",
      "Iteration: 14600 loss: 0.0000000046 time: 0.180436372756958\n",
      "Iteration: 14610 loss: 0.0000000013 time: 0.192002534866333\n",
      "Iteration: 14620 loss: 0.0000000005 time: 0.19041895866394043\n",
      "Iteration: 14630 loss: 0.0000000005 time: 0.1899559497833252\n",
      "Iteration: 14640 loss: 0.0000000005 time: 0.18260836601257324\n",
      "Iteration: 14650 loss: 0.0000000005 time: 0.18157696723937988\n",
      "Iteration: 14660 loss: 0.0000000005 time: 0.18236994743347168\n",
      "Iteration: 14670 loss: 0.0000000005 time: 0.18078327178955078\n",
      "Iteration: 14680 loss: 0.0000000005 time: 0.1781902313232422\n",
      "Iteration: 14690 loss: 0.0000000005 time: 0.17983770370483398\n",
      "Iteration: 14700 loss: 0.0000000004 time: 0.1828773021697998\n",
      "Iteration: 14710 loss: 0.0000000004 time: 0.17968130111694336\n",
      "Iteration: 14720 loss: 0.0000000004 time: 0.18786907196044922\n",
      "Iteration: 14730 loss: 0.0000000005 time: 0.1886584758758545\n",
      "Iteration: 14740 loss: 0.0000000081 time: 0.17920684814453125\n",
      "Iteration: 14750 loss: 0.0000000049 time: 0.17710638046264648\n",
      "Iteration: 14760 loss: 0.0000000027 time: 0.18044281005859375\n",
      "Iteration: 14770 loss: 0.0000000021 time: 0.17766499519348145\n",
      "Iteration: 14780 loss: 0.0000000006 time: 0.17949390411376953\n",
      "Iteration: 14790 loss: 0.0000000005 time: 0.19070911407470703\n",
      "Iteration: 14800 loss: 0.0000000005 time: 0.18804597854614258\n",
      "Iteration: 14810 loss: 0.0000000004 time: 0.18520021438598633\n",
      "Iteration: 14820 loss: 0.0000000005 time: 0.18477535247802734\n",
      "Iteration: 14830 loss: 0.0000000024 time: 0.1816999912261963\n",
      "Iteration: 14840 loss: 0.0000000083 time: 0.1969757080078125\n",
      "Iteration: 14850 loss: 0.0000000017 time: 0.18419194221496582\n",
      "Iteration: 14860 loss: 0.0000000004 time: 0.17858481407165527\n",
      "Iteration: 14870 loss: 0.0000000005 time: 0.18084049224853516\n",
      "Iteration: 14880 loss: 0.0000000005 time: 0.18268561363220215\n",
      "Iteration: 14890 loss: 0.0000000004 time: 0.19132590293884277\n",
      "Iteration: 14900 loss: 0.0000000004 time: 0.18622398376464844\n",
      "Iteration: 14910 loss: 0.0000000004 time: 0.1899874210357666\n",
      "Iteration: 14920 loss: 0.0000000004 time: 0.1871945858001709\n",
      "Iteration: 14930 loss: 0.0000000004 time: 0.18159747123718262\n",
      "Iteration: 14940 loss: 0.0000000004 time: 0.19093680381774902\n",
      "Iteration: 14950 loss: 0.0000000004 time: 0.17833471298217773\n",
      "Iteration: 14960 loss: 0.0000000004 time: 0.19017243385314941\n",
      "Iteration: 14970 loss: 0.0000000005 time: 0.18491458892822266\n",
      "Iteration: 14980 loss: 0.0000000065 time: 0.1789991855621338\n",
      "Iteration: 14990 loss: 0.0000000009 time: 0.17853331565856934\n",
      "Iteration: 15000 loss: 0.0000000039 time: 0.18708395957946777\n",
      "-->mesh : \n",
      "     n_triangles :  256\n",
      "     n_vertices  :  145\n",
      "     n_edges     :  400\n",
      "     h_max           :  0.12500000000033273\n",
      "     h_min           :  0.0883883476480272\n",
      "-->test_fun      : \n",
      "     order       :  1\n",
      "     dof         :  113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 11:51:15.439574: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:51:15.443265: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:51:15.446075: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:51:15.449115: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:51:15.450948: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:51:15.452702: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:51:15.454898: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:51:15.457148: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:51:15.459172: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:51:15.461400: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides}}]]\n",
      "2023-12-28 11:51:19.529804: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_26' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_26}}]]\n",
      "2023-12-28 11:51:19.532251: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_41' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_41}}]]\n",
      "2023-12-28 11:51:19.532421: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_58' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_58}}]]\n",
      "2023-12-28 11:51:19.532555: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_74' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_74}}]]\n",
      "2023-12-28 11:51:19.532697: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_90' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_90}}]]\n",
      "2023-12-28 11:51:19.532825: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_106' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_106}}]]\n",
      "2023-12-28 11:51:19.532949: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_122' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_122}}]]\n",
      "2023-12-28 11:51:19.533049: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_138' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_138}}]]\n",
      "2023-12-28 11:51:19.533167: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_154' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_154}}]]\n",
      "2023-12-28 11:51:19.533286: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_170' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_170}}]]\n",
      "2023-12-28 11:51:19.533367: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_186' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_186}}]]\n",
      "2023-12-28 11:51:19.533537: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_202' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_202}}]]\n",
      "2023-12-28 11:51:19.533655: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_218' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_218}}]]\n",
      "2023-12-28 11:51:19.533774: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_234' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_234}}]]\n",
      "2023-12-28 11:51:19.533893: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_250' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_250}}]]\n",
      "2023-12-28 11:51:19.534011: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_266' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_266}}]]\n",
      "2023-12-28 11:51:19.534121: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_282' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_282}}]]\n",
      "2023-12-28 11:51:19.534229: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_298' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_298}}]]\n",
      "2023-12-28 11:51:19.534348: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_314' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_314}}]]\n",
      "2023-12-28 11:51:19.534479: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_330' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_330}}]]\n",
      "2023-12-28 11:51:19.534635: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_350' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_350}}]]\n",
      "2023-12-28 11:51:19.534820: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_379' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_379}}]]\n",
      "2023-12-28 11:51:19.534930: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_397' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_397}}]]\n",
      "2023-12-28 11:51:19.535000: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_415' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_415}}]]\n",
      "2023-12-28 11:51:19.535108: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_420' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_420}}]]\n",
      "2023-12-28 11:51:19.535260: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_436' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_436}}]]\n",
      "2023-12-28 11:51:19.535331: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_454' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_454}}]]\n",
      "2023-12-28 11:51:19.535512: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_482' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_482}}]]\n",
      "2023-12-28 11:51:19.535699: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_490' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_490}}]]\n",
      "2023-12-28 11:51:19.535812: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_519' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_519}}]]\n",
      "2023-12-28 11:51:19.536008: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_537' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_537}}]]\n",
      "2023-12-28 11:51:19.536175: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_555' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_555}}]]\n",
      "2023-12-28 11:51:19.536341: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_560' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_560}}]]\n",
      "2023-12-28 11:51:19.536447: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_576' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_576}}]]\n",
      "2023-12-28 11:51:19.536515: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_592' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_592}}]]\n",
      "2023-12-28 11:51:19.536687: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_608' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_608}}]]\n",
      "2023-12-28 11:51:19.536871: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_624' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_624}}]]\n",
      "2023-12-28 11:51:19.537036: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_640' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_640}}]]\n",
      "2023-12-28 11:51:19.537185: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_656' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_656}}]]\n",
      "2023-12-28 11:51:19.537384: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_672' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_672}}]]\n",
      "2023-12-28 11:51:19.537563: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_688' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_688}}]]\n",
      "2023-12-28 11:51:19.537632: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_704' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_704}}]]\n",
      "2023-12-28 11:51:19.537739: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_720' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_720}}]]\n",
      "2023-12-28 11:51:19.537884: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_736' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_736}}]]\n",
      "2023-12-28 11:51:19.538028: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_752' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_752}}]]\n",
      "2023-12-28 11:51:19.538201: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_768' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_768}}]]\n",
      "2023-12-28 11:51:19.538419: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_786' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_786}}]]\n",
      "2023-12-28 11:51:19.538625: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_814' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_814}}]]\n",
      "2023-12-28 11:51:19.538782: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_818' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_818}}]]\n",
      "2023-12-28 11:51:19.538900: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_834' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_834}}]]\n",
      "2023-12-28 11:51:19.539017: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_850' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_850}}]]\n",
      "2023-12-28 11:51:19.539133: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_866' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_866}}]]\n",
      "2023-12-28 11:51:19.539360: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_886' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_886}}]]\n",
      "2023-12-28 11:51:19.539497: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_915' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_915}}]]\n",
      "2023-12-28 11:51:19.539615: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_933' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_933}}]]\n",
      "2023-12-28 11:51:19.539731: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_951' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_951}}]]\n",
      "2023-12-28 11:51:19.539849: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_956' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_956}}]]\n",
      "2023-12-28 11:51:19.539966: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_972' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_972}}]]\n",
      "2023-12-28 11:51:19.540082: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_990' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_990}}]]\n",
      "2023-12-28 11:51:19.540198: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1018' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1018}}]]\n",
      "2023-12-28 11:51:19.540315: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1026' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1026}}]]\n",
      "2023-12-28 11:51:19.540420: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1055' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1055}}]]\n",
      "2023-12-28 11:51:19.540522: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1073' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1073}}]]\n",
      "2023-12-28 11:51:19.540626: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1091' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1091}}]]\n",
      "2023-12-28 11:51:19.540729: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1096' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1096}}]]\n",
      "2023-12-28 11:51:19.540833: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1112' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1112}}]]\n",
      "2023-12-28 11:51:19.540937: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1128' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1128}}]]\n",
      "2023-12-28 11:51:19.541040: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1144' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1144}}]]\n",
      "2023-12-28 11:51:19.541146: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1160' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1160}}]]\n",
      "2023-12-28 11:51:19.541249: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1176' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1176}}]]\n",
      "2023-12-28 11:51:19.541353: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1192' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1192}}]]\n",
      "2023-12-28 11:51:19.541457: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1208' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1208}}]]\n",
      "2023-12-28 11:51:19.541560: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1224' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1224}}]]\n",
      "2023-12-28 11:51:19.541663: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1240' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1240}}]]\n",
      "2023-12-28 11:51:19.541766: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1256' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1256}}]]\n",
      "2023-12-28 11:51:19.541870: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1272' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1272}}]]\n",
      "2023-12-28 11:51:19.541976: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1288' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1288}}]]\n",
      "2023-12-28 11:51:19.542096: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1304' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1304}}]]\n",
      "2023-12-28 11:51:19.542215: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1320' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1320}}]]\n",
      "2023-12-28 11:51:19.542333: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1336' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1336}}]]\n",
      "2023-12-28 11:51:19.542451: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1352' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1352}}]]\n",
      "2023-12-28 11:51:19.542570: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1368' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1368}}]]\n",
      "2023-12-28 11:51:19.542790: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1384' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1384}}]]\n",
      "2023-12-28 11:51:19.542969: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1400' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1400}}]]\n",
      "2023-12-28 11:51:19.543118: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1420' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1420}}]]\n",
      "2023-12-28 11:51:19.543242: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1449' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1449}}]]\n",
      "2023-12-28 11:51:19.543441: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1467' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1467}}]]\n",
      "2023-12-28 11:51:19.543583: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1485' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1485}}]]\n",
      "2023-12-28 11:51:19.543735: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1490' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1490}}]]\n",
      "2023-12-28 11:51:19.543872: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1506' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1506}}]]\n",
      "2023-12-28 11:51:19.544015: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1524' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1524}}]]\n",
      "2023-12-28 11:51:19.544201: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1552' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1552}}]]\n",
      "2023-12-28 11:51:19.544408: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1560' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1560}}]]\n",
      "2023-12-28 11:51:19.544663: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1589' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1589}}]]\n",
      "2023-12-28 11:51:19.544838: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1607' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1607}}]]\n",
      "2023-12-28 11:51:19.545007: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1625' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1625}}]]\n",
      "2023-12-28 11:51:19.545190: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1630' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1630}}]]\n",
      "2023-12-28 11:51:19.545348: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1646' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1646}}]]\n",
      "2023-12-28 11:51:19.545468: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1662' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1662}}]]\n",
      "2023-12-28 11:51:19.545585: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1678' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1678}}]]\n",
      "2023-12-28 11:51:19.545719: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1694' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1694}}]]\n",
      "2023-12-28 11:51:19.545837: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1710' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1710}}]]\n",
      "2023-12-28 11:51:19.545955: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1726' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1726}}]]\n",
      "2023-12-28 11:51:19.546075: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1742' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1742}}]]\n",
      "2023-12-28 11:51:19.546193: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1758' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1758}}]]\n",
      "2023-12-28 11:51:19.546331: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1774' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1774}}]]\n",
      "2023-12-28 11:51:19.546450: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1790' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1790}}]]\n",
      "2023-12-28 11:51:19.546567: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1806' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1806}}]]\n",
      "2023-12-28 11:51:19.546676: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1822' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1822}}]]\n",
      "2023-12-28 11:51:19.546786: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1838' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1838}}]]\n",
      "2023-12-28 11:51:19.546894: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1856' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1856}}]]\n",
      "2023-12-28 11:51:19.547002: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1884' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1884}}]]\n",
      "2023-12-28 11:51:19.547109: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1888' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1888}}]]\n",
      "2023-12-28 11:51:19.547215: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1904' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1904}}]]\n",
      "2023-12-28 11:51:19.547323: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1920' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1920}}]]\n",
      "2023-12-28 11:51:19.547431: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1936' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1936}}]]\n",
      "2023-12-28 11:51:19.547538: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1956' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1956}}]]\n",
      "2023-12-28 11:51:19.547647: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1985' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1985}}]]\n",
      "2023-12-28 11:51:19.547755: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2003' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2003}}]]\n",
      "2023-12-28 11:51:19.547863: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2021' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2021}}]]\n",
      "2023-12-28 11:51:19.547969: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2026' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2026}}]]\n",
      "2023-12-28 11:51:19.548077: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2042' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2042}}]]\n",
      "2023-12-28 11:51:19.548184: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2060' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2060}}]]\n",
      "2023-12-28 11:51:19.548287: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2088' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2088}}]]\n",
      "2023-12-28 11:51:19.548389: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2096' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2096}}]]\n",
      "2023-12-28 11:51:19.548491: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2125' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2125}}]]\n",
      "2023-12-28 11:51:19.548594: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2143' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2143}}]]\n",
      "2023-12-28 11:51:19.548697: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2161' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2161}}]]\n",
      "2023-12-28 11:51:19.548798: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2166' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2166}}]]\n",
      "2023-12-28 11:51:19.548902: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2182' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2182}}]]\n",
      "2023-12-28 11:51:19.549004: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2198' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2198}}]]\n",
      "2023-12-28 11:51:19.549105: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2214' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2214}}]]\n",
      "2023-12-28 11:51:19.549208: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2230' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2230}}]]\n",
      "2023-12-28 11:51:19.549310: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2246' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2246}}]]\n",
      "2023-12-28 11:51:19.549412: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2262' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2262}}]]\n",
      "2023-12-28 11:51:19.549514: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2278' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2278}}]]\n",
      "2023-12-28 11:51:19.549616: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2294' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2294}}]]\n",
      "2023-12-28 11:51:19.549719: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2310' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2310}}]]\n",
      "2023-12-28 11:51:19.549822: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2326' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2326}}]]\n",
      "2023-12-28 11:51:19.549924: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2342' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2342}}]]\n",
      "2023-12-28 11:51:19.550027: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2358' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2358}}]]\n",
      "2023-12-28 11:51:19.550130: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2374' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2374}}]]\n",
      "2023-12-28 11:51:19.550233: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2390' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2390}}]]\n",
      "2023-12-28 11:51:19.550336: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2406' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2406}}]]\n",
      "2023-12-28 11:51:19.550438: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2422' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2422}}]]\n",
      "2023-12-28 11:51:19.550541: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2438' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2438}}]]\n",
      "2023-12-28 11:51:19.550659: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2454' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2454}}]]\n",
      "2023-12-28 11:51:19.550761: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2470' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2470}}]]\n",
      "2023-12-28 11:51:19.550864: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2490' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2490}}]]\n",
      "2023-12-28 11:51:19.550965: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2519' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2519}}]]\n",
      "2023-12-28 11:51:19.551068: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2537' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2537}}]]\n",
      "2023-12-28 11:51:19.551172: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2555' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2555}}]]\n",
      "2023-12-28 11:51:19.551302: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2560' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2560}}]]\n",
      "2023-12-28 11:51:19.551425: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2576' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2576}}]]\n",
      "2023-12-28 11:51:19.551536: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2594' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2594}}]]\n",
      "2023-12-28 11:51:19.551670: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2622' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2622}}]]\n",
      "2023-12-28 11:51:19.551786: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2630' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2630}}]]\n",
      "2023-12-28 11:51:19.551914: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2659' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2659}}]]\n",
      "2023-12-28 11:51:19.552024: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2677' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2677}}]]\n",
      "2023-12-28 11:51:19.552134: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2695' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2695}}]]\n",
      "2023-12-28 11:51:19.552243: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2700' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2700}}]]\n",
      "2023-12-28 11:51:19.552353: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2716' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2716}}]]\n",
      "2023-12-28 11:51:19.552463: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2732' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2732}}]]\n",
      "2023-12-28 11:51:19.552573: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2748' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2748}}]]\n",
      "2023-12-28 11:51:19.552685: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2764' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2764}}]]\n",
      "2023-12-28 11:51:19.552796: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2780' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2780}}]]\n",
      "2023-12-28 11:51:19.552906: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2796' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2796}}]]\n",
      "2023-12-28 11:51:19.553017: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2812' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2812}}]]\n",
      "2023-12-28 11:51:19.553128: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2828' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2828}}]]\n",
      "2023-12-28 11:51:19.553235: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2844' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2844}}]]\n",
      "2023-12-28 11:51:19.553342: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2860' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2860}}]]\n",
      "2023-12-28 11:51:19.553449: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2876' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2876}}]]\n",
      "2023-12-28 11:51:19.553583: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2892' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2892}}]]\n",
      "2023-12-28 11:51:19.553692: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2908' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2908}}]]\n",
      "2023-12-28 11:51:19.553801: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2926' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2926}}]]\n",
      "2023-12-28 11:51:19.553909: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2954' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2954}}]]\n",
      "2023-12-28 11:51:19.554015: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2958' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2958}}]]\n",
      "2023-12-28 11:51:19.554123: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2974' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2974}}]]\n",
      "2023-12-28 11:51:19.554231: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2990' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2990}}]]\n",
      "2023-12-28 11:51:19.554340: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3006' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3006}}]]\n",
      "2023-12-28 11:51:19.554450: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3026' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3026}}]]\n",
      "2023-12-28 11:51:19.554560: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3055' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3055}}]]\n",
      "2023-12-28 11:51:19.554671: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3073' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3073}}]]\n",
      "2023-12-28 11:51:19.554790: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3091' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3091}}]]\n",
      "2023-12-28 11:51:19.554895: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3096' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3096}}]]\n",
      "2023-12-28 11:51:19.554998: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3112' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3112}}]]\n",
      "2023-12-28 11:51:19.555103: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3130' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3130}}]]\n",
      "2023-12-28 11:51:19.555207: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3158' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3158}}]]\n",
      "2023-12-28 11:51:19.555312: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3166' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3166}}]]\n",
      "2023-12-28 11:51:19.555416: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3195' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3195}}]]\n",
      "2023-12-28 11:51:19.555521: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3213' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3213}}]]\n",
      "2023-12-28 11:51:19.555625: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3231' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3231}}]]\n",
      "2023-12-28 11:51:19.555729: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3236' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3236}}]]\n",
      "2023-12-28 11:51:19.555834: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3252' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3252}}]]\n",
      "2023-12-28 11:51:19.555939: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3268' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3268}}]]\n",
      "2023-12-28 11:51:19.556044: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3284' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3284}}]]\n",
      "2023-12-28 11:51:19.556150: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3300' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3300}}]]\n",
      "2023-12-28 11:51:19.556255: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3316' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3316}}]]\n",
      "2023-12-28 11:51:19.556532: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3332' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3332}}]]\n",
      "2023-12-28 11:51:19.556666: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3348' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3348}}]]\n",
      "2023-12-28 11:51:19.556803: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3364' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3364}}]]\n",
      "2023-12-28 11:51:19.556954: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3380' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3380}}]]\n",
      "2023-12-28 11:51:19.557063: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3396' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3396}}]]\n",
      "2023-12-28 11:51:19.557172: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3412' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3412}}]]\n",
      "2023-12-28 11:51:19.557247: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3428' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3428}}]]\n",
      "2023-12-28 11:51:19.557356: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3444' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3444}}]]\n",
      "2023-12-28 11:51:19.557428: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3460' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3460}}]]\n",
      "2023-12-28 11:51:19.557560: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3476' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3476}}]]\n",
      "2023-12-28 11:51:19.557750: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3492' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3492}}]]\n",
      "2023-12-28 11:51:19.557876: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3508' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3508}}]]\n",
      "2023-12-28 11:51:19.558006: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3524' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3524}}]]\n",
      "2023-12-28 11:51:19.558076: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3540' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3540}}]]\n",
      "2023-12-28 11:51:19.558183: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3560' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3560}}]]\n",
      "2023-12-28 11:51:19.558289: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3589' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3589}}]]\n",
      "2023-12-28 11:51:19.558352: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3607' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3607}}]]\n",
      "2023-12-28 11:51:19.558451: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3625' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3625}}]]\n",
      "2023-12-28 11:51:19.558552: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3630' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3630}}]]\n",
      "2023-12-28 11:51:19.558617: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3646' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3646}}]]\n",
      "2023-12-28 11:51:19.558715: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3664' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3664}}]]\n",
      "2023-12-28 11:51:19.558779: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3692' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3692}}]]\n",
      "2023-12-28 11:51:19.558878: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3700' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3700}}]]\n",
      "2023-12-28 11:51:19.558978: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3729' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3729}}]]\n",
      "2023-12-28 11:51:19.559043: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3747' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3747}}]]\n",
      "2023-12-28 11:51:19.559142: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3765' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3765}}]]\n",
      "2023-12-28 11:51:19.559242: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3770' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3770}}]]\n",
      "2023-12-28 11:51:19.559305: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3786' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3786}}]]\n",
      "2023-12-28 11:51:19.559404: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3802' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3802}}]]\n",
      "2023-12-28 11:51:19.559504: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3818' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3818}}]]\n",
      "2023-12-28 11:51:19.559568: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3834' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3834}}]]\n",
      "2023-12-28 11:51:19.559669: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3850' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3850}}]]\n",
      "2023-12-28 11:51:19.559775: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3866' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3866}}]]\n",
      "2023-12-28 11:51:19.559880: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3882' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3882}}]]\n",
      "2023-12-28 11:51:19.559986: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3898' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3898}}]]\n",
      "2023-12-28 11:51:19.560157: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3914' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3914}}]]\n",
      "2023-12-28 11:51:19.560347: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3930' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3930}}]]\n",
      "2023-12-28 11:51:19.560506: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3946' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3946}}]]\n",
      "2023-12-28 11:51:19.560687: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3962' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3962}}]]\n",
      "2023-12-28 11:51:19.560962: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3978' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3978}}]]\n",
      "2023-12-28 11:51:19.561172: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3996' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3996}}]]\n",
      "2023-12-28 11:51:19.561407: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4024' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4024}}]]\n",
      "2023-12-28 11:51:19.561587: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4028' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4028}}]]\n",
      "2023-12-28 11:51:19.561802: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4044' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4044}}]]\n",
      "2023-12-28 11:51:19.561982: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4060' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4060}}]]\n",
      "2023-12-28 11:51:19.562216: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4076' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4076}}]]\n",
      "2023-12-28 11:51:19.562395: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4096' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4096}}]]\n",
      "2023-12-28 11:51:19.562609: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4125' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4125}}]]\n",
      "2023-12-28 11:51:19.562785: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4143' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4143}}]]\n",
      "2023-12-28 11:51:19.562992: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4161' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4161}}]]\n",
      "2023-12-28 11:51:19.563131: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4166' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4166}}]]\n",
      "2023-12-28 11:51:19.563273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4182' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4182}}]]\n",
      "2023-12-28 11:51:19.563507: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4200' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4200}}]]\n",
      "2023-12-28 11:51:19.563692: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4228' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4228}}]]\n",
      "2023-12-28 11:51:19.563890: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4236' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4236}}]]\n",
      "2023-12-28 11:51:19.564072: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4264' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4264}}]]\n",
      "2023-12-28 11:51:19.564261: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4275' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4275}}]]\n",
      "2023-12-28 11:51:19.564462: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4282' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4282}}]]\n",
      "2023-12-28 11:51:19.564602: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4287' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4287}}]]\n",
      "2023-12-28 11:51:19.564735: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4290' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4290}}]]\n",
      "2023-12-28 11:51:19.564888: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4293' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4293}}]]\n",
      "2023-12-28 11:51:19.565000: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4296' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4296}}]]\n",
      "2023-12-28 11:51:19.565153: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4299' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4299}}]]\n",
      "2023-12-28 11:51:19.565265: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4302' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4302}}]]\n",
      "2023-12-28 11:51:19.565460: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4305' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4305}}]]\n",
      "2023-12-28 11:51:19.565626: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4308' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4308}}]]\n",
      "2023-12-28 11:51:19.565862: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4311' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4311}}]]\n",
      "2023-12-28 11:51:19.566077: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4314' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4314}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 loss: 0.0005437941 time: 37.59074831008911\n",
      "Iteration: 10 loss: 0.0004588096 time: 0.4724540710449219\n",
      "Iteration: 20 loss: 0.0003852494 time: 0.4412970542907715\n",
      "Iteration: 30 loss: 0.0003238366 time: 0.4315817356109619\n",
      "Iteration: 40 loss: 0.0002742001 time: 0.43503713607788086\n",
      "Iteration: 50 loss: 0.0002350905 time: 0.44859933853149414\n",
      "Iteration: 60 loss: 0.0002047677 time: 0.43678903579711914\n",
      "Iteration: 70 loss: 0.0001813453 time: 0.4439072608947754\n",
      "Iteration: 80 loss: 0.0001630702 time: 0.44216060638427734\n",
      "Iteration: 90 loss: 0.0001484875 time: 0.44797706604003906\n",
      "Iteration: 100 loss: 0.0001364949 time: 0.4495270252227783\n",
      "Iteration: 110 loss: 0.0001263194 time: 0.4411349296569824\n",
      "Iteration: 120 loss: 0.0001174522 time: 0.43564605712890625\n",
      "Iteration: 130 loss: 0.0001095725 time: 0.43149542808532715\n",
      "Iteration: 140 loss: 0.0001024813 time: 0.43908166885375977\n",
      "Iteration: 150 loss: 0.0000960535 time: 0.45523500442504883\n",
      "Iteration: 160 loss: 0.0000902072 time: 0.4323160648345947\n",
      "Iteration: 170 loss: 0.0000848864 time: 0.42719435691833496\n",
      "Iteration: 180 loss: 0.0000800504 time: 0.4300262928009033\n",
      "Iteration: 190 loss: 0.0000756682 time: 0.427182674407959\n",
      "Iteration: 200 loss: 0.0000717152 time: 0.4405643939971924\n",
      "Iteration: 210 loss: 0.0000681694 time: 0.4292442798614502\n",
      "Iteration: 220 loss: 0.0000650102 time: 0.4289083480834961\n",
      "Iteration: 230 loss: 0.0000622164 time: 0.43999576568603516\n",
      "Iteration: 240 loss: 0.0000597655 time: 0.4411942958831787\n",
      "Iteration: 250 loss: 0.0000576328 time: 0.4320557117462158\n",
      "Iteration: 260 loss: 0.0000557916 time: 0.43813323974609375\n",
      "Iteration: 270 loss: 0.0000542135 time: 0.42382383346557617\n",
      "Iteration: 280 loss: 0.0000528689 time: 0.4238109588623047\n",
      "Iteration: 290 loss: 0.0000517284 time: 0.43585777282714844\n",
      "Iteration: 300 loss: 0.0000507629 time: 0.4339320659637451\n",
      "Iteration: 310 loss: 0.0000499454 time: 0.42096710205078125\n",
      "Iteration: 320 loss: 0.0000492508 time: 0.4234278202056885\n",
      "Iteration: 330 loss: 0.0000486572 time: 0.41669416427612305\n",
      "Iteration: 340 loss: 0.0000481457 time: 0.4257471561431885\n",
      "Iteration: 350 loss: 0.0000477003 time: 0.4367189407348633\n",
      "Iteration: 360 loss: 0.0000473080 time: 0.4220592975616455\n",
      "Iteration: 370 loss: 0.0000469585 time: 0.4153940677642822\n",
      "Iteration: 380 loss: 0.0000466435 time: 0.41906237602233887\n",
      "Iteration: 390 loss: 0.0000463566 time: 0.419588565826416\n",
      "Iteration: 400 loss: 0.0000460927 time: 0.4080772399902344\n",
      "Iteration: 410 loss: 0.0000458480 time: 0.42026853561401367\n",
      "Iteration: 420 loss: 0.0000456192 time: 0.40883922576904297\n",
      "Iteration: 430 loss: 0.0000454040 time: 0.41296911239624023\n",
      "Iteration: 440 loss: 0.0000452004 time: 0.41536498069763184\n",
      "Iteration: 450 loss: 0.0000450066 time: 0.42289233207702637\n",
      "Iteration: 460 loss: 0.0000448213 time: 0.42261648178100586\n",
      "Iteration: 470 loss: 0.0000446432 time: 0.41904234886169434\n",
      "Iteration: 480 loss: 0.0000444711 time: 0.41645050048828125\n",
      "Iteration: 490 loss: 0.0000443042 time: 0.4233262538909912\n",
      "Iteration: 500 loss: 0.0000441416 time: 0.4160904884338379\n",
      "Iteration: 510 loss: 0.0000439825 time: 0.4350137710571289\n",
      "Iteration: 520 loss: 0.0000438262 time: 0.4115629196166992\n",
      "Iteration: 530 loss: 0.0000436720 time: 0.42540836334228516\n",
      "Iteration: 540 loss: 0.0000435194 time: 0.4160652160644531\n",
      "Iteration: 550 loss: 0.0000433678 time: 0.41405820846557617\n",
      "Iteration: 560 loss: 0.0000432168 time: 0.4477355480194092\n",
      "Iteration: 570 loss: 0.0000430659 time: 0.42972588539123535\n",
      "Iteration: 580 loss: 0.0000429147 time: 0.4308643341064453\n",
      "Iteration: 590 loss: 0.0000427629 time: 0.4186515808105469\n",
      "Iteration: 600 loss: 0.0000426101 time: 0.417316198348999\n",
      "Iteration: 610 loss: 0.0000424561 time: 0.4290962219238281\n",
      "Iteration: 620 loss: 0.0000423005 time: 0.4402141571044922\n",
      "Iteration: 630 loss: 0.0000421431 time: 0.4183964729309082\n",
      "Iteration: 640 loss: 0.0000419837 time: 0.41130900382995605\n",
      "Iteration: 650 loss: 0.0000418219 time: 0.4153759479522705\n",
      "Iteration: 660 loss: 0.0000416578 time: 0.41811323165893555\n",
      "Iteration: 670 loss: 0.0000414909 time: 0.43068671226501465\n",
      "Iteration: 680 loss: 0.0000413212 time: 0.40614819526672363\n",
      "Iteration: 690 loss: 0.0000411485 time: 0.39423561096191406\n",
      "Iteration: 700 loss: 0.0000409727 time: 0.40982794761657715\n",
      "Iteration: 710 loss: 0.0000407934 time: 0.41251611709594727\n",
      "Iteration: 720 loss: 0.0000406107 time: 0.4089813232421875\n",
      "Iteration: 730 loss: 0.0000404244 time: 0.41106176376342773\n",
      "Iteration: 740 loss: 0.0000402343 time: 0.40581774711608887\n",
      "Iteration: 750 loss: 0.0000400404 time: 0.39780616760253906\n",
      "Iteration: 760 loss: 0.0000398424 time: 0.3996565341949463\n",
      "Iteration: 770 loss: 0.0000396403 time: 0.39968276023864746\n",
      "Iteration: 780 loss: 0.0000394339 time: 0.39934349060058594\n",
      "Iteration: 790 loss: 0.0000392232 time: 0.40151476860046387\n",
      "Iteration: 800 loss: 0.0000390081 time: 0.41007542610168457\n",
      "Iteration: 810 loss: 0.0000387884 time: 0.4091665744781494\n",
      "Iteration: 820 loss: 0.0000385642 time: 0.4043738842010498\n",
      "Iteration: 830 loss: 0.0000383352 time: 0.4047675132751465\n",
      "Iteration: 840 loss: 0.0000381015 time: 0.40674567222595215\n",
      "Iteration: 850 loss: 0.0000378630 time: 0.4108102321624756\n",
      "Iteration: 860 loss: 0.0000376197 time: 0.400209903717041\n",
      "Iteration: 870 loss: 0.0000373715 time: 0.3990299701690674\n",
      "Iteration: 880 loss: 0.0000371186 time: 0.4018094539642334\n",
      "Iteration: 890 loss: 0.0000368608 time: 0.4019765853881836\n",
      "Iteration: 900 loss: 0.0000365982 time: 0.40209507942199707\n",
      "Iteration: 910 loss: 0.0000363310 time: 0.38981175422668457\n",
      "Iteration: 920 loss: 0.0000360591 time: 0.3948173522949219\n",
      "Iteration: 930 loss: 0.0000357828 time: 0.40506863594055176\n",
      "Iteration: 940 loss: 0.0000355021 time: 0.39980316162109375\n",
      "Iteration: 950 loss: 0.0000352173 time: 0.40183377265930176\n",
      "Iteration: 960 loss: 0.0000349286 time: 0.40228843688964844\n",
      "Iteration: 970 loss: 0.0000346363 time: 0.3860018253326416\n",
      "Iteration: 980 loss: 0.0000343406 time: 0.38910770416259766\n",
      "Iteration: 990 loss: 0.0000340419 time: 0.41174983978271484\n",
      "Iteration: 1000 loss: 0.0000337406 time: 0.3978888988494873\n",
      "Iteration: 1010 loss: 0.0000334371 time: 0.4035990238189697\n",
      "Iteration: 1020 loss: 0.0000331319 time: 0.3983149528503418\n",
      "Iteration: 1030 loss: 0.0000328255 time: 0.422940731048584\n",
      "Iteration: 1040 loss: 0.0000325184 time: 0.41330766677856445\n",
      "Iteration: 1050 loss: 0.0000322113 time: 0.439345121383667\n",
      "Iteration: 1060 loss: 0.0000319049 time: 0.4234001636505127\n",
      "Iteration: 1070 loss: 0.0000315996 time: 0.398423433303833\n",
      "Iteration: 1080 loss: 0.0000312964 time: 0.3839235305786133\n",
      "Iteration: 1090 loss: 0.0000309958 time: 0.42122650146484375\n",
      "Iteration: 1100 loss: 0.0000306986 time: 0.4352247714996338\n",
      "Iteration: 1110 loss: 0.0000304055 time: 0.4087188243865967\n",
      "Iteration: 1120 loss: 0.0000301173 time: 0.3947758674621582\n",
      "Iteration: 1130 loss: 0.0000298346 time: 0.39773035049438477\n",
      "Iteration: 1140 loss: 0.0000295582 time: 0.40342044830322266\n",
      "Iteration: 1150 loss: 0.0000292887 time: 0.3940889835357666\n",
      "Iteration: 1160 loss: 0.0000290266 time: 0.3979010581970215\n",
      "Iteration: 1170 loss: 0.0000287725 time: 0.3939487934112549\n",
      "Iteration: 1180 loss: 0.0000285269 time: 0.3964226245880127\n",
      "Iteration: 1190 loss: 0.0000282901 time: 0.39972782135009766\n",
      "Iteration: 1200 loss: 0.0000280625 time: 0.3983776569366455\n",
      "Iteration: 1210 loss: 0.0000278442 time: 0.40172529220581055\n",
      "Iteration: 1220 loss: 0.0000276353 time: 0.390239953994751\n",
      "Iteration: 1230 loss: 0.0000274358 time: 0.3835909366607666\n",
      "Iteration: 1240 loss: 0.0000272457 time: 0.4059751033782959\n",
      "Iteration: 1250 loss: 0.0000270648 time: 0.40926647186279297\n",
      "Iteration: 1260 loss: 0.0000268927 time: 0.395413875579834\n",
      "Iteration: 1270 loss: 0.0000267292 time: 0.4072420597076416\n",
      "Iteration: 1280 loss: 0.0000265739 time: 0.39260172843933105\n",
      "Iteration: 1290 loss: 0.0000264262 time: 0.38962721824645996\n",
      "Iteration: 1300 loss: 0.0000262857 time: 0.38257598876953125\n",
      "Iteration: 1310 loss: 0.0000261518 time: 0.3940849304199219\n",
      "Iteration: 1320 loss: 0.0000260240 time: 0.3949697017669678\n",
      "Iteration: 1330 loss: 0.0000259017 time: 0.39049363136291504\n",
      "Iteration: 1340 loss: 0.0000257843 time: 0.3904688358306885\n",
      "Iteration: 1350 loss: 0.0000256713 time: 0.3999629020690918\n",
      "Iteration: 1360 loss: 0.0000255622 time: 0.38147950172424316\n",
      "Iteration: 1370 loss: 0.0000254565 time: 0.39339256286621094\n",
      "Iteration: 1380 loss: 0.0000253537 time: 0.40085697174072266\n",
      "Iteration: 1390 loss: 0.0000252533 time: 0.38765478134155273\n",
      "Iteration: 1400 loss: 0.0000251549 time: 0.38865160942077637\n",
      "Iteration: 1410 loss: 0.0000250583 time: 0.39226484298706055\n",
      "Iteration: 1420 loss: 0.0000249630 time: 0.3960411548614502\n",
      "Iteration: 1430 loss: 0.0000248688 time: 0.4009225368499756\n",
      "Iteration: 1440 loss: 0.0000247755 time: 0.3943474292755127\n",
      "Iteration: 1450 loss: 0.0000246827 time: 0.39669322967529297\n",
      "Iteration: 1460 loss: 0.0000245904 time: 0.3826935291290283\n",
      "Iteration: 1470 loss: 0.0000244983 time: 0.413893461227417\n",
      "Iteration: 1480 loss: 0.0000244062 time: 0.3967704772949219\n",
      "Iteration: 1490 loss: 0.0000243142 time: 0.40712642669677734\n",
      "Iteration: 1500 loss: 0.0000242220 time: 0.39563703536987305\n",
      "Iteration: 1510 loss: 0.0000241295 time: 0.3882253170013428\n",
      "Iteration: 1520 loss: 0.0000240368 time: 0.3923985958099365\n",
      "Iteration: 1530 loss: 0.0000239436 time: 0.3907313346862793\n",
      "Iteration: 1540 loss: 0.0000238500 time: 0.3760673999786377\n",
      "Iteration: 1550 loss: 0.0000237558 time: 0.3991975784301758\n",
      "Iteration: 1560 loss: 0.0000236611 time: 0.38971471786499023\n",
      "Iteration: 1570 loss: 0.0000235658 time: 0.38421201705932617\n",
      "Iteration: 1580 loss: 0.0000234699 time: 0.38644886016845703\n",
      "Iteration: 1590 loss: 0.0000233733 time: 0.3850257396697998\n",
      "Iteration: 1600 loss: 0.0000232760 time: 0.3924751281738281\n",
      "Iteration: 1610 loss: 0.0000231780 time: 0.395282506942749\n",
      "Iteration: 1620 loss: 0.0000230792 time: 0.3849036693572998\n",
      "Iteration: 1630 loss: 0.0000229796 time: 0.4076731204986572\n",
      "Iteration: 1640 loss: 0.0000228792 time: 0.4201853275299072\n",
      "Iteration: 1650 loss: 0.0000227780 time: 0.41349339485168457\n",
      "Iteration: 1660 loss: 0.0000226760 time: 0.44592785835266113\n",
      "Iteration: 1670 loss: 0.0000225731 time: 0.42808032035827637\n",
      "Iteration: 1680 loss: 0.0000224693 time: 0.414109468460083\n",
      "Iteration: 1690 loss: 0.0000223645 time: 0.3978896141052246\n",
      "Iteration: 1700 loss: 0.0000222589 time: 0.4069364070892334\n",
      "Iteration: 1710 loss: 0.0000221524 time: 0.4104652404785156\n",
      "Iteration: 1720 loss: 0.0000220448 time: 0.4183461666107178\n",
      "Iteration: 1730 loss: 0.0000219364 time: 0.38022923469543457\n",
      "Iteration: 1740 loss: 0.0000218269 time: 0.3858211040496826\n",
      "Iteration: 1750 loss: 0.0000217165 time: 0.39215874671936035\n",
      "Iteration: 1760 loss: 0.0000216051 time: 0.3800313472747803\n",
      "Iteration: 1770 loss: 0.0000214926 time: 0.44086146354675293\n",
      "Iteration: 1780 loss: 0.0000213792 time: 0.3986833095550537\n",
      "Iteration: 1790 loss: 0.0000212647 time: 0.42572712898254395\n",
      "Iteration: 1800 loss: 0.0000211492 time: 0.4202251434326172\n",
      "Iteration: 1810 loss: 0.0000210327 time: 0.3869354724884033\n",
      "Iteration: 1820 loss: 0.0000209151 time: 0.39327383041381836\n",
      "Iteration: 1830 loss: 0.0000207965 time: 0.3984847068786621\n",
      "Iteration: 1840 loss: 0.0000206768 time: 0.39756226539611816\n",
      "Iteration: 1850 loss: 0.0000205560 time: 0.389955997467041\n",
      "Iteration: 1860 loss: 0.0000204342 time: 0.3965458869934082\n",
      "Iteration: 1870 loss: 0.0000203113 time: 0.39109230041503906\n",
      "Iteration: 1880 loss: 0.0000201873 time: 0.4099090099334717\n",
      "Iteration: 1890 loss: 0.0000200622 time: 0.3971877098083496\n",
      "Iteration: 1900 loss: 0.0000199360 time: 0.3948402404785156\n",
      "Iteration: 1910 loss: 0.0000198087 time: 0.38977932929992676\n",
      "Iteration: 1920 loss: 0.0000196802 time: 0.39168357849121094\n",
      "Iteration: 1930 loss: 0.0000195506 time: 0.3860335350036621\n",
      "Iteration: 1940 loss: 0.0000194198 time: 0.40940093994140625\n",
      "Iteration: 1950 loss: 0.0000192878 time: 0.4173429012298584\n",
      "Iteration: 1960 loss: 0.0000191545 time: 0.38550758361816406\n",
      "Iteration: 1970 loss: 0.0000190200 time: 0.4017519950866699\n",
      "Iteration: 1980 loss: 0.0000188841 time: 0.3901855945587158\n",
      "Iteration: 1990 loss: 0.0000187469 time: 0.41496777534484863\n",
      "Iteration: 2000 loss: 0.0000186082 time: 0.3875236511230469\n",
      "Iteration: 2010 loss: 0.0000184680 time: 0.40004491806030273\n",
      "Iteration: 2020 loss: 0.0000183263 time: 0.4004688262939453\n",
      "Iteration: 2030 loss: 0.0000181829 time: 0.3866913318634033\n",
      "Iteration: 2040 loss: 0.0000180378 time: 0.3919515609741211\n",
      "Iteration: 2050 loss: 0.0000178908 time: 0.4045395851135254\n",
      "Iteration: 2060 loss: 0.0000177419 time: 0.4028005599975586\n",
      "Iteration: 2070 loss: 0.0000175909 time: 0.3922598361968994\n",
      "Iteration: 2080 loss: 0.0000174377 time: 0.40436363220214844\n",
      "Iteration: 2090 loss: 0.0000172822 time: 0.38799357414245605\n",
      "Iteration: 2100 loss: 0.0000171241 time: 0.40120601654052734\n",
      "Iteration: 2110 loss: 0.0000169634 time: 0.39039182662963867\n",
      "Iteration: 2120 loss: 0.0000167999 time: 0.3901522159576416\n",
      "Iteration: 2130 loss: 0.0000166334 time: 0.3938257694244385\n",
      "Iteration: 2140 loss: 0.0000164637 time: 0.3801565170288086\n",
      "Iteration: 2150 loss: 0.0000162906 time: 0.38588786125183105\n",
      "Iteration: 2160 loss: 0.0000161139 time: 0.40163373947143555\n",
      "Iteration: 2170 loss: 0.0000159336 time: 0.38178086280822754\n",
      "Iteration: 2180 loss: 0.0000157492 time: 0.3820476531982422\n",
      "Iteration: 2190 loss: 0.0000155608 time: 0.38376569747924805\n",
      "Iteration: 2200 loss: 0.0000153681 time: 0.3777427673339844\n",
      "Iteration: 2210 loss: 0.0000151709 time: 0.39530134201049805\n",
      "Iteration: 2220 loss: 0.0000149691 time: 0.38936924934387207\n",
      "Iteration: 2230 loss: 0.0000147626 time: 0.3766331672668457\n",
      "Iteration: 2240 loss: 0.0000145512 time: 0.3902592658996582\n",
      "Iteration: 2250 loss: 0.0000143350 time: 0.38288211822509766\n",
      "Iteration: 2260 loss: 0.0000141138 time: 0.3827669620513916\n",
      "Iteration: 2270 loss: 0.0000138876 time: 0.39881110191345215\n",
      "Iteration: 2280 loss: 0.0000136566 time: 0.39741039276123047\n",
      "Iteration: 2290 loss: 0.0000134207 time: 0.3772299289703369\n",
      "Iteration: 2300 loss: 0.0000131802 time: 0.3872251510620117\n",
      "Iteration: 2310 loss: 0.0000129352 time: 0.39380836486816406\n",
      "Iteration: 2320 loss: 0.0000126860 time: 0.3903074264526367\n",
      "Iteration: 2330 loss: 0.0000124329 time: 0.3986997604370117\n",
      "Iteration: 2340 loss: 0.0000121763 time: 0.38109755516052246\n",
      "Iteration: 2350 loss: 0.0000119166 time: 0.3880171775817871\n",
      "Iteration: 2360 loss: 0.0000116543 time: 0.3853271007537842\n",
      "Iteration: 2370 loss: 0.0000113899 time: 0.3884305953979492\n",
      "Iteration: 2380 loss: 0.0000111240 time: 0.4015989303588867\n",
      "Iteration: 2390 loss: 0.0000108572 time: 0.38939690589904785\n",
      "Iteration: 2400 loss: 0.0000105903 time: 0.39287281036376953\n",
      "Iteration: 2410 loss: 0.0000103238 time: 0.3932490348815918\n",
      "Iteration: 2420 loss: 0.0000100584 time: 0.38306236267089844\n",
      "Iteration: 2430 loss: 0.0000097948 time: 0.39861607551574707\n",
      "Iteration: 2440 loss: 0.0000095337 time: 0.4082481861114502\n",
      "Iteration: 2450 loss: 0.0000092758 time: 0.38503289222717285\n",
      "Iteration: 2460 loss: 0.0000090215 time: 0.3933727741241455\n",
      "Iteration: 2470 loss: 0.0000087716 time: 0.37527036666870117\n",
      "Iteration: 2480 loss: 0.0000085264 time: 0.39329075813293457\n",
      "Iteration: 2490 loss: 0.0000082866 time: 0.3802320957183838\n",
      "Iteration: 2500 loss: 0.0000080524 time: 0.3978445529937744\n",
      "Iteration: 2510 loss: 0.0000078243 time: 0.401674747467041\n",
      "Iteration: 2520 loss: 0.0000076024 time: 0.4021949768066406\n",
      "Iteration: 2530 loss: 0.0000073870 time: 0.3977241516113281\n",
      "Iteration: 2540 loss: 0.0000071783 time: 0.4032902717590332\n",
      "Iteration: 2550 loss: 0.0000069762 time: 0.40058279037475586\n",
      "Iteration: 2560 loss: 0.0000067810 time: 0.4100382328033447\n",
      "Iteration: 2570 loss: 0.0000065924 time: 0.3987727165222168\n",
      "Iteration: 2580 loss: 0.0000064106 time: 0.40016603469848633\n",
      "Iteration: 2590 loss: 0.0000062353 time: 0.40275144577026367\n",
      "Iteration: 2600 loss: 0.0000060665 time: 0.40354442596435547\n",
      "Iteration: 2610 loss: 0.0000059040 time: 0.40567779541015625\n",
      "Iteration: 2620 loss: 0.0000057476 time: 0.3963747024536133\n",
      "Iteration: 2630 loss: 0.0000055972 time: 0.393294095993042\n",
      "Iteration: 2640 loss: 0.0000054526 time: 0.39874696731567383\n",
      "Iteration: 2650 loss: 0.0000053135 time: 0.40181660652160645\n",
      "Iteration: 2660 loss: 0.0000051798 time: 0.4092893600463867\n",
      "Iteration: 2670 loss: 0.0000050513 time: 0.38916015625\n",
      "Iteration: 2680 loss: 0.0000049278 time: 0.39230895042419434\n",
      "Iteration: 2690 loss: 0.0000048091 time: 0.4078238010406494\n",
      "Iteration: 2700 loss: 0.0000046950 time: 0.3873276710510254\n",
      "Iteration: 2710 loss: 0.0000045855 time: 0.3820500373840332\n",
      "Iteration: 2720 loss: 0.0000044802 time: 0.3940267562866211\n",
      "Iteration: 2730 loss: 0.0000043791 time: 0.38445591926574707\n",
      "Iteration: 2740 loss: 0.0000042820 time: 0.39887046813964844\n",
      "Iteration: 2750 loss: 0.0000041888 time: 0.3900015354156494\n",
      "Iteration: 2760 loss: 0.0000040994 time: 0.4358835220336914\n",
      "Iteration: 2770 loss: 0.0000040136 time: 0.4503471851348877\n",
      "Iteration: 2780 loss: 0.0000039313 time: 0.40813326835632324\n",
      "Iteration: 2790 loss: 0.0000038524 time: 0.43997812271118164\n",
      "Iteration: 2800 loss: 0.0000037768 time: 0.41066431999206543\n",
      "Iteration: 2810 loss: 0.0000037043 time: 0.4100937843322754\n",
      "Iteration: 2820 loss: 0.0000036350 time: 0.40630578994750977\n",
      "Iteration: 2830 loss: 0.0000035686 time: 0.39833736419677734\n",
      "Iteration: 2840 loss: 0.0000035051 time: 0.3774447441101074\n",
      "Iteration: 2850 loss: 0.0000034443 time: 0.354114294052124\n",
      "Iteration: 2860 loss: 0.0000033862 time: 0.4004831314086914\n",
      "Iteration: 2870 loss: 0.0000033307 time: 0.3930387496948242\n",
      "Iteration: 2880 loss: 0.0000032776 time: 0.3979146480560303\n",
      "Iteration: 2890 loss: 0.0000032269 time: 0.387221097946167\n",
      "Iteration: 2900 loss: 0.0000031785 time: 0.38515639305114746\n",
      "Iteration: 2910 loss: 0.0000031323 time: 0.3875906467437744\n",
      "Iteration: 2920 loss: 0.0000030881 time: 0.39716005325317383\n",
      "Iteration: 2930 loss: 0.0000030460 time: 0.3933417797088623\n",
      "Iteration: 2940 loss: 0.0000030058 time: 0.39838552474975586\n",
      "Iteration: 2950 loss: 0.0000029673 time: 0.39166927337646484\n",
      "Iteration: 2960 loss: 0.0000029306 time: 0.3981051445007324\n",
      "Iteration: 2970 loss: 0.0000028956 time: 0.3983640670776367\n",
      "Iteration: 2980 loss: 0.0000028621 time: 0.39611268043518066\n",
      "Iteration: 2990 loss: 0.0000028301 time: 0.4100322723388672\n",
      "Iteration: 3000 loss: 0.0000027996 time: 0.39483070373535156\n",
      "Iteration: 3010 loss: 0.0000027703 time: 0.39742183685302734\n",
      "Iteration: 3020 loss: 0.0000027423 time: 0.39901256561279297\n",
      "Iteration: 3030 loss: 0.0000027155 time: 0.3939650058746338\n",
      "Iteration: 3040 loss: 0.0000026898 time: 0.39235973358154297\n",
      "Iteration: 3050 loss: 0.0000026652 time: 0.405839204788208\n",
      "Iteration: 3060 loss: 0.0000026415 time: 0.399432897567749\n",
      "Iteration: 3070 loss: 0.0000026187 time: 0.39216184616088867\n",
      "Iteration: 3080 loss: 0.0000025969 time: 0.3971114158630371\n",
      "Iteration: 3090 loss: 0.0000025758 time: 0.40447235107421875\n",
      "Iteration: 3100 loss: 0.0000025555 time: 0.4066472053527832\n",
      "Iteration: 3110 loss: 0.0000025358 time: 0.41259002685546875\n",
      "Iteration: 3120 loss: 0.0000025169 time: 0.40244579315185547\n",
      "Iteration: 3130 loss: 0.0000024985 time: 0.39179134368896484\n",
      "Iteration: 3140 loss: 0.0000024807 time: 0.400590181350708\n",
      "Iteration: 3150 loss: 0.0000024634 time: 0.3975803852081299\n",
      "Iteration: 3160 loss: 0.0000024465 time: 0.41069817543029785\n",
      "Iteration: 3170 loss: 0.0000024301 time: 0.3931705951690674\n",
      "Iteration: 3180 loss: 0.0000024142 time: 0.40595173835754395\n",
      "Iteration: 3190 loss: 0.0000023985 time: 0.40247225761413574\n",
      "Iteration: 3200 loss: 0.0000023833 time: 0.4099392890930176\n",
      "Iteration: 3210 loss: 0.0000023683 time: 0.41902828216552734\n",
      "Iteration: 3220 loss: 0.0000023536 time: 0.3993568420410156\n",
      "Iteration: 3230 loss: 0.0000023392 time: 0.38805365562438965\n",
      "Iteration: 3240 loss: 0.0000023250 time: 0.400130033493042\n",
      "Iteration: 3250 loss: 0.0000023110 time: 0.40642786026000977\n",
      "Iteration: 3260 loss: 0.0000022972 time: 0.39331722259521484\n",
      "Iteration: 3270 loss: 0.0000022835 time: 0.41043567657470703\n",
      "Iteration: 3280 loss: 0.0000022700 time: 0.4112579822540283\n",
      "Iteration: 3290 loss: 0.0000022567 time: 0.39818596839904785\n",
      "Iteration: 3300 loss: 0.0000022435 time: 0.3968467712402344\n",
      "Iteration: 3310 loss: 0.0000022303 time: 0.4181051254272461\n",
      "Iteration: 3320 loss: 0.0000022173 time: 0.41248536109924316\n",
      "Iteration: 3330 loss: 0.0000022043 time: 0.41097164154052734\n",
      "Iteration: 3340 loss: 0.0000021914 time: 0.4067494869232178\n",
      "Iteration: 3350 loss: 0.0000021786 time: 0.40066957473754883\n",
      "Iteration: 3360 loss: 0.0000021658 time: 0.39638257026672363\n",
      "Iteration: 3370 loss: 0.0000021531 time: 0.39014101028442383\n",
      "Iteration: 3380 loss: 0.0000021403 time: 0.40231919288635254\n",
      "Iteration: 3390 loss: 0.0000021277 time: 0.39313840866088867\n",
      "Iteration: 3400 loss: 0.0000021150 time: 0.41178250312805176\n",
      "Iteration: 3410 loss: 0.0000021023 time: 0.3991374969482422\n",
      "Iteration: 3420 loss: 0.0000020897 time: 0.4034156799316406\n",
      "Iteration: 3430 loss: 0.0000020770 time: 0.40839552879333496\n",
      "Iteration: 3440 loss: 0.0000020644 time: 0.4123201370239258\n",
      "Iteration: 3450 loss: 0.0000020518 time: 0.4043607711791992\n",
      "Iteration: 3460 loss: 0.0000020391 time: 0.42582058906555176\n",
      "Iteration: 3470 loss: 0.0000020264 time: 0.38677525520324707\n",
      "Iteration: 3480 loss: 0.0000020138 time: 0.41126561164855957\n",
      "Iteration: 3490 loss: 0.0000020011 time: 0.4178893566131592\n",
      "Iteration: 3500 loss: 0.0000019884 time: 0.40189218521118164\n",
      "Iteration: 3510 loss: 0.0000019756 time: 0.4073460102081299\n",
      "Iteration: 3520 loss: 0.0000019629 time: 0.41707754135131836\n",
      "Iteration: 3530 loss: 0.0000019501 time: 0.40282249450683594\n",
      "Iteration: 3540 loss: 0.0000019373 time: 0.41635656356811523\n",
      "Iteration: 3550 loss: 0.0000019245 time: 0.40665245056152344\n",
      "Iteration: 3560 loss: 0.0000019117 time: 0.41594672203063965\n",
      "Iteration: 3570 loss: 0.0000018988 time: 0.40343809127807617\n",
      "Iteration: 3580 loss: 0.0000018860 time: 0.39760851860046387\n",
      "Iteration: 3590 loss: 0.0000018731 time: 0.403209924697876\n",
      "Iteration: 3600 loss: 0.0000018601 time: 0.4086294174194336\n",
      "Iteration: 3610 loss: 0.0000018472 time: 0.4179270267486572\n",
      "Iteration: 3620 loss: 0.0000018343 time: 0.4012269973754883\n",
      "Iteration: 3630 loss: 0.0000018213 time: 0.42397379875183105\n",
      "Iteration: 3640 loss: 0.0000018083 time: 0.41195154190063477\n",
      "Iteration: 3650 loss: 0.0000017953 time: 0.4147312641143799\n",
      "Iteration: 3660 loss: 0.0000017823 time: 0.4196488857269287\n",
      "Iteration: 3670 loss: 0.0000017693 time: 0.3895115852355957\n",
      "Iteration: 3680 loss: 0.0000017563 time: 0.40578293800354004\n",
      "Iteration: 3690 loss: 0.0000017432 time: 0.4068112373352051\n",
      "Iteration: 3700 loss: 0.0000017302 time: 0.40950679779052734\n",
      "Iteration: 3710 loss: 0.0000017172 time: 0.4071617126464844\n",
      "Iteration: 3720 loss: 0.0000017041 time: 0.4054851531982422\n",
      "Iteration: 3730 loss: 0.0000016911 time: 0.41543102264404297\n",
      "Iteration: 3740 loss: 0.0000016781 time: 0.4112429618835449\n",
      "Iteration: 3750 loss: 0.0000016651 time: 0.4053170680999756\n",
      "Iteration: 3760 loss: 0.0000016521 time: 0.4181215763092041\n",
      "Iteration: 3770 loss: 0.0000016391 time: 0.40847349166870117\n",
      "Iteration: 3780 loss: 0.0000016261 time: 0.4066298007965088\n",
      "Iteration: 3790 loss: 0.0000016132 time: 0.4084303379058838\n",
      "Iteration: 3800 loss: 0.0000016002 time: 0.4158332347869873\n",
      "Iteration: 3810 loss: 0.0000015873 time: 0.41115736961364746\n",
      "Iteration: 3820 loss: 0.0000015745 time: 0.41187500953674316\n",
      "Iteration: 3830 loss: 0.0000015617 time: 0.408203125\n",
      "Iteration: 3840 loss: 0.0000015489 time: 0.4018216133117676\n",
      "Iteration: 3850 loss: 0.0000015361 time: 0.4153463840484619\n",
      "Iteration: 3860 loss: 0.0000015234 time: 0.4072840213775635\n",
      "Iteration: 3870 loss: 0.0000015108 time: 0.40943408012390137\n",
      "Iteration: 3880 loss: 0.0000014982 time: 0.4092075824737549\n",
      "Iteration: 3890 loss: 0.0000014857 time: 0.40917062759399414\n",
      "Iteration: 3900 loss: 0.0000014732 time: 0.4202091693878174\n",
      "Iteration: 3910 loss: 0.0000014608 time: 0.39562106132507324\n",
      "Iteration: 3920 loss: 0.0000014484 time: 0.4212667942047119\n",
      "Iteration: 3930 loss: 0.0000014362 time: 0.40064072608947754\n",
      "Iteration: 3940 loss: 0.0000014240 time: 0.40167975425720215\n",
      "Iteration: 3950 loss: 0.0000014119 time: 0.3997623920440674\n",
      "Iteration: 3960 loss: 0.0000013998 time: 0.42254018783569336\n",
      "Iteration: 3970 loss: 0.0000013879 time: 0.427095890045166\n",
      "Iteration: 3980 loss: 0.0000013760 time: 0.4115011692047119\n",
      "Iteration: 3990 loss: 0.0000013643 time: 0.42029714584350586\n",
      "Iteration: 4000 loss: 0.0000013526 time: 0.42014312744140625\n",
      "Iteration: 4010 loss: 0.0000013410 time: 0.4097745418548584\n",
      "Iteration: 4020 loss: 0.0000013296 time: 0.4187006950378418\n",
      "Iteration: 4030 loss: 0.0000013182 time: 0.4142792224884033\n",
      "Iteration: 4040 loss: 0.0000013069 time: 0.4084188938140869\n",
      "Iteration: 4050 loss: 0.0000012958 time: 0.41528749465942383\n",
      "Iteration: 4060 loss: 0.0000012848 time: 0.404848575592041\n",
      "Iteration: 4070 loss: 0.0000012738 time: 0.39547276496887207\n",
      "Iteration: 4080 loss: 0.0000012630 time: 0.42479634284973145\n",
      "Iteration: 4090 loss: 0.0000012524 time: 0.4149622917175293\n",
      "Iteration: 4100 loss: 0.0000012418 time: 0.40755796432495117\n",
      "Iteration: 4110 loss: 0.0000012313 time: 0.40940427780151367\n",
      "Iteration: 4120 loss: 0.0000012210 time: 0.40474843978881836\n",
      "Iteration: 4130 loss: 0.0000012108 time: 0.41837596893310547\n",
      "Iteration: 4140 loss: 0.0000012007 time: 0.40896153450012207\n",
      "Iteration: 4150 loss: 0.0000011908 time: 0.4157547950744629\n",
      "Iteration: 4160 loss: 0.0000011810 time: 0.40383434295654297\n",
      "Iteration: 4170 loss: 0.0000011713 time: 0.4071979522705078\n",
      "Iteration: 4180 loss: 0.0000011617 time: 0.41672658920288086\n",
      "Iteration: 4190 loss: 0.0000011523 time: 0.4156942367553711\n",
      "Iteration: 4200 loss: 0.0000011429 time: 0.4059257507324219\n",
      "Iteration: 4210 loss: 0.0000011337 time: 0.40474987030029297\n",
      "Iteration: 4220 loss: 0.0000011247 time: 0.40270137786865234\n",
      "Iteration: 4230 loss: 0.0000011157 time: 0.4020521640777588\n",
      "Iteration: 4240 loss: 0.0000011069 time: 0.4105048179626465\n",
      "Iteration: 4250 loss: 0.0000010982 time: 0.41098999977111816\n",
      "Iteration: 4260 loss: 0.0000010896 time: 0.40369558334350586\n",
      "Iteration: 4270 loss: 0.0000010812 time: 0.40349745750427246\n",
      "Iteration: 4280 loss: 0.0000010728 time: 0.40735816955566406\n",
      "Iteration: 4290 loss: 0.0000010646 time: 0.4149589538574219\n",
      "Iteration: 4300 loss: 0.0000010565 time: 0.41178131103515625\n",
      "Iteration: 4310 loss: 0.0000010485 time: 0.4079258441925049\n",
      "Iteration: 4320 loss: 0.0000010406 time: 0.41460084915161133\n",
      "Iteration: 4330 loss: 0.0000010328 time: 0.4134972095489502\n",
      "Iteration: 4340 loss: 0.0000010252 time: 0.40726184844970703\n",
      "Iteration: 4350 loss: 0.0000010176 time: 0.4163799285888672\n",
      "Iteration: 4360 loss: 0.0000010102 time: 0.41106462478637695\n",
      "Iteration: 4370 loss: 0.0000010028 time: 0.4016280174255371\n",
      "Iteration: 4380 loss: 0.0000009955 time: 0.3961467742919922\n",
      "Iteration: 4390 loss: 0.0000009884 time: 0.39862775802612305\n",
      "Iteration: 4400 loss: 0.0000009813 time: 0.41268205642700195\n",
      "Iteration: 4410 loss: 0.0000009743 time: 0.4066920280456543\n",
      "Iteration: 4420 loss: 0.0000009674 time: 0.4135754108428955\n",
      "Iteration: 4430 loss: 0.0000009606 time: 0.40999698638916016\n",
      "Iteration: 4440 loss: 0.0000009539 time: 0.40804433822631836\n",
      "Iteration: 4450 loss: 0.0000009472 time: 0.40421366691589355\n",
      "Iteration: 4460 loss: 0.0000009407 time: 0.40665125846862793\n",
      "Iteration: 4470 loss: 0.0000009342 time: 0.4002060890197754\n",
      "Iteration: 4480 loss: 0.0000009278 time: 0.3969135284423828\n",
      "Iteration: 4490 loss: 0.0000009214 time: 0.4216153621673584\n",
      "Iteration: 4500 loss: 0.0000009151 time: 0.39926934242248535\n",
      "Iteration: 4510 loss: 0.0000009089 time: 0.4144022464752197\n",
      "Iteration: 4520 loss: 0.0000009028 time: 0.4103829860687256\n",
      "Iteration: 4530 loss: 0.0000008967 time: 0.41149473190307617\n",
      "Iteration: 4540 loss: 0.0000008907 time: 0.39197373390197754\n",
      "Iteration: 4550 loss: 0.0000008847 time: 0.40324902534484863\n",
      "Iteration: 4560 loss: 0.0000008788 time: 0.4071195125579834\n",
      "Iteration: 4570 loss: 0.0000008729 time: 0.4115145206451416\n",
      "Iteration: 4580 loss: 0.0000008671 time: 0.4091525077819824\n",
      "Iteration: 4590 loss: 0.0000008614 time: 0.40879058837890625\n",
      "Iteration: 4600 loss: 0.0000008556 time: 0.40703797340393066\n",
      "Iteration: 4610 loss: 0.0000008500 time: 0.40308237075805664\n",
      "Iteration: 4620 loss: 0.0000008444 time: 0.41126036643981934\n",
      "Iteration: 4630 loss: 0.0000008388 time: 0.4161500930786133\n",
      "Iteration: 4640 loss: 0.0000008332 time: 0.4020044803619385\n",
      "Iteration: 4650 loss: 0.0000008277 time: 0.40943074226379395\n",
      "Iteration: 4660 loss: 0.0000008223 time: 0.40840864181518555\n",
      "Iteration: 4670 loss: 0.0000008169 time: 0.41420745849609375\n",
      "Iteration: 4680 loss: 0.0000008115 time: 0.40233302116394043\n",
      "Iteration: 4690 loss: 0.0000008061 time: 0.40483713150024414\n",
      "Iteration: 4700 loss: 0.0000008008 time: 0.4109647274017334\n",
      "Iteration: 4710 loss: 0.0000007955 time: 0.4061264991760254\n",
      "Iteration: 4720 loss: 0.0000007902 time: 0.40054917335510254\n",
      "Iteration: 4730 loss: 0.0000007850 time: 0.4156363010406494\n",
      "Iteration: 4740 loss: 0.0000007798 time: 0.4136197566986084\n",
      "Iteration: 4750 loss: 0.0000007746 time: 0.4090147018432617\n",
      "Iteration: 4760 loss: 0.0000007694 time: 0.4092369079589844\n",
      "Iteration: 4770 loss: 0.0000007643 time: 0.39008474349975586\n",
      "Iteration: 4780 loss: 0.0000007592 time: 0.4224872589111328\n",
      "Iteration: 4790 loss: 0.0000007541 time: 0.40738701820373535\n",
      "Iteration: 4800 loss: 0.0000007490 time: 0.4150276184082031\n",
      "Iteration: 4810 loss: 0.0000007440 time: 0.39826488494873047\n",
      "Iteration: 4820 loss: 0.0000007390 time: 0.3911736011505127\n",
      "Iteration: 4830 loss: 0.0000007340 time: 0.3983781337738037\n",
      "Iteration: 4840 loss: 0.0000007290 time: 0.41275835037231445\n",
      "Iteration: 4850 loss: 0.0000007240 time: 0.39888811111450195\n",
      "Iteration: 4860 loss: 0.0000007191 time: 0.3909149169921875\n",
      "Iteration: 4870 loss: 0.0000007142 time: 0.39391565322875977\n",
      "Iteration: 4880 loss: 0.0000007093 time: 0.40145254135131836\n",
      "Iteration: 4890 loss: 0.0000007044 time: 0.41550135612487793\n",
      "Iteration: 4900 loss: 0.0000006995 time: 0.4011516571044922\n",
      "Iteration: 4910 loss: 0.0000006946 time: 0.4018545150756836\n",
      "Iteration: 4920 loss: 0.0000006898 time: 0.4083218574523926\n",
      "Iteration: 4930 loss: 0.0000006850 time: 0.4033186435699463\n",
      "Iteration: 4940 loss: 0.0000006802 time: 0.4097573757171631\n",
      "Iteration: 4950 loss: 0.0000006754 time: 0.40186190605163574\n",
      "Iteration: 4960 loss: 0.0000006706 time: 0.39632678031921387\n",
      "Iteration: 4970 loss: 0.0000006659 time: 0.402315616607666\n",
      "Iteration: 4980 loss: 0.0000006612 time: 0.4019491672515869\n",
      "Iteration: 4990 loss: 0.0000006564 time: 0.4076685905456543\n",
      "Iteration: 5000 loss: 0.0000006517 time: 0.41187572479248047\n",
      "Iteration: 5010 loss: 0.0000006470 time: 0.3964719772338867\n",
      "Iteration: 5020 loss: 0.0000006424 time: 0.40271759033203125\n",
      "Iteration: 5030 loss: 0.0000006377 time: 0.4007902145385742\n",
      "Iteration: 5040 loss: 0.0000006331 time: 0.4060392379760742\n",
      "Iteration: 5050 loss: 0.0000006285 time: 0.41429781913757324\n",
      "Iteration: 5060 loss: 0.0000006239 time: 0.41556787490844727\n",
      "Iteration: 5070 loss: 0.0000006193 time: 0.39512157440185547\n",
      "Iteration: 5080 loss: 0.0000006148 time: 0.4022798538208008\n",
      "Iteration: 5090 loss: 0.0000006102 time: 0.3929731845855713\n",
      "Iteration: 5100 loss: 0.0000006057 time: 0.4066884517669678\n",
      "Iteration: 5110 loss: 0.0000006012 time: 0.40758419036865234\n",
      "Iteration: 5120 loss: 0.0000005967 time: 0.40189337730407715\n",
      "Iteration: 5130 loss: 0.0000005922 time: 0.4078962802886963\n",
      "Iteration: 5140 loss: 0.0000005878 time: 0.3962867259979248\n",
      "Iteration: 5150 loss: 0.0000005834 time: 0.3983428478240967\n",
      "Iteration: 5160 loss: 0.0000005790 time: 0.41966843605041504\n",
      "Iteration: 5170 loss: 0.0000005746 time: 0.4152064323425293\n",
      "Iteration: 5180 loss: 0.0000005702 time: 0.4058840274810791\n",
      "Iteration: 5190 loss: 0.0000005659 time: 0.40502071380615234\n",
      "Iteration: 5200 loss: 0.0000005615 time: 0.41229248046875\n",
      "Iteration: 5210 loss: 0.0000005572 time: 0.40552806854248047\n",
      "Iteration: 5220 loss: 0.0000005530 time: 0.4200272560119629\n",
      "Iteration: 5230 loss: 0.0000005487 time: 0.4090616703033447\n",
      "Iteration: 5240 loss: 0.0000005445 time: 0.4043266773223877\n",
      "Iteration: 5250 loss: 0.0000005402 time: 0.41301846504211426\n",
      "Iteration: 5260 loss: 0.0000005360 time: 0.39761948585510254\n",
      "Iteration: 5270 loss: 0.0000005319 time: 0.40883946418762207\n",
      "Iteration: 5280 loss: 0.0000005277 time: 0.4042680263519287\n",
      "Iteration: 5290 loss: 0.0000005236 time: 0.4000256061553955\n",
      "Iteration: 5300 loss: 0.0000005195 time: 0.39937448501586914\n",
      "Iteration: 5310 loss: 0.0000005154 time: 0.4031054973602295\n",
      "Iteration: 5320 loss: 0.0000005113 time: 0.40557003021240234\n",
      "Iteration: 5330 loss: 0.0000005073 time: 0.4118766784667969\n",
      "Iteration: 5340 loss: 0.0000005033 time: 0.4008755683898926\n",
      "Iteration: 5350 loss: 0.0000004993 time: 0.4192643165588379\n",
      "Iteration: 5360 loss: 0.0000004953 time: 0.4042692184448242\n",
      "Iteration: 5370 loss: 0.0000004913 time: 0.39458465576171875\n",
      "Iteration: 5380 loss: 0.0000004874 time: 0.4093630313873291\n",
      "Iteration: 5390 loss: 0.0000004835 time: 0.3980295658111572\n",
      "Iteration: 5400 loss: 0.0000004796 time: 0.3960254192352295\n",
      "Iteration: 5410 loss: 0.0000004757 time: 0.39952540397644043\n",
      "Iteration: 5420 loss: 0.0000004719 time: 0.4139878749847412\n",
      "Iteration: 5430 loss: 0.0000004680 time: 0.4113941192626953\n",
      "Iteration: 5440 loss: 0.0000004642 time: 0.3994748592376709\n",
      "Iteration: 5450 loss: 0.0000004604 time: 0.4125833511352539\n",
      "Iteration: 5460 loss: 0.0000004566 time: 0.3970375061035156\n",
      "Iteration: 5470 loss: 0.0000004529 time: 0.4051997661590576\n",
      "Iteration: 5480 loss: 0.0000004491 time: 0.40581464767456055\n",
      "Iteration: 5490 loss: 0.0000004454 time: 0.4054405689239502\n",
      "Iteration: 5500 loss: 0.0000004417 time: 0.39478397369384766\n",
      "Iteration: 5510 loss: 0.0000004380 time: 0.41107678413391113\n",
      "Iteration: 5520 loss: 0.0000004343 time: 0.39636826515197754\n",
      "Iteration: 5530 loss: 0.0000004307 time: 0.40477919578552246\n",
      "Iteration: 5540 loss: 0.0000004270 time: 0.41220998764038086\n",
      "Iteration: 5550 loss: 0.0000004234 time: 0.4099457263946533\n",
      "Iteration: 5560 loss: 0.0000004198 time: 0.39772510528564453\n",
      "Iteration: 5570 loss: 0.0000004162 time: 0.40007758140563965\n",
      "Iteration: 5580 loss: 0.0000004126 time: 0.4082767963409424\n",
      "Iteration: 5590 loss: 0.0000004090 time: 0.40707945823669434\n",
      "Iteration: 5600 loss: 0.0000004054 time: 0.4075784683227539\n",
      "Iteration: 5610 loss: 0.0000004019 time: 0.39669036865234375\n",
      "Iteration: 5620 loss: 0.0000003984 time: 0.4034576416015625\n",
      "Iteration: 5630 loss: 0.0000003948 time: 0.40105247497558594\n",
      "Iteration: 5640 loss: 0.0000003913 time: 0.4113483428955078\n",
      "Iteration: 5650 loss: 0.0000003878 time: 0.4094092845916748\n",
      "Iteration: 5660 loss: 0.0000003843 time: 0.4171581268310547\n",
      "Iteration: 5670 loss: 0.0000003809 time: 0.40320277214050293\n",
      "Iteration: 5680 loss: 0.0000003774 time: 0.4112894535064697\n",
      "Iteration: 5690 loss: 0.0000003739 time: 0.4078681468963623\n",
      "Iteration: 5700 loss: 0.0000003705 time: 0.4123194217681885\n",
      "Iteration: 5710 loss: 0.0000003671 time: 0.41104793548583984\n",
      "Iteration: 5720 loss: 0.0000003636 time: 0.41518330574035645\n",
      "Iteration: 5730 loss: 0.0000003602 time: 0.41576337814331055\n",
      "Iteration: 5740 loss: 0.0000003568 time: 0.4096863269805908\n",
      "Iteration: 5750 loss: 0.0000003534 time: 0.40435075759887695\n",
      "Iteration: 5760 loss: 0.0000003500 time: 0.41353797912597656\n",
      "Iteration: 5770 loss: 0.0000003467 time: 0.41500258445739746\n",
      "Iteration: 5780 loss: 0.0000003433 time: 0.39905881881713867\n",
      "Iteration: 5790 loss: 0.0000003399 time: 0.4084792137145996\n",
      "Iteration: 5800 loss: 0.0000003366 time: 0.40981221199035645\n",
      "Iteration: 5810 loss: 0.0000003332 time: 0.40555787086486816\n",
      "Iteration: 5820 loss: 0.0000003299 time: 0.4221687316894531\n",
      "Iteration: 5830 loss: 0.0000003266 time: 0.414691686630249\n",
      "Iteration: 5840 loss: 0.0000003232 time: 0.41256165504455566\n",
      "Iteration: 5850 loss: 0.0000003199 time: 0.4321563243865967\n",
      "Iteration: 5860 loss: 0.0000003166 time: 0.427443265914917\n",
      "Iteration: 5870 loss: 0.0000003133 time: 0.4076802730560303\n",
      "Iteration: 5880 loss: 0.0000003101 time: 0.4165072441101074\n",
      "Iteration: 5890 loss: 0.0000003068 time: 0.42623162269592285\n",
      "Iteration: 5900 loss: 0.0000003035 time: 0.4123721122741699\n",
      "Iteration: 5910 loss: 0.0000003003 time: 0.41576123237609863\n",
      "Iteration: 5920 loss: 0.0000002970 time: 0.42023372650146484\n",
      "Iteration: 5930 loss: 0.0000002938 time: 0.4104170799255371\n",
      "Iteration: 5940 loss: 0.0000002906 time: 0.41065311431884766\n",
      "Iteration: 5950 loss: 0.0000002874 time: 0.4153134822845459\n",
      "Iteration: 5960 loss: 0.0000002842 time: 0.4227168560028076\n",
      "Iteration: 5970 loss: 0.0000002810 time: 0.4161703586578369\n",
      "Iteration: 5980 loss: 0.0000002778 time: 0.41184496879577637\n",
      "Iteration: 5990 loss: 0.0000002746 time: 0.4131178855895996\n",
      "Iteration: 6000 loss: 0.0000002715 time: 0.41892099380493164\n",
      "Iteration: 6010 loss: 0.0000002684 time: 0.40579938888549805\n",
      "Iteration: 6020 loss: 0.0000002652 time: 0.4153885841369629\n",
      "Iteration: 6030 loss: 0.0000002621 time: 0.4267234802246094\n",
      "Iteration: 6040 loss: 0.0000002590 time: 0.40901947021484375\n",
      "Iteration: 6050 loss: 0.0000002560 time: 0.4035789966583252\n",
      "Iteration: 6060 loss: 0.0000002529 time: 0.4033973217010498\n",
      "Iteration: 6070 loss: 0.0000002499 time: 0.41100406646728516\n",
      "Iteration: 6080 loss: 0.0000002468 time: 0.4094047546386719\n",
      "Iteration: 6090 loss: 0.0000002438 time: 0.3961958885192871\n",
      "Iteration: 6100 loss: 0.0000002408 time: 0.40378856658935547\n",
      "Iteration: 6110 loss: 0.0000002378 time: 0.4139723777770996\n",
      "Iteration: 6120 loss: 0.0000002349 time: 0.39671921730041504\n",
      "Iteration: 6130 loss: 0.0000002319 time: 0.4160165786743164\n",
      "Iteration: 6140 loss: 0.0000002290 time: 0.41072869300842285\n",
      "Iteration: 6150 loss: 0.0000002261 time: 0.40312790870666504\n",
      "Iteration: 6160 loss: 0.0000002232 time: 0.4071328639984131\n",
      "Iteration: 6170 loss: 0.0000002204 time: 0.4086158275604248\n",
      "Iteration: 6180 loss: 0.0000002175 time: 0.4005405902862549\n",
      "Iteration: 6190 loss: 0.0000002147 time: 0.42348790168762207\n",
      "Iteration: 6200 loss: 0.0000002119 time: 0.4113137722015381\n",
      "Iteration: 6210 loss: 0.0000002091 time: 0.4030423164367676\n",
      "Iteration: 6220 loss: 0.0000002064 time: 0.403118371963501\n",
      "Iteration: 6230 loss: 0.0000002036 time: 0.4117434024810791\n",
      "Iteration: 6240 loss: 0.0000002009 time: 0.4066505432128906\n",
      "Iteration: 6250 loss: 0.0000001982 time: 0.4142887592315674\n",
      "Iteration: 6260 loss: 0.0000001956 time: 0.40128135681152344\n",
      "Iteration: 6270 loss: 0.0000001929 time: 0.4055943489074707\n",
      "Iteration: 6280 loss: 0.0000001903 time: 0.4052577018737793\n",
      "Iteration: 6290 loss: 0.0000001877 time: 0.4016387462615967\n",
      "Iteration: 6300 loss: 0.0000001852 time: 0.41638684272766113\n",
      "Iteration: 6310 loss: 0.0000001827 time: 0.4026358127593994\n",
      "Iteration: 6320 loss: 0.0000001801 time: 0.4043881893157959\n",
      "Iteration: 6330 loss: 0.0000001777 time: 0.41080355644226074\n",
      "Iteration: 6340 loss: 0.0000001752 time: 0.41460633277893066\n",
      "Iteration: 6350 loss: 0.0000001728 time: 0.4140331745147705\n",
      "Iteration: 6360 loss: 0.0000001704 time: 0.4093647003173828\n",
      "Iteration: 6370 loss: 0.0000001680 time: 0.3966536521911621\n",
      "Iteration: 6380 loss: 0.0000001657 time: 0.41129374504089355\n",
      "Iteration: 6390 loss: 0.0000001633 time: 0.4134511947631836\n",
      "Iteration: 6400 loss: 0.0000001611 time: 0.4153594970703125\n",
      "Iteration: 6410 loss: 0.0000001588 time: 0.42113614082336426\n",
      "Iteration: 6420 loss: 0.0000001566 time: 0.40991687774658203\n",
      "Iteration: 6430 loss: 0.0000001544 time: 0.41410326957702637\n",
      "Iteration: 6440 loss: 0.0000001522 time: 0.40001583099365234\n",
      "Iteration: 6450 loss: 0.0000001500 time: 0.39954400062561035\n",
      "Iteration: 6460 loss: 0.0000001479 time: 0.42859554290771484\n",
      "Iteration: 6470 loss: 0.0000001458 time: 0.4013707637786865\n",
      "Iteration: 6480 loss: 0.0000001437 time: 0.4027125835418701\n",
      "Iteration: 6490 loss: 0.0000001417 time: 0.3879859447479248\n",
      "Iteration: 6500 loss: 0.0000001397 time: 0.40198659896850586\n",
      "Iteration: 6510 loss: 0.0000001377 time: 0.4056377410888672\n",
      "Iteration: 6520 loss: 0.0000001358 time: 0.4111204147338867\n",
      "Iteration: 6530 loss: 0.0000001338 time: 0.4062039852142334\n",
      "Iteration: 6540 loss: 0.0000001319 time: 0.3939995765686035\n",
      "Iteration: 6550 loss: 0.0000001301 time: 0.3934016227722168\n",
      "Iteration: 6560 loss: 0.0000001282 time: 0.40250563621520996\n",
      "Iteration: 6570 loss: 0.0000001264 time: 0.41038036346435547\n",
      "Iteration: 6580 loss: 0.0000001246 time: 0.3999190330505371\n",
      "Iteration: 6590 loss: 0.0000001229 time: 0.3872377872467041\n",
      "Iteration: 6600 loss: 0.0000001211 time: 0.40253305435180664\n",
      "Iteration: 6610 loss: 0.0000001194 time: 0.38875746726989746\n",
      "Iteration: 6620 loss: 0.0000001178 time: 0.3952343463897705\n",
      "Iteration: 6630 loss: 0.0000001161 time: 0.3948793411254883\n",
      "Iteration: 6640 loss: 0.0000001145 time: 0.397982120513916\n",
      "Iteration: 6650 loss: 0.0000001129 time: 0.40007877349853516\n",
      "Iteration: 6660 loss: 0.0000001113 time: 0.39757561683654785\n",
      "Iteration: 6670 loss: 0.0000001097 time: 0.3982350826263428\n",
      "Iteration: 6680 loss: 0.0000001082 time: 0.41588759422302246\n",
      "Iteration: 6690 loss: 0.0000001067 time: 0.39268040657043457\n",
      "Iteration: 6700 loss: 0.0000001052 time: 0.40026354789733887\n",
      "Iteration: 6710 loss: 0.0000001038 time: 0.40778136253356934\n",
      "Iteration: 6720 loss: 0.0000001023 time: 0.39767956733703613\n",
      "Iteration: 6730 loss: 0.0000001009 time: 0.40264201164245605\n",
      "Iteration: 6740 loss: 0.0000000996 time: 0.4046967029571533\n",
      "Iteration: 6750 loss: 0.0000000982 time: 0.4028778076171875\n",
      "Iteration: 6760 loss: 0.0000000969 time: 0.3932070732116699\n",
      "Iteration: 6770 loss: 0.0000000955 time: 0.3964505195617676\n",
      "Iteration: 6780 loss: 0.0000000942 time: 0.4218463897705078\n",
      "Iteration: 6790 loss: 0.0000000930 time: 0.40828633308410645\n",
      "Iteration: 6800 loss: 0.0000000917 time: 0.40934085845947266\n",
      "Iteration: 6810 loss: 0.0000000905 time: 0.3988983631134033\n",
      "Iteration: 6820 loss: 0.0000000893 time: 0.40543556213378906\n",
      "Iteration: 6830 loss: 0.0000000881 time: 0.39429283142089844\n",
      "Iteration: 6840 loss: 0.0000000869 time: 0.41000938415527344\n",
      "Iteration: 6850 loss: 0.0000000858 time: 0.4118788242340088\n",
      "Iteration: 6860 loss: 0.0000000846 time: 0.4017157554626465\n",
      "Iteration: 6870 loss: 0.0000000835 time: 0.4028024673461914\n",
      "Iteration: 6880 loss: 0.0000000824 time: 0.4000086784362793\n",
      "Iteration: 6890 loss: 0.0000000813 time: 0.4011244773864746\n",
      "Iteration: 6900 loss: 0.0000000803 time: 0.42218637466430664\n",
      "Iteration: 6910 loss: 0.0000000792 time: 0.39684367179870605\n",
      "Iteration: 6920 loss: 0.0000000782 time: 0.39415621757507324\n",
      "Iteration: 6930 loss: 0.0000000772 time: 0.3970012664794922\n",
      "Iteration: 6940 loss: 0.0000000762 time: 0.39783287048339844\n",
      "Iteration: 6950 loss: 0.0000000752 time: 0.3978302478790283\n",
      "Iteration: 6960 loss: 0.0000000743 time: 0.4085361957550049\n",
      "Iteration: 6970 loss: 0.0000000733 time: 0.39061498641967773\n",
      "Iteration: 6980 loss: 0.0000000724 time: 0.4017796516418457\n",
      "Iteration: 6990 loss: 0.0000000715 time: 0.40294742584228516\n",
      "Iteration: 7000 loss: 0.0000000706 time: 0.39815378189086914\n",
      "Iteration: 7010 loss: 0.0000000697 time: 0.40319228172302246\n",
      "Iteration: 7020 loss: 0.0000000689 time: 0.395078182220459\n",
      "Iteration: 7030 loss: 0.0000000680 time: 0.39023375511169434\n",
      "Iteration: 7040 loss: 0.0000000672 time: 0.39420080184936523\n",
      "Iteration: 7050 loss: 0.0000000664 time: 0.40621256828308105\n",
      "Iteration: 7060 loss: 0.0000000655 time: 0.3983120918273926\n",
      "Iteration: 7070 loss: 0.0000000647 time: 0.3939223289489746\n",
      "Iteration: 7080 loss: 0.0000000640 time: 0.39504289627075195\n",
      "Iteration: 7090 loss: 0.0000000632 time: 0.38749194145202637\n",
      "Iteration: 7100 loss: 0.0000000624 time: 0.41428422927856445\n",
      "Iteration: 7110 loss: 0.0000000617 time: 0.4020416736602783\n",
      "Iteration: 7120 loss: 0.0000000609 time: 0.4100327491760254\n",
      "Iteration: 7130 loss: 0.0000000602 time: 0.3969454765319824\n",
      "Iteration: 7140 loss: 0.0000000595 time: 0.4058647155761719\n",
      "Iteration: 7150 loss: 0.0000000588 time: 0.3954019546508789\n",
      "Iteration: 7160 loss: 0.0000000581 time: 0.4013099670410156\n",
      "Iteration: 7170 loss: 0.0000000574 time: 0.4064755439758301\n",
      "Iteration: 7180 loss: 0.0000000568 time: 0.40689826011657715\n",
      "Iteration: 7190 loss: 0.0000000561 time: 0.3934299945831299\n",
      "Iteration: 7200 loss: 0.0000000555 time: 0.3985903263092041\n",
      "Iteration: 7210 loss: 0.0000000548 time: 0.38919639587402344\n",
      "Iteration: 7220 loss: 0.0000000542 time: 0.3940589427947998\n",
      "Iteration: 7230 loss: 0.0000000536 time: 0.4143083095550537\n",
      "Iteration: 7240 loss: 0.0000000530 time: 0.3961315155029297\n",
      "Iteration: 7250 loss: 0.0000000524 time: 0.3984677791595459\n",
      "Iteration: 7260 loss: 0.0000000518 time: 0.3907179832458496\n",
      "Iteration: 7270 loss: 0.0000000512 time: 0.39057469367980957\n",
      "Iteration: 7280 loss: 0.0000000506 time: 0.4015512466430664\n",
      "Iteration: 7290 loss: 0.0000000500 time: 0.4214818477630615\n",
      "Iteration: 7300 loss: 0.0000000495 time: 0.4010598659515381\n",
      "Iteration: 7310 loss: 0.0000000489 time: 0.3947310447692871\n",
      "Iteration: 7320 loss: 0.0000000484 time: 0.39357686042785645\n",
      "Iteration: 7330 loss: 0.0000000479 time: 0.3912997245788574\n",
      "Iteration: 7340 loss: 0.0000000473 time: 0.4072835445404053\n",
      "Iteration: 7350 loss: 0.0000000468 time: 0.3977694511413574\n",
      "Iteration: 7360 loss: 0.0000000463 time: 0.3969902992248535\n",
      "Iteration: 7370 loss: 0.0000000458 time: 0.39339375495910645\n",
      "Iteration: 7380 loss: 0.0000000453 time: 0.39853954315185547\n",
      "Iteration: 7390 loss: 0.0000000448 time: 0.40822410583496094\n",
      "Iteration: 7400 loss: 0.0000000444 time: 0.4044315814971924\n",
      "Iteration: 7410 loss: 0.0000000439 time: 0.39701199531555176\n",
      "Iteration: 7420 loss: 0.0000000434 time: 0.40706610679626465\n",
      "Iteration: 7430 loss: 0.0000000429 time: 0.4017186164855957\n",
      "Iteration: 7440 loss: 0.0000000425 time: 0.40642833709716797\n",
      "Iteration: 7450 loss: 0.0000000420 time: 0.4207437038421631\n",
      "Iteration: 7460 loss: 0.0000000416 time: 0.42368030548095703\n",
      "Iteration: 7470 loss: 0.0000000412 time: 0.40874481201171875\n",
      "Iteration: 7480 loss: 0.0000000407 time: 0.4151804447174072\n",
      "Iteration: 7490 loss: 0.0000000403 time: 0.4128897190093994\n",
      "Iteration: 7500 loss: 0.0000000399 time: 0.4141988754272461\n",
      "Iteration: 7510 loss: 0.0000000395 time: 0.41356396675109863\n",
      "Iteration: 7520 loss: 0.0000000391 time: 0.40697193145751953\n",
      "Iteration: 7530 loss: 0.0000000387 time: 0.41820621490478516\n",
      "Iteration: 7540 loss: 0.0000000383 time: 0.40949487686157227\n",
      "Iteration: 7550 loss: 0.0000000379 time: 0.4153313636779785\n",
      "Iteration: 7560 loss: 0.0000000375 time: 0.4163084030151367\n",
      "Iteration: 7570 loss: 0.0000000371 time: 0.3920478820800781\n",
      "Iteration: 7580 loss: 0.0000000367 time: 0.4070115089416504\n",
      "Iteration: 7590 loss: 0.0000000364 time: 0.3992953300476074\n",
      "Iteration: 7600 loss: 0.0000000360 time: 0.4071667194366455\n",
      "Iteration: 7610 loss: 0.0000000356 time: 0.4215574264526367\n",
      "Iteration: 7620 loss: 0.0000000353 time: 0.4002232551574707\n",
      "Iteration: 7630 loss: 0.0000000349 time: 0.3943936824798584\n",
      "Iteration: 7640 loss: 0.0000000346 time: 0.41358327865600586\n",
      "Iteration: 7650 loss: 0.0000000342 time: 0.40065956115722656\n",
      "Iteration: 7660 loss: 0.0000000339 time: 0.4254953861236572\n",
      "Iteration: 7670 loss: 0.0000000335 time: 0.40462708473205566\n",
      "Iteration: 7680 loss: 0.0000000332 time: 0.4150879383087158\n",
      "Iteration: 7690 loss: 0.0000000329 time: 0.40883564949035645\n",
      "Iteration: 7700 loss: 0.0000000326 time: 0.40275096893310547\n",
      "Iteration: 7710 loss: 0.0000000322 time: 0.41218042373657227\n",
      "Iteration: 7720 loss: 0.0000000319 time: 0.4147038459777832\n",
      "Iteration: 7730 loss: 0.0000000316 time: 0.4013540744781494\n",
      "Iteration: 7740 loss: 0.0000000313 time: 0.42185091972351074\n",
      "Iteration: 7750 loss: 0.0000000310 time: 0.40821266174316406\n",
      "Iteration: 7760 loss: 0.0000000307 time: 0.4137437343597412\n",
      "Iteration: 7770 loss: 0.0000000304 time: 0.4289522171020508\n",
      "Iteration: 7780 loss: 0.0000000301 time: 0.4084339141845703\n",
      "Iteration: 7790 loss: 0.0000000298 time: 0.40247225761413574\n",
      "Iteration: 7800 loss: 0.0000000295 time: 0.40792226791381836\n",
      "Iteration: 7810 loss: 0.0000000292 time: 0.406174898147583\n",
      "Iteration: 7820 loss: 0.0000000289 time: 0.4051957130432129\n",
      "Iteration: 7830 loss: 0.0000000287 time: 0.42480921745300293\n",
      "Iteration: 7840 loss: 0.0000000284 time: 0.4131584167480469\n",
      "Iteration: 7850 loss: 0.0000000281 time: 0.4026927947998047\n",
      "Iteration: 7860 loss: 0.0000000278 time: 0.40276193618774414\n",
      "Iteration: 7870 loss: 0.0000000276 time: 0.40088462829589844\n",
      "Iteration: 7880 loss: 0.0000000273 time: 0.42421698570251465\n",
      "Iteration: 7890 loss: 0.0000000270 time: 0.39939236640930176\n",
      "Iteration: 7900 loss: 0.0000000268 time: 0.408355712890625\n",
      "Iteration: 7910 loss: 0.0000000265 time: 0.39254140853881836\n",
      "Iteration: 7920 loss: 0.0000000263 time: 0.40816426277160645\n",
      "Iteration: 7930 loss: 0.0000000260 time: 0.4062535762786865\n",
      "Iteration: 7940 loss: 0.0000000258 time: 0.41351747512817383\n",
      "Iteration: 7950 loss: 0.0000000255 time: 0.40166258811950684\n",
      "Iteration: 7960 loss: 0.0000000253 time: 0.40874600410461426\n",
      "Iteration: 7970 loss: 0.0000000251 time: 0.4073359966278076\n",
      "Iteration: 7980 loss: 0.0000000248 time: 0.40811634063720703\n",
      "Iteration: 7990 loss: 0.0000000246 time: 0.40434837341308594\n",
      "Iteration: 8000 loss: 0.0000000244 time: 0.4106178283691406\n",
      "Iteration: 8010 loss: 0.0000000241 time: 0.4052145481109619\n",
      "Iteration: 8020 loss: 0.0000000239 time: 0.4051990509033203\n",
      "Iteration: 8030 loss: 0.0000000237 time: 0.40468668937683105\n",
      "Iteration: 8040 loss: 0.0000000235 time: 0.4155864715576172\n",
      "Iteration: 8050 loss: 0.0000000232 time: 0.3978462219238281\n",
      "Iteration: 8060 loss: 0.0000000230 time: 0.4167459011077881\n",
      "Iteration: 8070 loss: 0.0000000228 time: 0.4078991413116455\n",
      "Iteration: 8080 loss: 0.0000000226 time: 0.3967595100402832\n",
      "Iteration: 8090 loss: 0.0000000224 time: 0.4080021381378174\n",
      "Iteration: 8100 loss: 0.0000000222 time: 0.4128849506378174\n",
      "Iteration: 8110 loss: 0.0000000220 time: 0.4032766819000244\n",
      "Iteration: 8120 loss: 0.0000000218 time: 0.40860438346862793\n",
      "Iteration: 8130 loss: 0.0000000216 time: 0.41322994232177734\n",
      "Iteration: 8140 loss: 0.0000000214 time: 0.4120972156524658\n",
      "Iteration: 8150 loss: 0.0000000212 time: 0.4073469638824463\n",
      "Iteration: 8160 loss: 0.0000000210 time: 0.4008455276489258\n",
      "Iteration: 8170 loss: 0.0000000208 time: 0.4025890827178955\n",
      "Iteration: 8180 loss: 0.0000000206 time: 0.39946532249450684\n",
      "Iteration: 8190 loss: 0.0000000204 time: 0.4125523567199707\n",
      "Iteration: 8200 loss: 0.0000000202 time: 0.396226167678833\n",
      "Iteration: 8210 loss: 0.0000000200 time: 0.41547131538391113\n",
      "Iteration: 8220 loss: 0.0000000198 time: 0.4171881675720215\n",
      "Iteration: 8230 loss: 0.0000000196 time: 0.4112129211425781\n",
      "Iteration: 8240 loss: 0.0000000195 time: 0.4017460346221924\n",
      "Iteration: 8250 loss: 0.0000000193 time: 0.39913105964660645\n",
      "Iteration: 8260 loss: 0.0000000191 time: 0.41327762603759766\n",
      "Iteration: 8270 loss: 0.0000000189 time: 0.40639686584472656\n",
      "Iteration: 8280 loss: 0.0000000187 time: 0.4019203186035156\n",
      "Iteration: 8290 loss: 0.0000000186 time: 0.3972666263580322\n",
      "Iteration: 8300 loss: 0.0000000184 time: 0.41495656967163086\n",
      "Iteration: 8310 loss: 0.0000000182 time: 0.4079289436340332\n",
      "Iteration: 8320 loss: 0.0000000181 time: 0.40956592559814453\n",
      "Iteration: 8330 loss: 0.0000000179 time: 0.3967702388763428\n",
      "Iteration: 8340 loss: 0.0000000177 time: 0.39757251739501953\n",
      "Iteration: 8350 loss: 0.0000000176 time: 0.41037440299987793\n",
      "Iteration: 8360 loss: 0.0000000174 time: 0.4042806625366211\n",
      "Iteration: 8370 loss: 0.0000000173 time: 0.4252920150756836\n",
      "Iteration: 8380 loss: 0.0000000171 time: 0.4009883403778076\n",
      "Iteration: 8390 loss: 0.0000000169 time: 0.40553736686706543\n",
      "Iteration: 8400 loss: 0.0000000168 time: 0.41763997077941895\n",
      "Iteration: 8410 loss: 0.0000000166 time: 0.4096100330352783\n",
      "Iteration: 8420 loss: 0.0000000165 time: 0.41169023513793945\n",
      "Iteration: 8430 loss: 0.0000000163 time: 0.4008820056915283\n",
      "Iteration: 8440 loss: 0.0000000162 time: 0.41184258460998535\n",
      "Iteration: 8450 loss: 0.0000000160 time: 0.4080851078033447\n",
      "Iteration: 8460 loss: 0.0000000159 time: 0.408221960067749\n",
      "Iteration: 8470 loss: 0.0000000158 time: 0.4076507091522217\n",
      "Iteration: 8480 loss: 0.0000000156 time: 0.4086313247680664\n",
      "Iteration: 8490 loss: 0.0000000155 time: 0.41204071044921875\n",
      "Iteration: 8500 loss: 0.0000000153 time: 0.39247751235961914\n",
      "Iteration: 8510 loss: 0.0000000152 time: 0.4068794250488281\n",
      "Iteration: 8520 loss: 0.0000000150 time: 0.4091935157775879\n",
      "Iteration: 8530 loss: 0.0000000149 time: 0.4154822826385498\n",
      "Iteration: 8540 loss: 0.0000000148 time: 0.4103527069091797\n",
      "Iteration: 8550 loss: 0.0000000146 time: 0.4030475616455078\n",
      "Iteration: 8560 loss: 0.0000000145 time: 0.4118518829345703\n",
      "Iteration: 8570 loss: 0.0000000144 time: 0.4067113399505615\n",
      "Iteration: 8580 loss: 0.0000000143 time: 0.4171171188354492\n",
      "Iteration: 8590 loss: 0.0000000141 time: 0.4092276096343994\n",
      "Iteration: 8600 loss: 0.0000000140 time: 0.41216588020324707\n",
      "Iteration: 8610 loss: 0.0000000139 time: 0.4149022102355957\n",
      "Iteration: 8620 loss: 0.0000000137 time: 0.41490983963012695\n",
      "Iteration: 8630 loss: 0.0000000136 time: 0.41182518005371094\n",
      "Iteration: 8640 loss: 0.0000000135 time: 0.4107201099395752\n",
      "Iteration: 8650 loss: 0.0000000134 time: 0.4119260311126709\n",
      "Iteration: 8660 loss: 0.0000000133 time: 0.41123390197753906\n",
      "Iteration: 8670 loss: 0.0000000131 time: 0.398040771484375\n",
      "Iteration: 8680 loss: 0.0000000130 time: 0.4106760025024414\n",
      "Iteration: 8690 loss: 0.0000000129 time: 0.4167652130126953\n",
      "Iteration: 8700 loss: 0.0000000128 time: 0.40264463424682617\n",
      "Iteration: 8710 loss: 0.0000000127 time: 0.4030134677886963\n",
      "Iteration: 8720 loss: 0.0000000126 time: 0.4038965702056885\n",
      "Iteration: 8730 loss: 0.0000000124 time: 0.39328432083129883\n",
      "Iteration: 8740 loss: 0.0000000123 time: 0.4086945056915283\n",
      "Iteration: 8750 loss: 0.0000000122 time: 0.40114474296569824\n",
      "Iteration: 8760 loss: 0.0000000121 time: 0.41350603103637695\n",
      "Iteration: 8770 loss: 0.0000000120 time: 0.40752339363098145\n",
      "Iteration: 8780 loss: 0.0000000119 time: 0.4051530361175537\n",
      "Iteration: 8790 loss: 0.0000000118 time: 0.41147494316101074\n",
      "Iteration: 8800 loss: 0.0000000117 time: 0.4150662422180176\n",
      "Iteration: 8810 loss: 0.0000000116 time: 0.40512585639953613\n",
      "Iteration: 8820 loss: 0.0000000115 time: 0.40613484382629395\n",
      "Iteration: 8830 loss: 0.0000000114 time: 0.40668320655822754\n",
      "Iteration: 8840 loss: 0.0000000113 time: 0.3989396095275879\n",
      "Iteration: 8850 loss: 0.0000000112 time: 0.3962113857269287\n",
      "Iteration: 8860 loss: 0.0000000111 time: 0.39931726455688477\n",
      "Iteration: 8870 loss: 0.0000000110 time: 0.41193604469299316\n",
      "Iteration: 8880 loss: 0.0000000109 time: 0.3998105525970459\n",
      "Iteration: 8890 loss: 0.0000000108 time: 0.41082334518432617\n",
      "Iteration: 8900 loss: 0.0000000107 time: 0.41315627098083496\n",
      "Iteration: 8910 loss: 0.0000000106 time: 0.4219329357147217\n",
      "Iteration: 8920 loss: 0.0000000105 time: 0.4141685962677002\n",
      "Iteration: 8930 loss: 0.0000000104 time: 0.3980379104614258\n",
      "Iteration: 8940 loss: 0.0000000103 time: 0.41115760803222656\n",
      "Iteration: 8950 loss: 0.0000000102 time: 0.4065408706665039\n",
      "Iteration: 8960 loss: 0.0000000101 time: 0.4012007713317871\n",
      "Iteration: 8970 loss: 0.0000000100 time: 0.4063422679901123\n",
      "Iteration: 8980 loss: 0.0000000100 time: 0.39655518531799316\n",
      "Iteration: 8990 loss: 0.0000000099 time: 0.39656662940979004\n",
      "Iteration: 9000 loss: 0.0000000098 time: 0.4128749370574951\n",
      "Iteration: 9010 loss: 0.0000000097 time: 0.42240095138549805\n",
      "Iteration: 9020 loss: 0.0000000096 time: 0.4101901054382324\n",
      "Iteration: 9030 loss: 0.0000000095 time: 0.41281843185424805\n",
      "Iteration: 9040 loss: 0.0000000094 time: 0.3992583751678467\n",
      "Iteration: 9050 loss: 0.0000000094 time: 0.406923770904541\n",
      "Iteration: 9060 loss: 0.0000000093 time: 0.4187805652618408\n",
      "Iteration: 9070 loss: 0.0000000092 time: 0.3807387351989746\n",
      "Iteration: 9080 loss: 0.0000000091 time: 0.3843364715576172\n",
      "Iteration: 9090 loss: 0.0000000090 time: 0.37749409675598145\n",
      "Iteration: 9100 loss: 0.0000000089 time: 0.3814816474914551\n",
      "Iteration: 9110 loss: 0.0000000089 time: 0.3923678398132324\n",
      "Iteration: 9120 loss: 0.0000000088 time: 0.39988040924072266\n",
      "Iteration: 9130 loss: 0.0000000087 time: 0.40278077125549316\n",
      "Iteration: 9140 loss: 0.0000000086 time: 0.3889787197113037\n",
      "Iteration: 9150 loss: 0.0000000086 time: 0.3906891345977783\n",
      "Iteration: 9160 loss: 0.0000000085 time: 0.3996870517730713\n",
      "Iteration: 9170 loss: 0.0000000084 time: 0.38617753982543945\n",
      "Iteration: 9180 loss: 0.0000000083 time: 0.4088571071624756\n",
      "Iteration: 9190 loss: 0.0000000083 time: 0.41545557975769043\n",
      "Iteration: 9200 loss: 0.0000000082 time: 0.385861873626709\n",
      "Iteration: 9210 loss: 0.0000000081 time: 0.3899378776550293\n",
      "Iteration: 9220 loss: 0.0000000080 time: 0.39623165130615234\n",
      "Iteration: 9230 loss: 0.0000000080 time: 0.3888998031616211\n",
      "Iteration: 9240 loss: 0.0000000079 time: 0.4087510108947754\n",
      "Iteration: 9250 loss: 0.0000000078 time: 0.39014124870300293\n",
      "Iteration: 9260 loss: 0.0000000078 time: 0.3858189582824707\n",
      "Iteration: 9270 loss: 0.0000000077 time: 0.3864452838897705\n",
      "Iteration: 9280 loss: 0.0000000076 time: 0.3858044147491455\n",
      "Iteration: 9290 loss: 0.0000000076 time: 0.4029836654663086\n",
      "Iteration: 9300 loss: 0.0000000075 time: 0.43591904640197754\n",
      "Iteration: 9310 loss: 0.0000000074 time: 0.3982408046722412\n",
      "Iteration: 9320 loss: 0.0000000074 time: 0.39806270599365234\n",
      "Iteration: 9330 loss: 0.0000000073 time: 0.3810451030731201\n",
      "Iteration: 9340 loss: 0.0000000072 time: 0.39350366592407227\n",
      "Iteration: 9350 loss: 0.0000000072 time: 0.40953612327575684\n",
      "Iteration: 9360 loss: 0.0000000071 time: 0.42203831672668457\n",
      "Iteration: 9370 loss: 0.0000000071 time: 0.4108743667602539\n",
      "Iteration: 9380 loss: 0.0000000070 time: 0.3934347629547119\n",
      "Iteration: 9390 loss: 0.0000000069 time: 0.40841221809387207\n",
      "Iteration: 9400 loss: 0.0000000069 time: 0.423811674118042\n",
      "Iteration: 9410 loss: 0.0000000068 time: 0.4172999858856201\n",
      "Iteration: 9420 loss: 0.0000000068 time: 0.3970527648925781\n",
      "Iteration: 9430 loss: 0.0000000067 time: 0.4159202575683594\n",
      "Iteration: 9440 loss: 0.0000000066 time: 0.40764331817626953\n",
      "Iteration: 9450 loss: 0.0000000066 time: 0.40224671363830566\n",
      "Iteration: 9460 loss: 0.0000000065 time: 0.42768192291259766\n",
      "Iteration: 9470 loss: 0.0000000065 time: 0.4043447971343994\n",
      "Iteration: 9480 loss: 0.0000000064 time: 0.405686616897583\n",
      "Iteration: 9490 loss: 0.0000000064 time: 0.40076518058776855\n",
      "Iteration: 9500 loss: 0.0000000063 time: 0.4045226573944092\n",
      "Iteration: 9510 loss: 0.0000000063 time: 0.4172983169555664\n",
      "Iteration: 9520 loss: 0.0000000062 time: 0.40292835235595703\n",
      "Iteration: 9530 loss: 0.0000000061 time: 0.4010603427886963\n",
      "Iteration: 9540 loss: 0.0000000061 time: 0.40390586853027344\n",
      "Iteration: 9550 loss: 0.0000000060 time: 0.4060399532318115\n",
      "Iteration: 9560 loss: 0.0000000060 time: 0.397305965423584\n",
      "Iteration: 9570 loss: 0.0000000059 time: 0.40885066986083984\n",
      "Iteration: 9580 loss: 0.0000000059 time: 0.4055969715118408\n",
      "Iteration: 9590 loss: 0.0000000058 time: 0.4074742794036865\n",
      "Iteration: 9600 loss: 0.0000000058 time: 0.41097044944763184\n",
      "Iteration: 9610 loss: 0.0000000057 time: 0.4200427532196045\n",
      "Iteration: 9620 loss: 0.0000000057 time: 0.42308640480041504\n",
      "Iteration: 9630 loss: 0.0000000056 time: 0.4082949161529541\n",
      "Iteration: 9640 loss: 0.0000000056 time: 0.411663293838501\n",
      "Iteration: 9650 loss: 0.0000000056 time: 0.4040234088897705\n",
      "Iteration: 9660 loss: 0.0000000055 time: 0.40073347091674805\n",
      "Iteration: 9670 loss: 0.0000000055 time: 0.4076375961303711\n",
      "Iteration: 9680 loss: 0.0000000054 time: 0.4141545295715332\n",
      "Iteration: 9690 loss: 0.0000000054 time: 0.42167186737060547\n",
      "Iteration: 9700 loss: 0.0000000053 time: 0.41279125213623047\n",
      "Iteration: 9710 loss: 0.0000000053 time: 0.40987372398376465\n",
      "Iteration: 9720 loss: 0.0000000052 time: 0.4147782325744629\n",
      "Iteration: 9730 loss: 0.0000000052 time: 0.41629528999328613\n",
      "Iteration: 9740 loss: 0.0000000052 time: 0.41817402839660645\n",
      "Iteration: 9750 loss: 0.0000000051 time: 0.409515380859375\n",
      "Iteration: 9760 loss: 0.0000000051 time: 0.40540313720703125\n",
      "Iteration: 9770 loss: 0.0000000050 time: 0.4056391716003418\n",
      "Iteration: 9780 loss: 0.0000000050 time: 0.42101073265075684\n",
      "Iteration: 9790 loss: 0.0000000050 time: 0.4092276096343994\n",
      "Iteration: 9800 loss: 0.0000000049 time: 0.4019615650177002\n",
      "Iteration: 9810 loss: 0.0000000049 time: 0.3997495174407959\n",
      "Iteration: 9820 loss: 0.0000000048 time: 0.3841128349304199\n",
      "Iteration: 9830 loss: 0.0000000048 time: 0.4092133045196533\n",
      "Iteration: 9840 loss: 0.0000000048 time: 0.417067289352417\n",
      "Iteration: 9850 loss: 0.0000000047 time: 0.3751697540283203\n",
      "Iteration: 9860 loss: 0.0000000048 time: 0.356417179107666\n",
      "Iteration: 9870 loss: 0.0000000047 time: 0.366959810256958\n",
      "Iteration: 9880 loss: 0.0000000046 time: 0.3571758270263672\n",
      "Iteration: 9890 loss: 0.0000000046 time: 0.3840169906616211\n",
      "Iteration: 9900 loss: 0.0000000046 time: 0.3966217041015625\n",
      "Iteration: 9910 loss: 0.0000000045 time: 0.3877127170562744\n",
      "Iteration: 9920 loss: 0.0000000045 time: 0.3897078037261963\n",
      "Iteration: 9930 loss: 0.0000000045 time: 0.4092428684234619\n",
      "Iteration: 9940 loss: 0.0000000044 time: 0.3980538845062256\n",
      "Iteration: 9950 loss: 0.0000000044 time: 0.40401387214660645\n",
      "Iteration: 9960 loss: 0.0000000044 time: 0.40123820304870605\n",
      "Iteration: 9970 loss: 0.0000000043 time: 0.3877389430999756\n",
      "Iteration: 9980 loss: 0.0000000043 time: 0.3946225643157959\n",
      "Iteration: 9990 loss: 0.0000000043 time: 0.40639257431030273\n",
      "Iteration: 10000 loss: 0.0000000042 time: 0.3334660530090332\n",
      "Iteration: 10010 loss: 0.0000000042 time: 0.39455199241638184\n",
      "Iteration: 10020 loss: 0.0000000042 time: 0.39717793464660645\n",
      "Iteration: 10030 loss: 0.0000000042 time: 0.39356064796447754\n",
      "Iteration: 10040 loss: 0.0000000041 time: 0.3773026466369629\n",
      "Iteration: 10050 loss: 0.0000000041 time: 0.39253973960876465\n",
      "Iteration: 10060 loss: 0.0000000041 time: 0.40389132499694824\n",
      "Iteration: 10070 loss: 0.0000000041 time: 0.40429139137268066\n",
      "Iteration: 10080 loss: 0.0000000040 time: 0.3922085762023926\n",
      "Iteration: 10090 loss: 0.0000000040 time: 0.39612889289855957\n",
      "Iteration: 10100 loss: 0.0000000040 time: 0.3893709182739258\n",
      "Iteration: 10110 loss: 0.0000000039 time: 0.40539050102233887\n",
      "Iteration: 10120 loss: 0.0000000039 time: 0.4084315299987793\n",
      "Iteration: 10130 loss: 0.0000000039 time: 0.3954150676727295\n",
      "Iteration: 10140 loss: 0.0000000039 time: 0.398390531539917\n",
      "Iteration: 10150 loss: 0.0000000038 time: 0.3916919231414795\n",
      "Iteration: 10160 loss: 0.0000000038 time: 0.39603328704833984\n",
      "Iteration: 10170 loss: 0.0000000038 time: 0.40408921241760254\n",
      "Iteration: 10180 loss: 0.0000000038 time: 0.4051179885864258\n",
      "Iteration: 10190 loss: 0.0000000037 time: 0.38608741760253906\n",
      "Iteration: 10200 loss: 0.0000000037 time: 0.3906407356262207\n",
      "Iteration: 10210 loss: 0.0000000037 time: 0.4041879177093506\n",
      "Iteration: 10220 loss: 0.0000000045 time: 0.40193700790405273\n",
      "Iteration: 10230 loss: 0.0000000040 time: 0.41914892196655273\n",
      "Iteration: 10240 loss: 0.0000000037 time: 0.40564513206481934\n",
      "Iteration: 10250 loss: 0.0000000036 time: 0.3943655490875244\n",
      "Iteration: 10260 loss: 0.0000000036 time: 0.3679978847503662\n",
      "Iteration: 10270 loss: 0.0000000036 time: 0.3718116283416748\n",
      "Iteration: 10280 loss: 0.0000000035 time: 0.36409568786621094\n",
      "Iteration: 10290 loss: 0.0000000035 time: 0.3634212017059326\n",
      "Iteration: 10300 loss: 0.0000000035 time: 0.35648226737976074\n",
      "Iteration: 10310 loss: 0.0000000035 time: 0.3453695774078369\n",
      "Iteration: 10320 loss: 0.0000000035 time: 0.3303053379058838\n",
      "Iteration: 10330 loss: 0.0000000034 time: 0.33693456649780273\n",
      "Iteration: 10340 loss: 0.0000000034 time: 0.3464939594268799\n",
      "Iteration: 10350 loss: 0.0000000034 time: 0.3507046699523926\n",
      "Iteration: 10360 loss: 0.0000000034 time: 0.3419065475463867\n",
      "Iteration: 10370 loss: 0.0000000034 time: 0.3614530563354492\n",
      "Iteration: 10380 loss: 0.0000000033 time: 0.35923004150390625\n",
      "Iteration: 10390 loss: 0.0000000033 time: 0.3694486618041992\n",
      "Iteration: 10400 loss: 0.0000000033 time: 0.3626432418823242\n",
      "Iteration: 10410 loss: 0.0000000033 time: 0.37891364097595215\n",
      "Iteration: 10420 loss: 0.0000000033 time: 0.3717379570007324\n",
      "Iteration: 10430 loss: 0.0000000032 time: 0.3658304214477539\n",
      "Iteration: 10440 loss: 0.0000000032 time: 0.36498212814331055\n",
      "Iteration: 10450 loss: 0.0000000032 time: 0.37900876998901367\n",
      "Iteration: 10460 loss: 0.0000000032 time: 0.3910069465637207\n",
      "Iteration: 10470 loss: 0.0000000032 time: 0.39261746406555176\n",
      "Iteration: 10480 loss: 0.0000000032 time: 0.37657713890075684\n",
      "Iteration: 10490 loss: 0.0000000031 time: 0.3741145133972168\n",
      "Iteration: 10500 loss: 0.0000000031 time: 0.3657979965209961\n",
      "Iteration: 10510 loss: 0.0000000031 time: 0.37533044815063477\n",
      "Iteration: 10520 loss: 0.0000000031 time: 0.37401914596557617\n",
      "Iteration: 10530 loss: 0.0000000031 time: 0.4011657238006592\n",
      "Iteration: 10540 loss: 0.0000000031 time: 0.36304283142089844\n",
      "Iteration: 10550 loss: 0.0000000030 time: 0.37157464027404785\n",
      "Iteration: 10560 loss: 0.0000000030 time: 0.37532854080200195\n",
      "Iteration: 10570 loss: 0.0000000035 time: 0.38893938064575195\n",
      "Iteration: 10580 loss: 0.0000000040 time: 0.3792719841003418\n",
      "Iteration: 10590 loss: 0.0000000033 time: 0.3941500186920166\n",
      "Iteration: 10600 loss: 0.0000000030 time: 0.38498997688293457\n",
      "Iteration: 10610 loss: 0.0000000030 time: 0.3694279193878174\n",
      "Iteration: 10620 loss: 0.0000000029 time: 0.36288952827453613\n",
      "Iteration: 10630 loss: 0.0000000029 time: 0.3855445384979248\n",
      "Iteration: 10640 loss: 0.0000000029 time: 0.3742189407348633\n",
      "Iteration: 10650 loss: 0.0000000029 time: 0.39598536491394043\n",
      "Iteration: 10660 loss: 0.0000000029 time: 0.3844730854034424\n",
      "Iteration: 10670 loss: 0.0000000029 time: 0.38179779052734375\n",
      "Iteration: 10680 loss: 0.0000000029 time: 0.368208646774292\n",
      "Iteration: 10690 loss: 0.0000000028 time: 0.378420352935791\n",
      "Iteration: 10700 loss: 0.0000000028 time: 0.3957211971282959\n",
      "Iteration: 10710 loss: 0.0000000028 time: 0.37212371826171875\n",
      "Iteration: 10720 loss: 0.0000000028 time: 0.37233495712280273\n",
      "Iteration: 10730 loss: 0.0000000028 time: 0.3794081211090088\n",
      "Iteration: 10740 loss: 0.0000000028 time: 0.38051533699035645\n",
      "Iteration: 10750 loss: 0.0000000028 time: 0.39078330993652344\n",
      "Iteration: 10760 loss: 0.0000000027 time: 0.40511298179626465\n",
      "Iteration: 10770 loss: 0.0000000027 time: 0.3793609142303467\n",
      "Iteration: 10780 loss: 0.0000000027 time: 0.39174628257751465\n",
      "Iteration: 10790 loss: 0.0000000027 time: 0.3817570209503174\n",
      "Iteration: 10800 loss: 0.0000000027 time: 0.38213109970092773\n",
      "Iteration: 10810 loss: 0.0000000027 time: 0.3751027584075928\n",
      "Iteration: 10820 loss: 0.0000000027 time: 0.3896951675415039\n",
      "Iteration: 10830 loss: 0.0000000028 time: 0.38939642906188965\n",
      "Iteration: 10840 loss: 0.0000000027 time: 0.38620448112487793\n",
      "Iteration: 10850 loss: 0.0000000026 time: 0.38985419273376465\n",
      "Iteration: 10860 loss: 0.0000000026 time: 0.3749532699584961\n",
      "Iteration: 10870 loss: 0.0000000026 time: 0.405289888381958\n",
      "Iteration: 10880 loss: 0.0000000026 time: 0.394284725189209\n",
      "Iteration: 10890 loss: 0.0000000026 time: 0.38822460174560547\n",
      "Iteration: 10900 loss: 0.0000000026 time: 0.39015674591064453\n",
      "Iteration: 10910 loss: 0.0000000026 time: 0.38556861877441406\n",
      "Iteration: 10920 loss: 0.0000000026 time: 0.3887193202972412\n",
      "Iteration: 10930 loss: 0.0000000026 time: 0.4139091968536377\n",
      "Iteration: 10940 loss: 0.0000000035 time: 0.38414835929870605\n",
      "Iteration: 10950 loss: 0.0000000029 time: 0.3775484561920166\n",
      "Iteration: 10960 loss: 0.0000000025 time: 0.38496923446655273\n",
      "Iteration: 10970 loss: 0.0000000025 time: 0.38237643241882324\n",
      "Iteration: 10980 loss: 0.0000000025 time: 0.39586758613586426\n",
      "Iteration: 10990 loss: 0.0000000025 time: 0.3969871997833252\n",
      "Iteration: 11000 loss: 0.0000000025 time: 0.3940279483795166\n",
      "Iteration: 11010 loss: 0.0000000025 time: 0.3829622268676758\n",
      "Iteration: 11020 loss: 0.0000000025 time: 0.38363051414489746\n",
      "Iteration: 11030 loss: 0.0000000024 time: 0.40936279296875\n",
      "Iteration: 11040 loss: 0.0000000024 time: 0.4201991558074951\n",
      "Iteration: 11050 loss: 0.0000000024 time: 0.4096872806549072\n",
      "Iteration: 11060 loss: 0.0000000024 time: 0.41553473472595215\n",
      "Iteration: 11070 loss: 0.0000000024 time: 0.3948493003845215\n",
      "Iteration: 11080 loss: 0.0000000024 time: 0.3842329978942871\n",
      "Iteration: 11090 loss: 0.0000000024 time: 0.40274739265441895\n",
      "Iteration: 11100 loss: 0.0000000024 time: 0.3683891296386719\n",
      "Iteration: 11110 loss: 0.0000000024 time: 0.3810446262359619\n",
      "Iteration: 11120 loss: 0.0000000024 time: 0.39226603507995605\n",
      "Iteration: 11130 loss: 0.0000000036 time: 0.3757345676422119\n",
      "Iteration: 11140 loss: 0.0000000024 time: 0.35968923568725586\n",
      "Iteration: 11150 loss: 0.0000000024 time: 0.3721952438354492\n",
      "Iteration: 11160 loss: 0.0000000024 time: 0.3744235038757324\n",
      "Iteration: 11170 loss: 0.0000000023 time: 0.41301846504211426\n",
      "Iteration: 11180 loss: 0.0000000023 time: 0.437938928604126\n",
      "Iteration: 11190 loss: 0.0000000023 time: 0.42075133323669434\n",
      "Iteration: 11200 loss: 0.0000000023 time: 0.43093371391296387\n",
      "Iteration: 11210 loss: 0.0000000023 time: 0.457735538482666\n",
      "Iteration: 11220 loss: 0.0000000023 time: 0.42271852493286133\n",
      "Iteration: 11230 loss: 0.0000000023 time: 0.40962815284729004\n",
      "Iteration: 11240 loss: 0.0000000023 time: 0.3852112293243408\n",
      "Iteration: 11250 loss: 0.0000000022 time: 0.39075326919555664\n",
      "Iteration: 11260 loss: 0.0000000022 time: 0.39806675910949707\n",
      "Iteration: 11270 loss: 0.0000000026 time: 0.4129505157470703\n",
      "Iteration: 11280 loss: 0.0000000023 time: 0.4177095890045166\n",
      "Iteration: 11290 loss: 0.0000000023 time: 0.40070152282714844\n",
      "Iteration: 11300 loss: 0.0000000022 time: 0.41395998001098633\n",
      "Iteration: 11310 loss: 0.0000000022 time: 0.3929121494293213\n",
      "Iteration: 11320 loss: 0.0000000022 time: 0.4212315082550049\n",
      "Iteration: 11330 loss: 0.0000000022 time: 0.3909749984741211\n",
      "Iteration: 11340 loss: 0.0000000022 time: 0.38387608528137207\n",
      "Iteration: 11350 loss: 0.0000000022 time: 0.3848872184753418\n",
      "Iteration: 11360 loss: 0.0000000022 time: 0.38213348388671875\n",
      "Iteration: 11370 loss: 0.0000000022 time: 0.3748488426208496\n",
      "Iteration: 11380 loss: 0.0000000021 time: 0.39139676094055176\n",
      "Iteration: 11390 loss: 0.0000000021 time: 0.3823423385620117\n",
      "Iteration: 11400 loss: 0.0000000021 time: 0.38211870193481445\n",
      "Iteration: 11410 loss: 0.0000000021 time: 0.3884260654449463\n",
      "Iteration: 11420 loss: 0.0000000021 time: 0.38642048835754395\n",
      "Iteration: 11430 loss: 0.0000000027 time: 0.39141082763671875\n",
      "Iteration: 11440 loss: 0.0000000023 time: 0.3846104145050049\n",
      "Iteration: 11450 loss: 0.0000000023 time: 0.39961862564086914\n",
      "Iteration: 11460 loss: 0.0000000021 time: 0.37418150901794434\n",
      "Iteration: 11470 loss: 0.0000000021 time: 0.37389111518859863\n",
      "Iteration: 11480 loss: 0.0000000021 time: 0.38405394554138184\n",
      "Iteration: 11490 loss: 0.0000000021 time: 0.39620256423950195\n",
      "Iteration: 11500 loss: 0.0000000021 time: 0.3894960880279541\n",
      "Iteration: 11510 loss: 0.0000000021 time: 0.3868422508239746\n",
      "Iteration: 11520 loss: 0.0000000020 time: 0.37644386291503906\n",
      "Iteration: 11530 loss: 0.0000000020 time: 0.3742365837097168\n",
      "Iteration: 11540 loss: 0.0000000020 time: 0.38303375244140625\n",
      "Iteration: 11550 loss: 0.0000000020 time: 0.38512420654296875\n",
      "Iteration: 11560 loss: 0.0000000020 time: 0.38350653648376465\n",
      "Iteration: 11570 loss: 0.0000000029 time: 0.3818809986114502\n",
      "Iteration: 11580 loss: 0.0000000022 time: 0.3895542621612549\n",
      "Iteration: 11590 loss: 0.0000000021 time: 0.3775777816772461\n",
      "Iteration: 11600 loss: 0.0000000020 time: 0.37494754791259766\n",
      "Iteration: 11610 loss: 0.0000000020 time: 0.37212681770324707\n",
      "Iteration: 11620 loss: 0.0000000020 time: 0.36876463890075684\n",
      "Iteration: 11630 loss: 0.0000000020 time: 0.364971399307251\n",
      "Iteration: 11640 loss: 0.0000000020 time: 0.36385512351989746\n",
      "Iteration: 11650 loss: 0.0000000020 time: 0.3785429000854492\n",
      "Iteration: 11660 loss: 0.0000000020 time: 0.37886762619018555\n",
      "Iteration: 11670 loss: 0.0000000020 time: 0.40662646293640137\n",
      "Iteration: 11680 loss: 0.0000000020 time: 0.42755889892578125\n",
      "Iteration: 11690 loss: 0.0000000020 time: 0.387068510055542\n",
      "Iteration: 11700 loss: 0.0000000033 time: 0.3477034568786621\n",
      "Iteration: 11710 loss: 0.0000000020 time: 0.38727712631225586\n",
      "Iteration: 11720 loss: 0.0000000021 time: 0.4092581272125244\n",
      "Iteration: 11730 loss: 0.0000000019 time: 0.38892078399658203\n",
      "Iteration: 11740 loss: 0.0000000019 time: 0.3750793933868408\n",
      "Iteration: 11750 loss: 0.0000000019 time: 0.3908686637878418\n",
      "Iteration: 11760 loss: 0.0000000019 time: 0.3866596221923828\n",
      "Iteration: 11770 loss: 0.0000000019 time: 0.3791656494140625\n",
      "Iteration: 11780 loss: 0.0000000019 time: 0.39734888076782227\n",
      "Iteration: 11790 loss: 0.0000000019 time: 0.36421871185302734\n",
      "Iteration: 11800 loss: 0.0000000019 time: 0.3772590160369873\n",
      "Iteration: 11810 loss: 0.0000000019 time: 0.3917405605316162\n",
      "Iteration: 11820 loss: 0.0000000019 time: 0.3808889389038086\n",
      "Iteration: 11830 loss: 0.0000000019 time: 0.38112497329711914\n",
      "Iteration: 11840 loss: 0.0000000019 time: 0.39760684967041016\n",
      "Iteration: 11850 loss: 0.0000000019 time: 0.37732863426208496\n",
      "Iteration: 11860 loss: 0.0000000019 time: 0.3887338638305664\n",
      "Iteration: 11870 loss: 0.0000000018 time: 0.3899219036102295\n",
      "Iteration: 11880 loss: 0.0000000019 time: 0.38114285469055176\n",
      "Iteration: 11890 loss: 0.0000000024 time: 0.37149977684020996\n",
      "Iteration: 11900 loss: 0.0000000020 time: 0.3846778869628906\n",
      "Iteration: 11910 loss: 0.0000000018 time: 0.3825395107269287\n",
      "Iteration: 11920 loss: 0.0000000018 time: 0.38439059257507324\n",
      "Iteration: 11930 loss: 0.0000000018 time: 0.38994669914245605\n",
      "Iteration: 11940 loss: 0.0000000018 time: 0.3852357864379883\n",
      "Iteration: 11950 loss: 0.0000000018 time: 0.3966963291168213\n",
      "Iteration: 11960 loss: 0.0000000018 time: 0.38654613494873047\n",
      "Iteration: 11970 loss: 0.0000000018 time: 0.37979793548583984\n",
      "Iteration: 11980 loss: 0.0000000019 time: 0.3766918182373047\n",
      "Iteration: 11990 loss: 0.0000000024 time: 0.36742448806762695\n",
      "Iteration: 12000 loss: 0.0000000018 time: 0.38048219680786133\n",
      "Iteration: 12010 loss: 0.0000000018 time: 0.39792490005493164\n",
      "Iteration: 12020 loss: 0.0000000018 time: 0.3887290954589844\n",
      "Iteration: 12030 loss: 0.0000000018 time: 0.39072275161743164\n",
      "Iteration: 12040 loss: 0.0000000018 time: 0.3869006633758545\n",
      "Iteration: 12050 loss: 0.0000000018 time: 0.3871493339538574\n",
      "Iteration: 12060 loss: 0.0000000018 time: 0.397139310836792\n",
      "Iteration: 12070 loss: 0.0000000018 time: 0.3897216320037842\n",
      "Iteration: 12080 loss: 0.0000000017 time: 0.37607407569885254\n",
      "Iteration: 12090 loss: 0.0000000017 time: 0.3835585117340088\n",
      "Iteration: 12100 loss: 0.0000000017 time: 0.3921980857849121\n",
      "Iteration: 12110 loss: 0.0000000017 time: 0.396007776260376\n",
      "Iteration: 12120 loss: 0.0000000017 time: 0.4048190116882324\n",
      "Iteration: 12130 loss: 0.0000000017 time: 0.39316225051879883\n",
      "Iteration: 12140 loss: 0.0000000017 time: 0.38044261932373047\n",
      "Iteration: 12150 loss: 0.0000000030 time: 0.38858628273010254\n",
      "Iteration: 12160 loss: 0.0000000023 time: 0.38276171684265137\n",
      "Iteration: 12170 loss: 0.0000000017 time: 0.37428784370422363\n",
      "Iteration: 12180 loss: 0.0000000018 time: 0.3997621536254883\n",
      "Iteration: 12190 loss: 0.0000000017 time: 0.3843846321105957\n",
      "Iteration: 12200 loss: 0.0000000017 time: 0.3815426826477051\n",
      "Iteration: 12210 loss: 0.0000000017 time: 0.3828728199005127\n",
      "Iteration: 12220 loss: 0.0000000017 time: 0.39418840408325195\n",
      "Iteration: 12230 loss: 0.0000000017 time: 0.3899500370025635\n",
      "Iteration: 12240 loss: 0.0000000017 time: 0.3996908664703369\n",
      "Iteration: 12250 loss: 0.0000000017 time: 0.389514684677124\n",
      "Iteration: 12260 loss: 0.0000000017 time: 0.3991100788116455\n",
      "Iteration: 12270 loss: 0.0000000017 time: 0.38968825340270996\n",
      "Iteration: 12280 loss: 0.0000000017 time: 0.39870762825012207\n",
      "Iteration: 12290 loss: 0.0000000017 time: 0.4074118137359619\n",
      "Iteration: 12300 loss: 0.0000000020 time: 0.39080262184143066\n",
      "Iteration: 12310 loss: 0.0000000019 time: 0.38691139221191406\n",
      "Iteration: 12320 loss: 0.0000000017 time: 0.38938069343566895\n",
      "Iteration: 12330 loss: 0.0000000017 time: 0.37529897689819336\n",
      "Iteration: 12340 loss: 0.0000000017 time: 0.3815934658050537\n",
      "Iteration: 12350 loss: 0.0000000016 time: 0.4074242115020752\n",
      "Iteration: 12360 loss: 0.0000000016 time: 0.39046239852905273\n",
      "Iteration: 12370 loss: 0.0000000016 time: 0.39195942878723145\n",
      "Iteration: 12380 loss: 0.0000000016 time: 0.39496588706970215\n",
      "Iteration: 12390 loss: 0.0000000016 time: 0.38875341415405273\n",
      "Iteration: 12400 loss: 0.0000000016 time: 0.4018900394439697\n",
      "Iteration: 12410 loss: 0.0000000016 time: 0.3860740661621094\n",
      "Iteration: 12420 loss: 0.0000000024 time: 0.4036738872528076\n",
      "Iteration: 12430 loss: 0.0000000019 time: 0.39241576194763184\n",
      "Iteration: 12440 loss: 0.0000000018 time: 0.4097323417663574\n",
      "Iteration: 12450 loss: 0.0000000016 time: 0.39678359031677246\n",
      "Iteration: 12460 loss: 0.0000000016 time: 0.4065971374511719\n",
      "Iteration: 12470 loss: 0.0000000016 time: 0.3952152729034424\n",
      "Iteration: 12480 loss: 0.0000000016 time: 0.39852237701416016\n",
      "Iteration: 12490 loss: 0.0000000016 time: 0.3941378593444824\n",
      "Iteration: 12500 loss: 0.0000000016 time: 0.404094934463501\n",
      "Iteration: 12510 loss: 0.0000000016 time: 0.40965819358825684\n",
      "Iteration: 12520 loss: 0.0000000016 time: 0.39861440658569336\n",
      "Iteration: 12530 loss: 0.0000000016 time: 0.37663793563842773\n",
      "Iteration: 12540 loss: 0.0000000016 time: 0.4014737606048584\n",
      "Iteration: 12550 loss: 0.0000000016 time: 0.39940357208251953\n",
      "Iteration: 12560 loss: 0.0000000016 time: 0.38451266288757324\n",
      "Iteration: 12570 loss: 0.0000000016 time: 0.4091825485229492\n",
      "Iteration: 12580 loss: 0.0000000016 time: 0.3971896171569824\n",
      "Iteration: 12590 loss: 0.0000000016 time: 0.40211987495422363\n",
      "Iteration: 12600 loss: 0.0000000023 time: 0.39592647552490234\n",
      "Iteration: 12610 loss: 0.0000000020 time: 0.3959157466888428\n",
      "Iteration: 12620 loss: 0.0000000016 time: 0.40815281867980957\n",
      "Iteration: 12630 loss: 0.0000000016 time: 0.4075355529785156\n",
      "Iteration: 12640 loss: 0.0000000015 time: 0.3896615505218506\n",
      "Iteration: 12650 loss: 0.0000000015 time: 0.4124138355255127\n",
      "Iteration: 12660 loss: 0.0000000015 time: 0.41297459602355957\n",
      "Iteration: 12670 loss: 0.0000000015 time: 0.39856433868408203\n",
      "Iteration: 12680 loss: 0.0000000015 time: 0.41752099990844727\n",
      "Iteration: 12690 loss: 0.0000000015 time: 0.39674901962280273\n",
      "Iteration: 12700 loss: 0.0000000015 time: 0.4084947109222412\n",
      "Iteration: 12710 loss: 0.0000000015 time: 0.4102330207824707\n",
      "Iteration: 12720 loss: 0.0000000027 time: 0.4016268253326416\n",
      "Iteration: 12730 loss: 0.0000000015 time: 0.4082934856414795\n",
      "Iteration: 12740 loss: 0.0000000016 time: 0.40494728088378906\n",
      "Iteration: 12750 loss: 0.0000000015 time: 0.4092421531677246\n",
      "Iteration: 12760 loss: 0.0000000015 time: 0.40341854095458984\n",
      "Iteration: 12770 loss: 0.0000000015 time: 0.39960193634033203\n",
      "Iteration: 12780 loss: 0.0000000015 time: 0.4055194854736328\n",
      "Iteration: 12790 loss: 0.0000000015 time: 0.4223971366882324\n",
      "Iteration: 12800 loss: 0.0000000015 time: 0.39847493171691895\n",
      "Iteration: 12810 loss: 0.0000000015 time: 0.3997011184692383\n",
      "Iteration: 12820 loss: 0.0000000015 time: 0.3957505226135254\n",
      "Iteration: 12830 loss: 0.0000000015 time: 0.39715099334716797\n",
      "Iteration: 12840 loss: 0.0000000015 time: 0.4107675552368164\n",
      "Iteration: 12850 loss: 0.0000000015 time: 0.41100454330444336\n",
      "Iteration: 12860 loss: 0.0000000015 time: 0.4093661308288574\n",
      "Iteration: 12870 loss: 0.0000000015 time: 0.3955037593841553\n",
      "Iteration: 12880 loss: 0.0000000031 time: 0.3952922821044922\n",
      "Iteration: 12890 loss: 0.0000000015 time: 0.3993532657623291\n",
      "Iteration: 12900 loss: 0.0000000016 time: 0.4146292209625244\n",
      "Iteration: 12910 loss: 0.0000000015 time: 0.4100334644317627\n",
      "Iteration: 12920 loss: 0.0000000015 time: 0.412839412689209\n",
      "Iteration: 12930 loss: 0.0000000015 time: 0.4119713306427002\n",
      "Iteration: 12940 loss: 0.0000000014 time: 0.4083893299102783\n",
      "Iteration: 12950 loss: 0.0000000014 time: 0.40464043617248535\n",
      "Iteration: 12960 loss: 0.0000000014 time: 0.401918888092041\n",
      "Iteration: 12970 loss: 0.0000000014 time: 0.40001511573791504\n",
      "Iteration: 12980 loss: 0.0000000014 time: 0.40323901176452637\n",
      "Iteration: 12990 loss: 0.0000000014 time: 0.40598511695861816\n",
      "Iteration: 13000 loss: 0.0000000014 time: 0.3974721431732178\n",
      "Iteration: 13010 loss: 0.0000000014 time: 0.41306066513061523\n",
      "Iteration: 13020 loss: 0.0000000014 time: 0.4138672351837158\n",
      "Iteration: 13030 loss: 0.0000000014 time: 0.40891385078430176\n",
      "Iteration: 13040 loss: 0.0000000019 time: 0.4129040241241455\n",
      "Iteration: 13050 loss: 0.0000000018 time: 0.4181382656097412\n",
      "Iteration: 13060 loss: 0.0000000014 time: 0.4301924705505371\n",
      "Iteration: 13070 loss: 0.0000000015 time: 0.4100463390350342\n",
      "Iteration: 13080 loss: 0.0000000014 time: 0.4068419933319092\n",
      "Iteration: 13090 loss: 0.0000000014 time: 0.4003183841705322\n",
      "Iteration: 13100 loss: 0.0000000014 time: 0.4106767177581787\n",
      "Iteration: 13110 loss: 0.0000000014 time: 0.417163610458374\n",
      "Iteration: 13120 loss: 0.0000000014 time: 0.4113020896911621\n",
      "Iteration: 13130 loss: 0.0000000014 time: 0.4042775630950928\n",
      "Iteration: 13140 loss: 0.0000000014 time: 0.4036064147949219\n",
      "Iteration: 13150 loss: 0.0000000016 time: 0.41467785835266113\n",
      "Iteration: 13160 loss: 0.0000000016 time: 0.412921667098999\n",
      "Iteration: 13170 loss: 0.0000000015 time: 0.4149935245513916\n",
      "Iteration: 13180 loss: 0.0000000015 time: 0.4076809883117676\n",
      "Iteration: 13190 loss: 0.0000000014 time: 0.40454602241516113\n",
      "Iteration: 13200 loss: 0.0000000014 time: 0.4120786190032959\n",
      "Iteration: 13210 loss: 0.0000000014 time: 0.4012267589569092\n",
      "Iteration: 13220 loss: 0.0000000014 time: 0.4166374206542969\n",
      "Iteration: 13230 loss: 0.0000000014 time: 0.4022648334503174\n",
      "Iteration: 13240 loss: 0.0000000014 time: 0.4013853073120117\n",
      "Iteration: 13250 loss: 0.0000000014 time: 0.4044811725616455\n",
      "Iteration: 13260 loss: 0.0000000014 time: 0.4151439666748047\n",
      "Iteration: 13270 loss: 0.0000000014 time: 0.4046812057495117\n",
      "Iteration: 13280 loss: 0.0000000014 time: 0.41795802116394043\n",
      "Iteration: 13290 loss: 0.0000000014 time: 0.4085566997528076\n",
      "Iteration: 13300 loss: 0.0000000013 time: 0.3982088565826416\n",
      "Iteration: 13310 loss: 0.0000000013 time: 0.3945779800415039\n",
      "Iteration: 13320 loss: 0.0000000013 time: 0.4067108631134033\n",
      "Iteration: 13330 loss: 0.0000000013 time: 0.41378235816955566\n",
      "Iteration: 13340 loss: 0.0000000014 time: 0.41143250465393066\n",
      "Iteration: 13350 loss: 0.0000000034 time: 0.40327930450439453\n",
      "Iteration: 13360 loss: 0.0000000017 time: 0.40172386169433594\n",
      "Iteration: 13370 loss: 0.0000000015 time: 0.38552045822143555\n",
      "Iteration: 13380 loss: 0.0000000014 time: 0.4106614589691162\n",
      "Iteration: 13390 loss: 0.0000000013 time: 0.4192380905151367\n",
      "Iteration: 13400 loss: 0.0000000013 time: 0.4129455089569092\n",
      "Iteration: 13410 loss: 0.0000000013 time: 0.4033932685852051\n",
      "Iteration: 13420 loss: 0.0000000013 time: 0.40274834632873535\n",
      "Iteration: 13430 loss: 0.0000000013 time: 0.3970670700073242\n",
      "Iteration: 13440 loss: 0.0000000013 time: 0.4101378917694092\n",
      "Iteration: 13450 loss: 0.0000000013 time: 0.4134829044342041\n",
      "Iteration: 13460 loss: 0.0000000013 time: 0.4116361141204834\n",
      "Iteration: 13470 loss: 0.0000000013 time: 0.39252495765686035\n",
      "Iteration: 13480 loss: 0.0000000013 time: 0.39850568771362305\n",
      "Iteration: 13490 loss: 0.0000000014 time: 0.3981809616088867\n",
      "Iteration: 13500 loss: 0.0000000020 time: 0.4131155014038086\n",
      "Iteration: 13510 loss: 0.0000000013 time: 0.40354061126708984\n",
      "Iteration: 13520 loss: 0.0000000014 time: 0.4011988639831543\n",
      "Iteration: 13530 loss: 0.0000000013 time: 0.3973958492279053\n",
      "Iteration: 13540 loss: 0.0000000013 time: 0.3919050693511963\n",
      "Iteration: 13550 loss: 0.0000000013 time: 0.41400647163391113\n",
      "Iteration: 13560 loss: 0.0000000013 time: 0.403637170791626\n",
      "Iteration: 13570 loss: 0.0000000013 time: 0.4106285572052002\n",
      "Iteration: 13580 loss: 0.0000000013 time: 0.39547038078308105\n",
      "Iteration: 13590 loss: 0.0000000013 time: 0.3988025188446045\n",
      "Iteration: 13600 loss: 0.0000000013 time: 0.40529537200927734\n",
      "Iteration: 13610 loss: 0.0000000013 time: 0.42510223388671875\n",
      "Iteration: 13620 loss: 0.0000000013 time: 0.4035911560058594\n",
      "Iteration: 13630 loss: 0.0000000013 time: 0.4143824577331543\n",
      "Iteration: 13640 loss: 0.0000000013 time: 0.4080541133880615\n",
      "Iteration: 13650 loss: 0.0000000013 time: 0.4128108024597168\n",
      "Iteration: 13660 loss: 0.0000000013 time: 0.40482449531555176\n",
      "Iteration: 13670 loss: 0.0000000013 time: 0.41083550453186035\n",
      "Iteration: 13680 loss: 0.0000000013 time: 0.40298032760620117\n",
      "Iteration: 13690 loss: 0.0000000013 time: 0.39756131172180176\n",
      "Iteration: 13700 loss: 0.0000000027 time: 0.4099607467651367\n",
      "Iteration: 13710 loss: 0.0000000021 time: 0.4095644950866699\n",
      "Iteration: 13720 loss: 0.0000000015 time: 0.4033787250518799\n",
      "Iteration: 13730 loss: 0.0000000013 time: 0.4107186794281006\n",
      "Iteration: 13740 loss: 0.0000000013 time: 0.40351080894470215\n",
      "Iteration: 13750 loss: 0.0000000012 time: 0.38962864875793457\n",
      "Iteration: 13760 loss: 0.0000000012 time: 0.3858916759490967\n",
      "Iteration: 13770 loss: 0.0000000012 time: 0.40032362937927246\n",
      "Iteration: 13780 loss: 0.0000000012 time: 0.39908409118652344\n",
      "Iteration: 13790 loss: 0.0000000012 time: 0.3922116756439209\n",
      "Iteration: 13800 loss: 0.0000000012 time: 0.39156317710876465\n",
      "Iteration: 13810 loss: 0.0000000013 time: 0.39374542236328125\n",
      "Iteration: 13820 loss: 0.0000000017 time: 0.4106769561767578\n",
      "Iteration: 13830 loss: 0.0000000013 time: 0.41120100021362305\n",
      "Iteration: 13840 loss: 0.0000000013 time: 0.39931702613830566\n",
      "Iteration: 13850 loss: 0.0000000012 time: 0.3971381187438965\n",
      "Iteration: 13860 loss: 0.0000000012 time: 0.40241479873657227\n",
      "Iteration: 13870 loss: 0.0000000012 time: 0.4078383445739746\n",
      "Iteration: 13880 loss: 0.0000000012 time: 0.41015076637268066\n",
      "Iteration: 13890 loss: 0.0000000012 time: 0.41608142852783203\n",
      "Iteration: 13900 loss: 0.0000000012 time: 0.3918609619140625\n",
      "Iteration: 13910 loss: 0.0000000012 time: 0.39197611808776855\n",
      "Iteration: 13920 loss: 0.0000000012 time: 0.4043600559234619\n",
      "Iteration: 13930 loss: 0.0000000012 time: 0.3911559581756592\n",
      "Iteration: 13940 loss: 0.0000000012 time: 0.4055619239807129\n",
      "Iteration: 13950 loss: 0.0000000012 time: 0.4053630828857422\n",
      "Iteration: 13960 loss: 0.0000000012 time: 0.3961203098297119\n",
      "Iteration: 13970 loss: 0.0000000012 time: 0.3991727828979492\n",
      "Iteration: 13980 loss: 0.0000000012 time: 0.3994598388671875\n",
      "Iteration: 13990 loss: 0.0000000012 time: 0.4155998229980469\n",
      "Iteration: 14000 loss: 0.0000000012 time: 0.40100908279418945\n",
      "Iteration: 14010 loss: 0.0000000012 time: 0.41742467880249023\n",
      "Iteration: 14020 loss: 0.0000000018 time: 0.4294712543487549\n",
      "Iteration: 14030 loss: 0.0000000024 time: 0.4135856628417969\n",
      "Iteration: 14040 loss: 0.0000000014 time: 0.41339683532714844\n",
      "Iteration: 14050 loss: 0.0000000013 time: 0.41399455070495605\n",
      "Iteration: 14060 loss: 0.0000000012 time: 0.39719486236572266\n",
      "Iteration: 14070 loss: 0.0000000012 time: 0.3981204032897949\n",
      "Iteration: 14080 loss: 0.0000000012 time: 0.40228843688964844\n",
      "Iteration: 14090 loss: 0.0000000012 time: 0.395862340927124\n",
      "Iteration: 14100 loss: 0.0000000012 time: 0.41544413566589355\n",
      "Iteration: 14110 loss: 0.0000000012 time: 0.40616631507873535\n",
      "Iteration: 14120 loss: 0.0000000012 time: 0.3972151279449463\n",
      "Iteration: 14130 loss: 0.0000000012 time: 0.40253210067749023\n",
      "Iteration: 14140 loss: 0.0000000012 time: 0.39638400077819824\n",
      "Iteration: 14150 loss: 0.0000000012 time: 0.3873462677001953\n",
      "Iteration: 14160 loss: 0.0000000014 time: 0.4138832092285156\n",
      "Iteration: 14170 loss: 0.0000000013 time: 0.4050896167755127\n",
      "Iteration: 14180 loss: 0.0000000012 time: 0.40012359619140625\n",
      "Iteration: 14190 loss: 0.0000000012 time: 0.3954734802246094\n",
      "Iteration: 14200 loss: 0.0000000012 time: 0.40883827209472656\n",
      "Iteration: 14210 loss: 0.0000000012 time: 0.4065823554992676\n",
      "Iteration: 14220 loss: 0.0000000012 time: 0.3937854766845703\n",
      "Iteration: 14230 loss: 0.0000000011 time: 0.40119457244873047\n",
      "Iteration: 14240 loss: 0.0000000011 time: 0.39237213134765625\n",
      "Iteration: 14250 loss: 0.0000000011 time: 0.39253830909729004\n",
      "Iteration: 14260 loss: 0.0000000011 time: 0.38784313201904297\n",
      "Iteration: 14270 loss: 0.0000000011 time: 0.40595531463623047\n",
      "Iteration: 14280 loss: 0.0000000011 time: 0.39296650886535645\n",
      "Iteration: 14290 loss: 0.0000000011 time: 0.3970456123352051\n",
      "Iteration: 14300 loss: 0.0000000011 time: 0.4007244110107422\n",
      "Iteration: 14310 loss: 0.0000000011 time: 0.3897359371185303\n",
      "Iteration: 14320 loss: 0.0000000011 time: 0.3993048667907715\n",
      "Iteration: 14330 loss: 0.0000000012 time: 0.407916784286499\n",
      "Iteration: 14340 loss: 0.0000000027 time: 0.4000084400177002\n",
      "Iteration: 14350 loss: 0.0000000012 time: 0.38680148124694824\n",
      "Iteration: 14360 loss: 0.0000000013 time: 0.40022706985473633\n",
      "Iteration: 14370 loss: 0.0000000011 time: 0.41166114807128906\n",
      "Iteration: 14380 loss: 0.0000000011 time: 0.4187502861022949\n",
      "Iteration: 14390 loss: 0.0000000011 time: 0.39240550994873047\n",
      "Iteration: 14400 loss: 0.0000000011 time: 0.389354944229126\n",
      "Iteration: 14410 loss: 0.0000000011 time: 0.39986538887023926\n",
      "Iteration: 14420 loss: 0.0000000011 time: 0.40249204635620117\n",
      "Iteration: 14430 loss: 0.0000000011 time: 0.4048619270324707\n",
      "Iteration: 14440 loss: 0.0000000011 time: 0.4106874465942383\n",
      "Iteration: 14450 loss: 0.0000000011 time: 0.4009370803833008\n",
      "Iteration: 14460 loss: 0.0000000011 time: 0.3989701271057129\n",
      "Iteration: 14470 loss: 0.0000000011 time: 0.3895761966705322\n",
      "Iteration: 14480 loss: 0.0000000021 time: 0.40197014808654785\n",
      "Iteration: 14490 loss: 0.0000000016 time: 0.40793633460998535\n",
      "Iteration: 14500 loss: 0.0000000011 time: 0.39005112648010254\n",
      "Iteration: 14510 loss: 0.0000000011 time: 0.3936328887939453\n",
      "Iteration: 14520 loss: 0.0000000011 time: 0.40195679664611816\n",
      "Iteration: 14530 loss: 0.0000000011 time: 0.39977335929870605\n",
      "Iteration: 14540 loss: 0.0000000011 time: 0.41752099990844727\n",
      "Iteration: 14550 loss: 0.0000000011 time: 0.396472692489624\n",
      "Iteration: 14560 loss: 0.0000000011 time: 0.39466309547424316\n",
      "Iteration: 14570 loss: 0.0000000011 time: 0.3995194435119629\n",
      "Iteration: 14580 loss: 0.0000000011 time: 0.3928565979003906\n",
      "Iteration: 14590 loss: 0.0000000011 time: 0.385495662689209\n",
      "Iteration: 14600 loss: 0.0000000011 time: 0.4036569595336914\n",
      "Iteration: 14610 loss: 0.0000000012 time: 0.40059542655944824\n",
      "Iteration: 14620 loss: 0.0000000026 time: 0.3982524871826172\n",
      "Iteration: 14630 loss: 0.0000000011 time: 0.39783263206481934\n",
      "Iteration: 14640 loss: 0.0000000012 time: 0.40085697174072266\n",
      "Iteration: 14650 loss: 0.0000000011 time: 0.40305328369140625\n",
      "Iteration: 14660 loss: 0.0000000011 time: 0.3989720344543457\n",
      "Iteration: 14670 loss: 0.0000000011 time: 0.3972506523132324\n",
      "Iteration: 14680 loss: 0.0000000011 time: 0.39315152168273926\n",
      "Iteration: 14690 loss: 0.0000000011 time: 0.39580392837524414\n",
      "Iteration: 14700 loss: 0.0000000011 time: 0.38500332832336426\n",
      "Iteration: 14710 loss: 0.0000000011 time: 0.4051074981689453\n",
      "Iteration: 14720 loss: 0.0000000011 time: 0.39719295501708984\n",
      "Iteration: 14730 loss: 0.0000000011 time: 0.39220309257507324\n",
      "Iteration: 14740 loss: 0.0000000011 time: 0.39578771591186523\n",
      "Iteration: 14750 loss: 0.0000000011 time: 0.40410900115966797\n",
      "Iteration: 14760 loss: 0.0000000010 time: 0.39782285690307617\n",
      "Iteration: 14770 loss: 0.0000000010 time: 0.39898204803466797\n",
      "Iteration: 14780 loss: 0.0000000011 time: 0.3932945728302002\n",
      "Iteration: 14790 loss: 0.0000000027 time: 0.3859443664550781\n",
      "Iteration: 14800 loss: 0.0000000011 time: 0.4018840789794922\n",
      "Iteration: 14810 loss: 0.0000000012 time: 0.39010190963745117\n",
      "Iteration: 14820 loss: 0.0000000011 time: 0.41327476501464844\n",
      "Iteration: 14830 loss: 0.0000000010 time: 0.38007473945617676\n",
      "Iteration: 14840 loss: 0.0000000010 time: 0.3917694091796875\n",
      "Iteration: 14850 loss: 0.0000000010 time: 0.39275169372558594\n",
      "Iteration: 14860 loss: 0.0000000010 time: 0.3994746208190918\n",
      "Iteration: 14870 loss: 0.0000000010 time: 0.3942835330963135\n",
      "Iteration: 14880 loss: 0.0000000010 time: 0.3881218433380127\n",
      "Iteration: 14890 loss: 0.0000000010 time: 0.40614843368530273\n",
      "Iteration: 14900 loss: 0.0000000010 time: 0.3913083076477051\n",
      "Iteration: 14910 loss: 0.0000000011 time: 0.40213537216186523\n",
      "Iteration: 14920 loss: 0.0000000023 time: 0.40117716789245605\n",
      "Iteration: 14930 loss: 0.0000000012 time: 0.4088265895843506\n",
      "Iteration: 14940 loss: 0.0000000011 time: 0.3881669044494629\n",
      "Iteration: 14950 loss: 0.0000000011 time: 0.41104650497436523\n",
      "Iteration: 14960 loss: 0.0000000010 time: 0.40464091300964355\n",
      "Iteration: 14970 loss: 0.0000000010 time: 0.3917672634124756\n",
      "Iteration: 14980 loss: 0.0000000010 time: 0.4062163829803467\n",
      "Iteration: 14990 loss: 0.0000000010 time: 0.40294599533081055\n",
      "Iteration: 15000 loss: 0.0000000010 time: 0.4008753299713135\n"
     ]
    }
   ],
   "source": [
    "geo_file = './unit_square_triangles'\n",
    "worker = gw.gmsh_worker(geo_file)\n",
    "worker.generate_parallel_chain(False, False, False)\n",
    "\n",
    "# seed for reproducibility\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=1000)\n",
    "\n",
    "# mesh init\n",
    "domain = ((0, 0), (1, 0), (1, 1), (0, 1))\n",
    "\n",
    "# order of test function \n",
    "N_test = 1\n",
    "params = {'scheme': 'VPINNs','N_test':N_test}\n",
    "\n",
    "xy, w = get_quad_rule(30)\n",
    "\n",
    "H1 = []\n",
    "L2 = []\n",
    "semi = []\n",
    "h = []\n",
    "\n",
    "for refinement in [0,1,2,3]:\n",
    "    model = restart_model()\n",
    "    mesh, _ = ml.take_parallel_mesh_chain(worker.chain[refinement],worker.chain[-1], 'DDDD')\n",
    "    mesh = mesh.convert_to_dict()\n",
    "    vp=VPINN(pb,params,mesh,False,model)\n",
    "    history=vp.train(15000, 0.00005)\n",
    "    # history=vp.train(10000, 0.0005)\n",
    "    # ml.compare(worker.chain[refinement],worker.chain[refinement])\n",
    "\n",
    "\n",
    "    L2_error_=L2_error(xy,w)\n",
    "\n",
    "\n",
    "    semi_H1_err = semi_H1_error(xy,w)\n",
    "    #H1_err = np.sqrt(L2_error**2 + semi_H1_err**2)\n",
    "\n",
    "    #H1.append(H1_err)\n",
    "    semi.append(semi_H1_err)\n",
    "    L2.append(L2_error_)\n",
    "    a, b = mesh['h_max'], mesh['h_min']\n",
    "    h.append(0.5*a + 0.5*b)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.array(h)\n",
    "\n",
    "#H1 = np.array(H1)\n",
    "L2 = np.array(L2)\n",
    "semi = np.array(semi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semi rate: 3.092049282417616\n",
      "L2 rate: 3.802529459820002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f950aff7ca0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slope_H1, intercept_H1 = np.polyfit(h, H1, 1)\n",
    "# slope_L2, intercept_L2 = np.polyfit(h, L2, 1)\n",
    "\n",
    "coeffs = np.polyfit(np.log10(h), np.log10(semi), 1)\n",
    "polynomial = np.poly1d(coeffs)\n",
    "log10_H1_fit = polynomial(np.log10(h))\n",
    "\n",
    "print(f\"semi rate: {coeffs[0]}\")\n",
    "\n",
    "coeffs = np.polyfit(np.log10(h), np.log10(L2), 1)\n",
    "polynomial = np.poly1d(coeffs)\n",
    "log10_L2_fit = polynomial(np.log10(h))\n",
    "\n",
    "print(f\"L2 rate: {coeffs[0]}\")\n",
    "\n",
    "#plt.loglog(h, H1, marker='o', label='H1')\n",
    "plt.loglog(h, L2, marker='o', label='L2')\n",
    "plt.loglog(h, semi, marker='o', label='semi')\n",
    "#plt.loglog(h, 10**log10_H1_fit)\n",
    "# plt.loglog(h, 10**log10_L2_fit)\n",
    "plt.loglog(h, h, linestyle=':', label='h1')\n",
    "plt.loglog(h, h*h, linestyle=':', label='h2')\n",
    "plt.loglog(h, h*h*h, linestyle=':', label='h3')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
