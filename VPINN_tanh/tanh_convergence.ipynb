{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 12:10:18.068053: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-26 12:10:18.980641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "my_types lib imported\n",
      "\n",
      "generate mesh lib imported\n",
      "\n",
      "interpolator_lib imported\n",
      "\n",
      "settings_lib imported \n",
      "\n",
      "MeshLib imported\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mesh import *\n",
    "\n",
    "from VPINN_tri_final import *\n",
    "\n",
    "from PROBDEF import PROBDEF\n",
    "\n",
    "import os; os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import logging; logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "\n",
    "import gmsh_worker as gw\n",
    "from MeshLib import MeshLib as ml\n",
    "\n",
    "# importing probdef \n",
    "pb = PROBDEF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quad pairs\n",
    "from numpy.polynomial import legendre\n",
    "\n",
    "def get_quad_rule(n: int):\n",
    "    # Degree of the Legendre polynomial (number of nodes - 1)\n",
    "\n",
    "    # Nodes and weights for Gauss-Legendre quadrature\n",
    "    nodes, weights = legendre.leggauss(n + 1)\n",
    "\n",
    "\n",
    "\n",
    "    nodes=(nodes+1.0)/2.0\n",
    "\n",
    "\n",
    "    # Nodes=np.array([nodes],dtype=np_type)\n",
    "\n",
    "    Weights=np.array([weights],dtype=np_type)\n",
    "    w = Weights.T @ Weights\n",
    "    w = np.reshape(w, (-1,1))\n",
    "\n",
    "\n",
    "    x, y =np.meshgrid(nodes,nodes)\n",
    "\n",
    "\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    xy=np.array([x,y]).T\n",
    "\n",
    "\n",
    "    return xy, w/4\n",
    "\n",
    "def L2_error(xy, w):\n",
    "    \n",
    "    u_NN = vp.NN_imposeBC(xy)\n",
    "\n",
    "    # Find the exact solution\n",
    "    u_ex = pb.u_exact_np(xy[:,0], xy[:,1])\n",
    "    u_ex = np.reshape(u_ex, (-1,1))\n",
    "\n",
    "    integrand = (u_ex - u_NN)**2\n",
    "\n",
    "\n",
    "    # Find the difference between exact and NN \n",
    "    # return (u_ex - u_NN)**2\n",
    "\n",
    "    return np.sqrt(np.sum(w*integrand))\n",
    "\n",
    "def semi_H1_error(xy, w):\n",
    "\n",
    "    # Find the gradient of the exact solution\n",
    "    grad_ex = np.array([pb.dudx(xy[:,0], xy[:,1]), pb.dudy(xy[:,0], xy[:,1])],dtype=np_type).T\n",
    "\n",
    "\n",
    "    # Find the gradient of the NN solution\n",
    "    grad_NN = vp.eval_grad_NN_BC(tf.constant(xy, dtype=tf_type))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Find the difference between exact and NN \n",
    "    pw_diff = grad_ex - grad_NN\n",
    "\n",
    "    pw_diff = tf.reduce_sum(tf.square(pw_diff),axis=1)\n",
    "\n",
    "    pw_diff=tf.reshape(pw_diff,(-1,1))\n",
    "\n",
    "\n",
    "    return np.sqrt(np.sum(pw_diff*w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restart_model():\n",
    "    model = tfk.models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(2,),dtype=tf.float64))\n",
    "    model.add(tfkl.Dense(50, activation='tanh',kernel_initializer=initializer,dtype=tf_type,bias_initializer=initializer))\n",
    "    model.add(tfkl.Dense(50, activation='tanh',kernel_initializer=initializer,dtype=tf_type,bias_initializer=initializer))\n",
    "    model.add(tfkl.Dense(50, activation='tanh',kernel_initializer=initializer,dtype=tf_type,bias_initializer=initializer))\n",
    "    model.add(tfkl.Dense(50, activation='tanh',kernel_initializer=initializer,dtype=tf_type,bias_initializer=initializer))\n",
    "    model.add(tfkl.Dense(50, activation='tanh',kernel_initializer=initializer,dtype=tf_type,bias_initializer=initializer))\n",
    "    model.add(tf.keras.layers.Dense(1,activation='linear',kernel_initializer=initializer,dtype=tf_type,use_bias=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 12:10:21.943790: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->mesh : \n",
      "     n_triangles :  4\n",
      "     n_vertices  :  5\n",
      "     n_edges     :  8\n",
      "     h_max           :  1.0\n",
      "     h_min           :  0.7071067811865476\n",
      "-->test_fun      : \n",
      "     order       :  1\n",
      "     dof         :  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 12:10:46.748453: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:10:46.752195: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:10:46.754937: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:10:46.756670: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:10:46.758280: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:10:46.760019: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:10:46.761812: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:10:46.763494: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:10:46.765485: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:10:46.768161: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:10:47.248867: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_43' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_43}}]]\n",
      "2023-12-26 12:10:47.248990: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_61' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_61}}]]\n",
      "2023-12-26 12:10:47.249048: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_75' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_75}}]]\n",
      "2023-12-26 12:10:47.249119: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_82' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_82}}]]\n",
      "2023-12-26 12:10:47.249172: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_87' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_87}}]]\n",
      "2023-12-26 12:10:47.249251: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_90' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_90}}]]\n",
      "2023-12-26 12:10:47.249324: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_93' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_93}}]]\n",
      "2023-12-26 12:10:47.249393: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_96' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_96}}]]\n",
      "2023-12-26 12:10:47.249463: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_99' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_99}}]]\n",
      "2023-12-26 12:10:47.249530: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_102' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_102}}]]\n",
      "2023-12-26 12:10:47.249655: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_105' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_105}}]]\n",
      "2023-12-26 12:10:47.249851: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_108' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_108}}]]\n",
      "2023-12-26 12:10:47.250022: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_111' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_111}}]]\n",
      "2023-12-26 12:10:47.250099: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_114' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_114}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 loss: 0.7905848660 time: 2.4879701137542725\n",
      "Iteration: 10 loss: 0.0211412887 time: 0.056490182876586914\n",
      "Iteration: 20 loss: 0.0246921956 time: 0.054799795150756836\n",
      "Iteration: 30 loss: 0.0097595312 time: 0.057351112365722656\n",
      "Iteration: 40 loss: 0.0009980606 time: 0.05601000785827637\n",
      "Iteration: 50 loss: 0.0020135766 time: 0.060976266860961914\n",
      "Iteration: 60 loss: 0.0000700154 time: 0.05264425277709961\n",
      "Iteration: 70 loss: 0.0001305067 time: 0.053154945373535156\n",
      "Iteration: 80 loss: 0.0000919079 time: 0.05169939994812012\n",
      "Iteration: 90 loss: 0.0000060521 time: 0.050173282623291016\n",
      "Iteration: 100 loss: 0.0000021869 time: 0.045819997787475586\n",
      "Iteration: 110 loss: 0.0000039338 time: 0.039420127868652344\n",
      "Iteration: 120 loss: 0.0000013230 time: 0.03429603576660156\n",
      "Iteration: 130 loss: 0.0000001163 time: 0.03215479850769043\n",
      "Iteration: 140 loss: 0.0000000051 time: 0.03532147407531738\n",
      "Iteration: 150 loss: 0.0000000326 time: 0.03364682197570801\n",
      "Iteration: 160 loss: 0.0000000230 time: 0.029192686080932617\n",
      "Iteration: 170 loss: 0.0000000085 time: 0.028944969177246094\n",
      "Iteration: 180 loss: 0.0000000020 time: 0.03245043754577637\n",
      "Iteration: 190 loss: 0.0000000003 time: 0.030998945236206055\n",
      "Iteration: 200 loss: 0.0000000000 time: 0.03428339958190918\n",
      "Iteration: 210 loss: 0.0000000000 time: 0.03286170959472656\n",
      "Iteration: 220 loss: 0.0000000000 time: 0.0341639518737793\n",
      "Iteration: 230 loss: 0.0000000000 time: 0.03566408157348633\n",
      "Iteration: 240 loss: 0.0000000000 time: 0.036052703857421875\n",
      "Iteration: 250 loss: 0.0000000000 time: 0.03954768180847168\n",
      "Iteration: 260 loss: 0.0000000000 time: 0.03505444526672363\n",
      "Iteration: 270 loss: 0.0000000000 time: 0.03684282302856445\n",
      "Iteration: 280 loss: 0.0000000000 time: 0.03347039222717285\n",
      "Iteration: 290 loss: 0.0000000000 time: 0.0345609188079834\n",
      "Iteration: 300 loss: 0.0000000000 time: 0.03369283676147461\n",
      "Iteration: 310 loss: 0.0000000000 time: 0.033666133880615234\n",
      "Iteration: 320 loss: 0.0000000000 time: 0.03462553024291992\n",
      "Iteration: 330 loss: 0.0000000000 time: 0.036438941955566406\n",
      "Iteration: 340 loss: 0.0000000000 time: 0.030898332595825195\n",
      "Iteration: 350 loss: 0.0000000000 time: 0.029724597930908203\n",
      "Iteration: 360 loss: 0.0000000000 time: 0.0287473201751709\n",
      "Iteration: 370 loss: 0.0000000000 time: 0.028433561325073242\n",
      "Iteration: 380 loss: 0.0000000000 time: 0.03183341026306152\n",
      "Iteration: 390 loss: 0.0000000000 time: 0.03337264060974121\n",
      "Iteration: 400 loss: 0.0000000000 time: 0.032030344009399414\n",
      "Iteration: 410 loss: 0.0000000000 time: 0.0279543399810791\n",
      "Iteration: 420 loss: 0.0000000000 time: 0.02844858169555664\n",
      "Iteration: 430 loss: 0.0000000000 time: 0.03213024139404297\n",
      "Iteration: 440 loss: 0.0000000000 time: 0.02985668182373047\n",
      "Iteration: 450 loss: 0.0000000000 time: 0.03667140007019043\n",
      "Iteration: 460 loss: 0.0000000000 time: 0.04359912872314453\n",
      "Iteration: 470 loss: 0.0000000000 time: 0.0406956672668457\n",
      "Iteration: 480 loss: 0.0000000000 time: 0.03342413902282715\n",
      "Iteration: 490 loss: 0.0000000000 time: 0.03126072883605957\n",
      "Iteration: 500 loss: 0.0000000000 time: 0.03517580032348633\n",
      "Iteration: 510 loss: 0.0000000000 time: 0.03542160987854004\n",
      "Iteration: 520 loss: 0.0000000000 time: 0.03484082221984863\n",
      "Iteration: 530 loss: 0.0000000000 time: 0.03614521026611328\n",
      "Iteration: 540 loss: 0.0000000000 time: 0.03641963005065918\n",
      "Iteration: 550 loss: 0.0000000000 time: 0.03454232215881348\n",
      "Iteration: 560 loss: 0.0000000000 time: 0.03279542922973633\n",
      "Iteration: 570 loss: 0.0000000000 time: 0.03360152244567871\n",
      "Iteration: 580 loss: 0.0000000000 time: 0.02974867820739746\n",
      "Iteration: 590 loss: 0.0000000000 time: 0.033907175064086914\n",
      "Iteration: 600 loss: 0.0000000000 time: 0.030109882354736328\n",
      "Iteration: 610 loss: 0.0000000000 time: 0.028993606567382812\n",
      "Iteration: 620 loss: 0.0000000000 time: 0.028280258178710938\n",
      "Iteration: 630 loss: 0.0000000000 time: 0.030472278594970703\n",
      "Iteration: 640 loss: 0.0000000000 time: 0.03092336654663086\n",
      "Iteration: 650 loss: 0.0000000000 time: 0.03201174736022949\n",
      "Iteration: 660 loss: 0.0000000000 time: 0.03412890434265137\n",
      "Iteration: 670 loss: 0.0000000000 time: 0.031075239181518555\n",
      "Iteration: 680 loss: 0.0000000000 time: 0.028904199600219727\n",
      "Iteration: 690 loss: 0.0000000000 time: 0.02705097198486328\n",
      "Iteration: 700 loss: 0.0000000000 time: 0.03326129913330078\n",
      "Iteration: 710 loss: 0.0000000000 time: 0.030199289321899414\n",
      "Iteration: 720 loss: 0.0000000000 time: 0.029773235321044922\n",
      "Iteration: 730 loss: 0.0000000000 time: 0.03623676300048828\n",
      "Iteration: 740 loss: 0.0000000000 time: 0.029694795608520508\n",
      "Iteration: 750 loss: 0.0000000000 time: 0.03075242042541504\n",
      "Iteration: 760 loss: 0.0000000000 time: 0.03202533721923828\n",
      "Iteration: 770 loss: 0.0000000000 time: 0.02996039390563965\n",
      "Iteration: 780 loss: 0.0000000000 time: 0.031342267990112305\n",
      "Iteration: 790 loss: 0.0000000000 time: 0.036988258361816406\n",
      "Iteration: 800 loss: 0.0000000000 time: 0.03284120559692383\n",
      "Iteration: 810 loss: 0.0000000000 time: 0.034105777740478516\n",
      "Iteration: 820 loss: 0.0000000000 time: 0.03212690353393555\n",
      "Iteration: 830 loss: 0.0000000000 time: 0.027903079986572266\n",
      "Iteration: 840 loss: 0.0000000000 time: 0.030326366424560547\n",
      "Iteration: 850 loss: 0.0000000000 time: 0.028888702392578125\n",
      "Iteration: 860 loss: 0.0000000000 time: 0.03411221504211426\n",
      "Iteration: 870 loss: 0.0000000000 time: 0.0333552360534668\n",
      "Iteration: 880 loss: 0.0000000000 time: 0.036478519439697266\n",
      "Iteration: 890 loss: 0.0000000000 time: 0.034236907958984375\n",
      "Iteration: 900 loss: 0.0000000000 time: 0.0366206169128418\n",
      "Iteration: 910 loss: 0.0000000000 time: 0.03648090362548828\n",
      "Iteration: 920 loss: 0.0000000000 time: 0.034863948822021484\n",
      "Iteration: 930 loss: 0.0000000000 time: 0.03772306442260742\n",
      "Iteration: 940 loss: 0.0000000000 time: 0.035562753677368164\n",
      "Iteration: 950 loss: 0.0000000000 time: 0.03432798385620117\n",
      "Iteration: 960 loss: 0.0000000000 time: 0.03424668312072754\n",
      "Iteration: 970 loss: 0.0000000000 time: 0.03303074836730957\n",
      "Iteration: 980 loss: 0.0000000000 time: 0.031148672103881836\n",
      "Iteration: 990 loss: 0.0000000000 time: 0.03433489799499512\n",
      "Iteration: 1000 loss: 0.0000000000 time: 0.03506135940551758\n",
      "Iteration: 1010 loss: 0.0000000000 time: 0.03520607948303223\n",
      "Iteration: 1020 loss: 0.0000000000 time: 0.030422449111938477\n",
      "Iteration: 1030 loss: 0.0000000000 time: 0.033155202865600586\n",
      "Iteration: 1040 loss: 0.0000000000 time: 0.033622026443481445\n",
      "Iteration: 1050 loss: 0.0000000000 time: 0.03419804573059082\n",
      "Iteration: 1060 loss: 0.0000000000 time: 0.03483748435974121\n",
      "Iteration: 1070 loss: 0.0000000000 time: 0.030137300491333008\n",
      "Iteration: 1080 loss: 0.0000000000 time: 0.028277873992919922\n",
      "Iteration: 1090 loss: 0.0000000000 time: 0.033453941345214844\n",
      "Iteration: 1100 loss: 0.0000000000 time: 0.03600120544433594\n",
      "Iteration: 1110 loss: 0.0000000000 time: 0.03315615653991699\n",
      "Iteration: 1120 loss: 0.0000000000 time: 0.03356361389160156\n",
      "Iteration: 1130 loss: 0.0000000000 time: 0.031469106674194336\n",
      "Iteration: 1140 loss: 0.0000000000 time: 0.03017449378967285\n",
      "Iteration: 1150 loss: 0.0000000000 time: 0.029682397842407227\n",
      "Iteration: 1160 loss: 0.0000000000 time: 0.03333878517150879\n",
      "Iteration: 1170 loss: 0.0000000000 time: 0.032018184661865234\n",
      "Iteration: 1180 loss: 0.0000000000 time: 0.0332026481628418\n",
      "Iteration: 1190 loss: 0.0000000000 time: 0.0313262939453125\n",
      "Iteration: 1200 loss: 0.0000000000 time: 0.02937030792236328\n",
      "Iteration: 1210 loss: 0.0000000000 time: 0.03060436248779297\n",
      "Iteration: 1220 loss: 0.0000000000 time: 0.03463172912597656\n",
      "Iteration: 1230 loss: 0.0000000000 time: 0.029658079147338867\n",
      "Iteration: 1240 loss: 0.0000000000 time: 0.0320127010345459\n",
      "Iteration: 1250 loss: 0.0000000000 time: 0.03407692909240723\n",
      "Iteration: 1260 loss: 0.0000000000 time: 0.03209877014160156\n",
      "Iteration: 1270 loss: 0.0000000000 time: 0.030817031860351562\n",
      "Iteration: 1280 loss: 0.0000000000 time: 0.03237438201904297\n",
      "Iteration: 1290 loss: 0.0000000000 time: 0.03147745132446289\n",
      "Iteration: 1300 loss: 0.0000000000 time: 0.029108524322509766\n",
      "Iteration: 1310 loss: 0.0000000000 time: 0.02891397476196289\n",
      "Iteration: 1320 loss: 0.0000000000 time: 0.03688359260559082\n",
      "Iteration: 1330 loss: 0.0000000000 time: 0.03354382514953613\n",
      "Iteration: 1340 loss: 0.0000000000 time: 0.029331684112548828\n",
      "Iteration: 1350 loss: 0.0000000000 time: 0.031080245971679688\n",
      "Iteration: 1360 loss: 0.0000000000 time: 0.02834916114807129\n",
      "Iteration: 1370 loss: 0.0000000000 time: 0.031365394592285156\n",
      "Iteration: 1380 loss: 0.0000000000 time: 0.030463457107543945\n",
      "Iteration: 1390 loss: 0.0000000000 time: 0.030368804931640625\n",
      "Iteration: 1400 loss: 0.0000000000 time: 0.036438703536987305\n",
      "Iteration: 1410 loss: 0.0000000000 time: 0.028704404830932617\n",
      "Iteration: 1420 loss: 0.0000000000 time: 0.030024290084838867\n",
      "Iteration: 1430 loss: 0.0000000000 time: 0.028825759887695312\n",
      "Iteration: 1440 loss: 0.0000000000 time: 0.032579660415649414\n",
      "Iteration: 1450 loss: 0.0000000000 time: 0.03129148483276367\n",
      "Iteration: 1460 loss: 0.0000000000 time: 0.03231167793273926\n",
      "Iteration: 1470 loss: 0.0000000000 time: 0.03175091743469238\n",
      "Iteration: 1480 loss: 0.0000000000 time: 0.0332334041595459\n",
      "Iteration: 1490 loss: 0.0000000000 time: 0.03539276123046875\n",
      "Iteration: 1500 loss: 0.0000000000 time: 0.03150200843811035\n",
      "Iteration: 1510 loss: 0.0000000000 time: 0.030916452407836914\n",
      "Iteration: 1520 loss: 0.0000000000 time: 0.03358578681945801\n",
      "Iteration: 1530 loss: 0.0000000000 time: 0.037114858627319336\n",
      "Iteration: 1540 loss: 0.0000000000 time: 0.03415226936340332\n",
      "Iteration: 1550 loss: 0.0000000000 time: 0.03516268730163574\n",
      "Iteration: 1560 loss: 0.0000000000 time: 0.03849458694458008\n",
      "Iteration: 1570 loss: 0.0000000000 time: 0.029051542282104492\n",
      "Iteration: 1580 loss: 0.0000000000 time: 0.028830528259277344\n",
      "Iteration: 1590 loss: 0.0000000000 time: 0.02889537811279297\n",
      "Iteration: 1600 loss: 0.0000000000 time: 0.030227184295654297\n",
      "Iteration: 1610 loss: 0.0000000000 time: 0.03202509880065918\n",
      "Iteration: 1620 loss: 0.0000000000 time: 0.031447410583496094\n",
      "Iteration: 1630 loss: 0.0000000000 time: 0.028568744659423828\n",
      "Iteration: 1640 loss: 0.0000000000 time: 0.0332188606262207\n",
      "Iteration: 1650 loss: 0.0000000000 time: 0.03160381317138672\n",
      "Iteration: 1660 loss: 0.0000000000 time: 0.030323505401611328\n",
      "Iteration: 1670 loss: 0.0000000000 time: 0.03133654594421387\n",
      "Iteration: 1680 loss: 0.0000000000 time: 0.029694080352783203\n",
      "Iteration: 1690 loss: 0.0000000000 time: 0.02831411361694336\n",
      "Iteration: 1700 loss: 0.0000000000 time: 0.02943730354309082\n",
      "Iteration: 1710 loss: 0.0000000000 time: 0.026701688766479492\n",
      "Iteration: 1720 loss: 0.0000000000 time: 0.032158851623535156\n",
      "Iteration: 1730 loss: 0.0000000000 time: 0.03260397911071777\n",
      "Iteration: 1740 loss: 0.0000000000 time: 0.03351879119873047\n",
      "Iteration: 1750 loss: 0.0000000000 time: 0.03441905975341797\n",
      "Iteration: 1760 loss: 0.0000000000 time: 0.029829978942871094\n",
      "Iteration: 1770 loss: 0.0000000000 time: 0.03038311004638672\n",
      "Iteration: 1780 loss: 0.0000000000 time: 0.03090810775756836\n",
      "Iteration: 1790 loss: 0.0000000000 time: 0.03293776512145996\n",
      "Iteration: 1800 loss: 0.0000000000 time: 0.029117345809936523\n",
      "Iteration: 1810 loss: 0.0000000000 time: 0.03129887580871582\n",
      "Iteration: 1820 loss: 0.0000000000 time: 0.03422880172729492\n",
      "Iteration: 1830 loss: 0.0000000000 time: 0.030321598052978516\n",
      "Iteration: 1840 loss: 0.0000000000 time: 0.03214859962463379\n",
      "Iteration: 1850 loss: 0.0000000000 time: 0.03459310531616211\n",
      "Iteration: 1860 loss: 0.0000000000 time: 0.033142805099487305\n",
      "Iteration: 1870 loss: 0.0000000000 time: 0.028661012649536133\n",
      "Iteration: 1880 loss: 0.0000000000 time: 0.03560614585876465\n",
      "Iteration: 1890 loss: 0.0000000000 time: 0.03184962272644043\n",
      "Iteration: 1900 loss: 0.0000000000 time: 0.029171228408813477\n",
      "Iteration: 1910 loss: 0.0000000000 time: 0.030476093292236328\n",
      "Iteration: 1920 loss: 0.0000000000 time: 0.030436992645263672\n",
      "Iteration: 1930 loss: 0.0000000000 time: 0.030237674713134766\n",
      "Iteration: 1940 loss: 0.0000000000 time: 0.03214073181152344\n",
      "Iteration: 1950 loss: 0.0000000000 time: 0.03135085105895996\n",
      "Iteration: 1960 loss: 0.0000000000 time: 0.04001450538635254\n",
      "Iteration: 1970 loss: 0.0000000000 time: 0.031133174896240234\n",
      "Iteration: 1980 loss: 0.0000000000 time: 0.028033018112182617\n",
      "Iteration: 1990 loss: 0.0000000000 time: 0.029100894927978516\n",
      "Iteration: 2000 loss: 0.0000000000 time: 0.03040146827697754\n",
      "Iteration: 2010 loss: 0.0000000000 time: 0.03249526023864746\n",
      "Iteration: 2020 loss: 0.0000000000 time: 0.03119039535522461\n",
      "Iteration: 2030 loss: 0.0000000000 time: 0.033292293548583984\n",
      "Iteration: 2040 loss: 0.0000000000 time: 0.032091379165649414\n",
      "Iteration: 2050 loss: 0.0000000000 time: 0.02893543243408203\n",
      "Iteration: 2060 loss: 0.0000000000 time: 0.03134655952453613\n",
      "Iteration: 2070 loss: 0.0000000000 time: 0.034328460693359375\n",
      "Iteration: 2080 loss: 0.0000000000 time: 0.030373811721801758\n",
      "Iteration: 2090 loss: 0.0000000000 time: 0.03295612335205078\n",
      "Iteration: 2100 loss: 0.0000000000 time: 0.03358602523803711\n",
      "Iteration: 2110 loss: 0.0000000000 time: 0.03173661231994629\n",
      "Iteration: 2120 loss: 0.0000000000 time: 0.033814191818237305\n",
      "Iteration: 2130 loss: 0.0000000000 time: 0.03509354591369629\n",
      "Iteration: 2140 loss: 0.0000000000 time: 0.031032562255859375\n",
      "Iteration: 2150 loss: 0.0000000000 time: 0.028290510177612305\n",
      "Iteration: 2160 loss: 0.0000000000 time: 0.03360557556152344\n",
      "Iteration: 2170 loss: 0.0000000000 time: 0.03481912612915039\n",
      "Iteration: 2180 loss: 0.0000000000 time: 0.0286102294921875\n",
      "Iteration: 2190 loss: 0.0000000000 time: 0.028664827346801758\n",
      "Iteration: 2200 loss: 0.0000000000 time: 0.03428006172180176\n",
      "Iteration: 2210 loss: 0.0000000000 time: 0.03770256042480469\n",
      "Iteration: 2220 loss: 0.0000000000 time: 0.036210060119628906\n",
      "Iteration: 2230 loss: 0.0000000000 time: 0.03924441337585449\n",
      "Iteration: 2240 loss: 0.0000000000 time: 0.03466510772705078\n",
      "Iteration: 2250 loss: 0.0000000000 time: 0.0353245735168457\n",
      "Iteration: 2260 loss: 0.0000000000 time: 0.03590750694274902\n",
      "Iteration: 2270 loss: 0.0000000000 time: 0.037474632263183594\n",
      "Iteration: 2280 loss: 0.0000000000 time: 0.02924323081970215\n",
      "Iteration: 2290 loss: 0.0000000000 time: 0.02938532829284668\n",
      "Iteration: 2300 loss: 0.0000000000 time: 0.029379606246948242\n",
      "Iteration: 2310 loss: 0.0000000000 time: 0.029194116592407227\n",
      "Iteration: 2320 loss: 0.0000000000 time: 0.028296232223510742\n",
      "Iteration: 2330 loss: 0.0000000000 time: 0.02963566780090332\n",
      "Iteration: 2340 loss: 0.0000000000 time: 0.029353618621826172\n",
      "Iteration: 2350 loss: 0.0000000000 time: 0.02982020378112793\n",
      "Iteration: 2360 loss: 0.0000000000 time: 0.03486013412475586\n",
      "Iteration: 2370 loss: 0.0000000000 time: 0.035161495208740234\n",
      "Iteration: 2380 loss: 0.0000000000 time: 0.032922983169555664\n",
      "Iteration: 2390 loss: 0.0000000000 time: 0.03329277038574219\n",
      "Iteration: 2400 loss: 0.0000000000 time: 0.030484437942504883\n",
      "Iteration: 2410 loss: 0.0000000000 time: 0.02864551544189453\n",
      "Iteration: 2420 loss: 0.0000000000 time: 0.030326366424560547\n",
      "Iteration: 2430 loss: 0.0000000000 time: 0.036749839782714844\n",
      "Iteration: 2440 loss: 0.0000000000 time: 0.029787540435791016\n",
      "Iteration: 2450 loss: 0.0000000000 time: 0.0291898250579834\n",
      "Iteration: 2460 loss: 0.0000000000 time: 0.03344011306762695\n",
      "Iteration: 2470 loss: 0.0000000000 time: 0.03285646438598633\n",
      "Iteration: 2480 loss: 0.0000000000 time: 0.030155420303344727\n",
      "Iteration: 2490 loss: 0.0000000000 time: 0.031377315521240234\n",
      "Iteration: 2500 loss: 0.0000000000 time: 0.0350193977355957\n",
      "Iteration: 2510 loss: 0.0000000000 time: 0.034860849380493164\n",
      "Iteration: 2520 loss: 0.0000000000 time: 0.030882596969604492\n",
      "Iteration: 2530 loss: 0.0000000000 time: 0.033638954162597656\n",
      "Iteration: 2540 loss: 0.0000000000 time: 0.03092026710510254\n",
      "Iteration: 2550 loss: 0.0000000000 time: 0.031036853790283203\n",
      "Iteration: 2560 loss: 0.0000000000 time: 0.030945777893066406\n",
      "Iteration: 2570 loss: 0.0000000000 time: 0.03701376914978027\n",
      "Iteration: 2580 loss: 0.0000000000 time: 0.04096722602844238\n",
      "Iteration: 2590 loss: 0.0000000000 time: 0.031609535217285156\n",
      "Iteration: 2600 loss: 0.0000000000 time: 0.032378435134887695\n",
      "Iteration: 2610 loss: 0.0000000000 time: 0.03243279457092285\n",
      "Iteration: 2620 loss: 0.0000000000 time: 0.0322420597076416\n",
      "Iteration: 2630 loss: 0.0000000000 time: 0.031549692153930664\n",
      "Iteration: 2640 loss: 0.0000000000 time: 0.03534126281738281\n",
      "Iteration: 2650 loss: 0.0000000000 time: 0.03482484817504883\n",
      "Iteration: 2660 loss: 0.0000000000 time: 0.031221389770507812\n",
      "Iteration: 2670 loss: 0.0000000000 time: 0.031824350357055664\n",
      "Iteration: 2680 loss: 0.0000000000 time: 0.03217506408691406\n",
      "Iteration: 2690 loss: 0.0000000000 time: 0.03184700012207031\n",
      "Iteration: 2700 loss: 0.0000000000 time: 0.03119516372680664\n",
      "Iteration: 2710 loss: 0.0000000000 time: 0.033844947814941406\n",
      "Iteration: 2720 loss: 0.0000000000 time: 0.027115583419799805\n",
      "Iteration: 2730 loss: 0.0000000000 time: 0.031630754470825195\n",
      "Iteration: 2740 loss: 0.0000000000 time: 0.03365015983581543\n",
      "Iteration: 2750 loss: 0.0000000000 time: 0.030270099639892578\n",
      "Iteration: 2760 loss: 0.0000000000 time: 0.03202986717224121\n",
      "Iteration: 2770 loss: 0.0000000000 time: 0.031035184860229492\n",
      "Iteration: 2780 loss: 0.0000000000 time: 0.033818721771240234\n",
      "Iteration: 2790 loss: 0.0000000000 time: 0.028359651565551758\n",
      "Iteration: 2800 loss: 0.0000000000 time: 0.02746438980102539\n",
      "Iteration: 2810 loss: 0.0000000000 time: 0.031513214111328125\n",
      "Iteration: 2820 loss: 0.0000000000 time: 0.033396005630493164\n",
      "Iteration: 2830 loss: 0.0000000000 time: 0.027521371841430664\n",
      "Iteration: 2840 loss: 0.0000000000 time: 0.03280210494995117\n",
      "Iteration: 2850 loss: 0.0000000000 time: 0.034069061279296875\n",
      "Iteration: 2860 loss: 0.0000000000 time: 0.03175854682922363\n",
      "Iteration: 2870 loss: 0.0000000000 time: 0.03501319885253906\n",
      "Iteration: 2880 loss: 0.0000000000 time: 0.03292369842529297\n",
      "Iteration: 2890 loss: 0.0000000000 time: 0.03210330009460449\n",
      "Iteration: 2900 loss: 0.0000000000 time: 0.0366520881652832\n",
      "Iteration: 2910 loss: 0.0000000000 time: 0.038666486740112305\n",
      "Iteration: 2920 loss: 0.0000000000 time: 0.03511166572570801\n",
      "Iteration: 2930 loss: 0.0000000000 time: 0.03548765182495117\n",
      "Iteration: 2940 loss: 0.0000000000 time: 0.0361781120300293\n",
      "Iteration: 2950 loss: 0.0000000000 time: 0.03581428527832031\n",
      "Iteration: 2960 loss: 0.0000000000 time: 0.03708457946777344\n",
      "Iteration: 2970 loss: 0.0000000000 time: 0.03857541084289551\n",
      "Iteration: 2980 loss: 0.0000000000 time: 0.03598308563232422\n",
      "Iteration: 2990 loss: 0.0000000000 time: 0.03627371788024902\n",
      "Iteration: 3000 loss: 0.0000000000 time: 0.03322887420654297\n",
      "Iteration: 3010 loss: 0.0000000000 time: 0.032802581787109375\n",
      "Iteration: 3020 loss: 0.0000000000 time: 0.029958486557006836\n",
      "Iteration: 3030 loss: 0.0000000000 time: 0.03163623809814453\n",
      "Iteration: 3040 loss: 0.0000000000 time: 0.03269839286804199\n",
      "Iteration: 3050 loss: 0.0000000000 time: 0.029015302658081055\n",
      "Iteration: 3060 loss: 0.0000000000 time: 0.029942035675048828\n",
      "Iteration: 3070 loss: 0.0000000000 time: 0.028694629669189453\n",
      "Iteration: 3080 loss: 0.0000000000 time: 0.028390884399414062\n",
      "Iteration: 3090 loss: 0.0000000000 time: 0.03642749786376953\n",
      "Iteration: 3100 loss: 0.0000000000 time: 0.03349566459655762\n",
      "Iteration: 3110 loss: 0.0000000000 time: 0.03079962730407715\n",
      "Iteration: 3120 loss: 0.0000000000 time: 0.03197336196899414\n",
      "Iteration: 3130 loss: 0.0000000000 time: 0.028558969497680664\n",
      "Iteration: 3140 loss: 0.0000000000 time: 0.028664827346801758\n",
      "Iteration: 3150 loss: 0.0000000000 time: 0.02878284454345703\n",
      "Iteration: 3160 loss: 0.0000000000 time: 0.03479576110839844\n",
      "Iteration: 3170 loss: 0.0000000000 time: 0.03509998321533203\n",
      "Iteration: 3180 loss: 0.0000000000 time: 0.029473066329956055\n",
      "Iteration: 3190 loss: 0.0000000000 time: 0.0307919979095459\n",
      "Iteration: 3200 loss: 0.0000000000 time: 0.03822517395019531\n",
      "Iteration: 3210 loss: 0.0000000000 time: 0.036109209060668945\n",
      "Iteration: 3220 loss: 0.0000000000 time: 0.030972957611083984\n",
      "Iteration: 3230 loss: 0.0000000000 time: 0.03749990463256836\n",
      "Iteration: 3240 loss: 0.0000000000 time: 0.03019237518310547\n",
      "Iteration: 3250 loss: 0.0000000000 time: 0.029653549194335938\n",
      "Iteration: 3260 loss: 0.0000000000 time: 0.03384518623352051\n",
      "Iteration: 3270 loss: 0.0000000000 time: 0.03033614158630371\n",
      "Iteration: 3280 loss: 0.0000000000 time: 0.030660629272460938\n",
      "Iteration: 3290 loss: 0.0000000000 time: 0.029868602752685547\n",
      "Iteration: 3300 loss: 0.0000000000 time: 0.0348048210144043\n",
      "Iteration: 3310 loss: 0.0000000000 time: 0.03420066833496094\n",
      "Iteration: 3320 loss: 0.0000000000 time: 0.03594613075256348\n",
      "Iteration: 3330 loss: 0.0000000000 time: 0.032181739807128906\n",
      "Iteration: 3340 loss: 0.0000000000 time: 0.02849578857421875\n",
      "Iteration: 3350 loss: 0.0000000000 time: 0.028604507446289062\n",
      "Iteration: 3360 loss: 0.0000000000 time: 0.0370488166809082\n",
      "Iteration: 3370 loss: 0.0000000000 time: 0.03609800338745117\n",
      "Iteration: 3380 loss: 0.0000000000 time: 0.033145904541015625\n",
      "Iteration: 3390 loss: 0.0000000000 time: 0.027939796447753906\n",
      "Iteration: 3400 loss: 0.0000000000 time: 0.0315549373626709\n",
      "Iteration: 3410 loss: 0.0000000000 time: 0.02926015853881836\n",
      "Iteration: 3420 loss: 0.0000000000 time: 0.03337430953979492\n",
      "Iteration: 3430 loss: 0.0000000000 time: 0.03428936004638672\n",
      "Iteration: 3440 loss: 0.0000000000 time: 0.030652523040771484\n",
      "Iteration: 3450 loss: 0.0000000000 time: 0.03221464157104492\n",
      "Iteration: 3460 loss: 0.0000000000 time: 0.03019547462463379\n",
      "Iteration: 3470 loss: 0.0000000000 time: 0.03214216232299805\n",
      "Iteration: 3480 loss: 0.0000000000 time: 0.031153440475463867\n",
      "Iteration: 3490 loss: 0.0000000000 time: 0.031951189041137695\n",
      "Iteration: 3500 loss: 0.0000000000 time: 0.02945232391357422\n",
      "Iteration: 3510 loss: 0.0000000000 time: 0.03696393966674805\n",
      "Iteration: 3520 loss: 0.0000000000 time: 0.03766822814941406\n",
      "Iteration: 3530 loss: 0.0000000000 time: 0.03599262237548828\n",
      "Iteration: 3540 loss: 0.0000000000 time: 0.03960990905761719\n",
      "Iteration: 3550 loss: 0.0000000000 time: 0.03675103187561035\n",
      "Iteration: 3560 loss: 0.0000000000 time: 0.03699684143066406\n",
      "Iteration: 3570 loss: 0.0000000000 time: 0.03939962387084961\n",
      "Iteration: 3580 loss: 0.0000000000 time: 0.03738546371459961\n",
      "Iteration: 3590 loss: 0.0000000000 time: 0.04073357582092285\n",
      "Iteration: 3600 loss: 0.0000000000 time: 0.03294062614440918\n",
      "Iteration: 3610 loss: 0.0000000000 time: 0.030580997467041016\n",
      "Iteration: 3620 loss: 0.0000000000 time: 0.029078960418701172\n",
      "Iteration: 3630 loss: 0.0000000000 time: 0.030683279037475586\n",
      "Iteration: 3640 loss: 0.0000000000 time: 0.03407692909240723\n",
      "Iteration: 3650 loss: 0.0000000000 time: 0.034081459045410156\n",
      "Iteration: 3660 loss: 0.0000000000 time: 0.03863811492919922\n",
      "Iteration: 3670 loss: 0.0000000000 time: 0.03600788116455078\n",
      "Iteration: 3680 loss: 0.0000000000 time: 0.03635239601135254\n",
      "Iteration: 3690 loss: 0.0000000000 time: 0.03894996643066406\n",
      "Iteration: 3700 loss: 0.0000000000 time: 0.03761005401611328\n",
      "Iteration: 3710 loss: 0.0000000000 time: 0.03588271141052246\n",
      "Iteration: 3720 loss: 0.0000000000 time: 0.036649227142333984\n",
      "Iteration: 3730 loss: 0.0000000000 time: 0.036415815353393555\n",
      "Iteration: 3740 loss: 0.0000000000 time: 0.03374314308166504\n",
      "Iteration: 3750 loss: 0.0000000000 time: 0.03629732131958008\n",
      "Iteration: 3760 loss: 0.0000000000 time: 0.031762123107910156\n",
      "Iteration: 3770 loss: 0.0000000000 time: 0.027845144271850586\n",
      "Iteration: 3780 loss: 0.0000000000 time: 0.030660629272460938\n",
      "Iteration: 3790 loss: 0.0000000000 time: 0.0341486930847168\n",
      "Iteration: 3800 loss: 0.0000000000 time: 0.03339529037475586\n",
      "Iteration: 3810 loss: 0.0000000000 time: 0.03615975379943848\n",
      "Iteration: 3820 loss: 0.0000000000 time: 0.03943991661071777\n",
      "Iteration: 3830 loss: 0.0000000000 time: 0.032007455825805664\n",
      "Iteration: 3840 loss: 0.0000000000 time: 0.030515193939208984\n",
      "Iteration: 3850 loss: 0.0000000000 time: 0.034020185470581055\n",
      "Iteration: 3860 loss: 0.0000000000 time: 0.030475378036499023\n",
      "Iteration: 3870 loss: 0.0000000000 time: 0.032063961029052734\n",
      "Iteration: 3880 loss: 0.0000000000 time: 0.03272843360900879\n",
      "Iteration: 3890 loss: 0.0000000000 time: 0.034189462661743164\n",
      "Iteration: 3900 loss: 0.0000000000 time: 0.03169441223144531\n",
      "Iteration: 3910 loss: 0.0000000000 time: 0.02909564971923828\n",
      "Iteration: 3920 loss: 0.0000000000 time: 0.02866077423095703\n",
      "Iteration: 3930 loss: 0.0000000000 time: 0.03162789344787598\n",
      "Iteration: 3940 loss: 0.0000000000 time: 0.029513835906982422\n",
      "Iteration: 3950 loss: 0.0000000000 time: 0.030508756637573242\n",
      "Iteration: 3960 loss: 0.0000000000 time: 0.03977370262145996\n",
      "Iteration: 3970 loss: 0.0000000000 time: 0.03474926948547363\n",
      "Iteration: 3980 loss: 0.0000000000 time: 0.029159069061279297\n",
      "Iteration: 3990 loss: 0.0000000000 time: 0.03028583526611328\n",
      "Iteration: 4000 loss: 0.0000000000 time: 0.02901434898376465\n",
      "Iteration: 4010 loss: 0.0000000000 time: 0.02893233299255371\n",
      "Iteration: 4020 loss: 0.0000000000 time: 0.03192758560180664\n",
      "Iteration: 4030 loss: 0.0000000000 time: 0.03180694580078125\n",
      "Iteration: 4040 loss: 0.0000000000 time: 0.03198885917663574\n",
      "Iteration: 4050 loss: 0.0000000000 time: 0.031958580017089844\n",
      "Iteration: 4060 loss: 0.0000000000 time: 0.03160548210144043\n",
      "Iteration: 4070 loss: 0.0000000000 time: 0.030846834182739258\n",
      "Iteration: 4080 loss: 0.0000000000 time: 0.03177332878112793\n",
      "Iteration: 4090 loss: 0.0000000000 time: 0.02863311767578125\n",
      "Iteration: 4100 loss: 0.0000000000 time: 0.036333560943603516\n",
      "Iteration: 4110 loss: 0.0000000000 time: 0.03453636169433594\n",
      "Iteration: 4120 loss: 0.0000000000 time: 0.03247547149658203\n",
      "Iteration: 4130 loss: 0.0000000000 time: 0.030713319778442383\n",
      "Iteration: 4140 loss: 0.0000000000 time: 0.029860258102416992\n",
      "Iteration: 4150 loss: 0.0000000000 time: 0.035921573638916016\n",
      "Iteration: 4160 loss: 0.0000000000 time: 0.03464555740356445\n",
      "Iteration: 4170 loss: 0.0000000000 time: 0.03881549835205078\n",
      "Iteration: 4180 loss: 0.0000000000 time: 0.03157496452331543\n",
      "Iteration: 4190 loss: 0.0000000000 time: 0.028167724609375\n",
      "Iteration: 4200 loss: 0.0000000000 time: 0.028357505798339844\n",
      "Iteration: 4210 loss: 0.0000000000 time: 0.028301000595092773\n",
      "Iteration: 4220 loss: 0.0000000000 time: 0.028916358947753906\n",
      "Iteration: 4230 loss: 0.0000000000 time: 0.027470827102661133\n",
      "Iteration: 4240 loss: 0.0000000000 time: 0.02788996696472168\n",
      "Iteration: 4250 loss: 0.0000000000 time: 0.03600788116455078\n",
      "Iteration: 4260 loss: 0.0000000000 time: 0.03198099136352539\n",
      "Iteration: 4270 loss: 0.0000000000 time: 0.029248952865600586\n",
      "Iteration: 4280 loss: 0.0000000000 time: 0.03771400451660156\n",
      "Iteration: 4290 loss: 0.0000000000 time: 0.03327178955078125\n",
      "Iteration: 4300 loss: 0.0000000000 time: 0.03026556968688965\n",
      "Iteration: 4310 loss: 0.0000000000 time: 0.033123016357421875\n",
      "Iteration: 4320 loss: 0.0000000000 time: 0.033571720123291016\n",
      "Iteration: 4330 loss: 0.0000000000 time: 0.02949810028076172\n",
      "Iteration: 4340 loss: 0.0000000000 time: 0.029816865921020508\n",
      "Iteration: 4350 loss: 0.0000000000 time: 0.034594058990478516\n",
      "Iteration: 4360 loss: 0.0000000000 time: 0.03078317642211914\n",
      "Iteration: 4370 loss: 0.0000000000 time: 0.03226017951965332\n",
      "Iteration: 4380 loss: 0.0000000000 time: 0.029427289962768555\n",
      "Iteration: 4390 loss: 0.0000000000 time: 0.03232216835021973\n",
      "Iteration: 4400 loss: 0.0000000000 time: 0.0391387939453125\n",
      "Iteration: 4410 loss: 0.0000000000 time: 0.029733896255493164\n",
      "Iteration: 4420 loss: 0.0000000000 time: 0.02873826026916504\n",
      "Iteration: 4430 loss: 0.0000000000 time: 0.03250479698181152\n",
      "Iteration: 4440 loss: 0.0000000000 time: 0.0289614200592041\n",
      "Iteration: 4450 loss: 0.0000000000 time: 0.02978205680847168\n",
      "Iteration: 4460 loss: 0.0000000000 time: 0.03307914733886719\n",
      "Iteration: 4470 loss: 0.0000000000 time: 0.031282901763916016\n",
      "Iteration: 4480 loss: 0.0000000000 time: 0.029264211654663086\n",
      "Iteration: 4490 loss: 0.0000000000 time: 0.02837228775024414\n",
      "Iteration: 4500 loss: 0.0000000000 time: 0.02919793128967285\n",
      "Iteration: 4510 loss: 0.0000000000 time: 0.03205704689025879\n",
      "Iteration: 4520 loss: 0.0000000000 time: 0.032462358474731445\n",
      "Iteration: 4530 loss: 0.0000000000 time: 0.029062509536743164\n",
      "Iteration: 4540 loss: 0.0000000000 time: 0.033150434494018555\n",
      "Iteration: 4550 loss: 0.0000000000 time: 0.03390336036682129\n",
      "Iteration: 4560 loss: 0.0000000000 time: 0.029163122177124023\n",
      "Iteration: 4570 loss: 0.0000000000 time: 0.030622005462646484\n",
      "Iteration: 4580 loss: 0.0000000000 time: 0.03300189971923828\n",
      "Iteration: 4590 loss: 0.0000000000 time: 0.03265118598937988\n",
      "Iteration: 4600 loss: 0.0000000000 time: 0.035393714904785156\n",
      "Iteration: 4610 loss: 0.0000000000 time: 0.029694795608520508\n",
      "Iteration: 4620 loss: 0.0000000000 time: 0.03153657913208008\n",
      "Iteration: 4630 loss: 0.0000000000 time: 0.02958989143371582\n",
      "Iteration: 4640 loss: 0.0000000000 time: 0.029349565505981445\n",
      "Iteration: 4650 loss: 0.0000000000 time: 0.031784772872924805\n",
      "Iteration: 4660 loss: 0.0000000000 time: 0.03177046775817871\n",
      "Iteration: 4670 loss: 0.0000000000 time: 0.03358030319213867\n",
      "Iteration: 4680 loss: 0.0000000000 time: 0.033768415451049805\n",
      "Iteration: 4690 loss: 0.0000000000 time: 0.029779911041259766\n",
      "Iteration: 4700 loss: 0.0000000000 time: 0.029410600662231445\n",
      "Iteration: 4710 loss: 0.0000000000 time: 0.029866933822631836\n",
      "Iteration: 4720 loss: 0.0000000000 time: 0.03482246398925781\n",
      "Iteration: 4730 loss: 0.0000000000 time: 0.030484914779663086\n",
      "Iteration: 4740 loss: 0.0000000000 time: 0.031090736389160156\n",
      "Iteration: 4750 loss: 0.0000000000 time: 0.03347492218017578\n",
      "Iteration: 4760 loss: 0.0000000000 time: 0.03184032440185547\n",
      "Iteration: 4770 loss: 0.0000000000 time: 0.029469013214111328\n",
      "Iteration: 4780 loss: 0.0000000000 time: 0.03126859664916992\n",
      "Iteration: 4790 loss: 0.0000000000 time: 0.028475046157836914\n",
      "Iteration: 4800 loss: 0.0000000000 time: 0.031456947326660156\n",
      "Iteration: 4810 loss: 0.0000000000 time: 0.03372812271118164\n",
      "Iteration: 4820 loss: 0.0000000000 time: 0.036771535873413086\n",
      "Iteration: 4830 loss: 0.0000000000 time: 0.036911725997924805\n",
      "Iteration: 4840 loss: 0.0000000000 time: 0.03494381904602051\n",
      "Iteration: 4850 loss: 0.0000000000 time: 0.03351593017578125\n",
      "Iteration: 4860 loss: 0.0000000000 time: 0.038541555404663086\n",
      "Iteration: 4870 loss: 0.0000000000 time: 0.03725767135620117\n",
      "Iteration: 4880 loss: 0.0000000000 time: 0.03887772560119629\n",
      "Iteration: 4890 loss: 0.0000000000 time: 0.036873817443847656\n",
      "Iteration: 4900 loss: 0.0000000000 time: 0.038718223571777344\n",
      "Iteration: 4910 loss: 0.0000000000 time: 0.034921884536743164\n",
      "Iteration: 4920 loss: 0.0000000000 time: 0.030705690383911133\n",
      "Iteration: 4930 loss: 0.0000000000 time: 0.032707929611206055\n",
      "Iteration: 4940 loss: 0.0000000000 time: 0.0330049991607666\n",
      "Iteration: 4950 loss: 0.0000000000 time: 0.029575109481811523\n",
      "Iteration: 4960 loss: 0.0000000000 time: 0.03165173530578613\n",
      "Iteration: 4970 loss: 0.0000000000 time: 0.03544783592224121\n",
      "Iteration: 4980 loss: 0.0000000000 time: 0.03159642219543457\n",
      "Iteration: 4990 loss: 0.0000000000 time: 0.03101348876953125\n",
      "Iteration: 5000 loss: 0.0000000000 time: 0.03354692459106445\n",
      "Iteration: 5010 loss: 0.0000000000 time: 0.03279447555541992\n",
      "Iteration: 5020 loss: 0.0000000000 time: 0.03115391731262207\n",
      "Iteration: 5030 loss: 0.0000000000 time: 0.030696392059326172\n",
      "Iteration: 5040 loss: 0.0000000000 time: 0.029410600662231445\n",
      "Iteration: 5050 loss: 0.0000000000 time: 0.031005382537841797\n",
      "Iteration: 5060 loss: 0.0000000000 time: 0.036887407302856445\n",
      "Iteration: 5070 loss: 0.0000000000 time: 0.03611350059509277\n",
      "Iteration: 5080 loss: 0.0000000000 time: 0.03231406211853027\n",
      "Iteration: 5090 loss: 0.0000000000 time: 0.03208422660827637\n",
      "Iteration: 5100 loss: 0.0000000000 time: 0.03260183334350586\n",
      "Iteration: 5110 loss: 0.0000000000 time: 0.029000043869018555\n",
      "Iteration: 5120 loss: 0.0000000000 time: 0.028896808624267578\n",
      "Iteration: 5130 loss: 0.0000000000 time: 0.03245973587036133\n",
      "Iteration: 5140 loss: 0.0000000000 time: 0.03323554992675781\n",
      "Iteration: 5150 loss: 0.0000000000 time: 0.03166079521179199\n",
      "Iteration: 5160 loss: 0.0000000000 time: 0.02897357940673828\n",
      "Iteration: 5170 loss: 0.0000000000 time: 0.03448224067687988\n",
      "Iteration: 5180 loss: 0.0000000000 time: 0.031424760818481445\n",
      "Iteration: 5190 loss: 0.0000000000 time: 0.03247570991516113\n",
      "Iteration: 5200 loss: 0.0000000000 time: 0.03415060043334961\n",
      "Iteration: 5210 loss: 0.0000000000 time: 0.03447461128234863\n",
      "Iteration: 5220 loss: 0.0000000000 time: 0.03539562225341797\n",
      "Iteration: 5230 loss: 0.0000000000 time: 0.028992176055908203\n",
      "Iteration: 5240 loss: 0.0000000000 time: 0.031152963638305664\n",
      "Iteration: 5250 loss: 0.0000000000 time: 0.03174877166748047\n",
      "Iteration: 5260 loss: 0.0000000000 time: 0.030792951583862305\n",
      "Iteration: 5270 loss: 0.0000000000 time: 0.031952857971191406\n",
      "Iteration: 5280 loss: 0.0000000000 time: 0.03149676322937012\n",
      "Iteration: 5290 loss: 0.0000000000 time: 0.03081345558166504\n",
      "Iteration: 5300 loss: 0.0000000000 time: 0.029047250747680664\n",
      "Iteration: 5310 loss: 0.0000000000 time: 0.03223824501037598\n",
      "Iteration: 5320 loss: 0.0000000000 time: 0.03085613250732422\n",
      "Iteration: 5330 loss: 0.0000000000 time: 0.031447410583496094\n",
      "Iteration: 5340 loss: 0.0000000000 time: 0.029888153076171875\n",
      "Iteration: 5350 loss: 0.0000000000 time: 0.03269529342651367\n",
      "Iteration: 5360 loss: 0.0000000000 time: 0.03342247009277344\n",
      "Iteration: 5370 loss: 0.0000000000 time: 0.032727718353271484\n",
      "Iteration: 5380 loss: 0.0000000000 time: 0.02883315086364746\n",
      "Iteration: 5390 loss: 0.0000000000 time: 0.03380632400512695\n",
      "Iteration: 5400 loss: 0.0000000000 time: 0.028271198272705078\n",
      "Iteration: 5410 loss: 0.0000000000 time: 0.027363300323486328\n",
      "Iteration: 5420 loss: 0.0000000000 time: 0.028948307037353516\n",
      "Iteration: 5430 loss: 0.0000000000 time: 0.03523087501525879\n",
      "Iteration: 5440 loss: 0.0000000000 time: 0.03102588653564453\n",
      "Iteration: 5450 loss: 0.0000000000 time: 0.028978824615478516\n",
      "Iteration: 5460 loss: 0.0000000000 time: 0.03211522102355957\n",
      "Iteration: 5470 loss: 0.0000000000 time: 0.028949737548828125\n",
      "Iteration: 5480 loss: 0.0000000000 time: 0.033948421478271484\n",
      "Iteration: 5490 loss: 0.0000000000 time: 0.03869748115539551\n",
      "Iteration: 5500 loss: 0.0000000000 time: 0.03582262992858887\n",
      "Iteration: 5510 loss: 0.0000000000 time: 0.03369259834289551\n",
      "Iteration: 5520 loss: 0.0000000000 time: 0.03453421592712402\n",
      "Iteration: 5530 loss: 0.0000000000 time: 0.03378582000732422\n",
      "Iteration: 5540 loss: 0.0000000000 time: 0.03245687484741211\n",
      "Iteration: 5550 loss: 0.0000000000 time: 0.03208732604980469\n",
      "Iteration: 5560 loss: 0.0000000000 time: 0.03591203689575195\n",
      "Iteration: 5570 loss: 0.0000000000 time: 0.03186631202697754\n",
      "Iteration: 5580 loss: 0.0000000000 time: 0.029334068298339844\n",
      "Iteration: 5590 loss: 0.0000000000 time: 0.028698205947875977\n",
      "Iteration: 5600 loss: 0.0000000000 time: 0.02672290802001953\n",
      "Iteration: 5610 loss: 0.0000000000 time: 0.031038522720336914\n",
      "Iteration: 5620 loss: 0.0000000000 time: 0.030511856079101562\n",
      "Iteration: 5630 loss: 0.0000000000 time: 0.033533334732055664\n",
      "Iteration: 5640 loss: 0.0000000000 time: 0.03312945365905762\n",
      "Iteration: 5650 loss: 0.0000000000 time: 0.03153705596923828\n",
      "Iteration: 5660 loss: 0.0000000000 time: 0.029897212982177734\n",
      "Iteration: 5670 loss: 0.0000000000 time: 0.029705047607421875\n",
      "Iteration: 5680 loss: 0.0000000000 time: 0.029750585556030273\n",
      "Iteration: 5690 loss: 0.0000000000 time: 0.03441286087036133\n",
      "Iteration: 5700 loss: 0.0000000000 time: 0.032949209213256836\n",
      "Iteration: 5710 loss: 0.0000000000 time: 0.02945256233215332\n",
      "Iteration: 5720 loss: 0.0000000000 time: 0.03204035758972168\n",
      "Iteration: 5730 loss: 0.0000000000 time: 0.032646894454956055\n",
      "Iteration: 5740 loss: 0.0000000000 time: 0.03148627281188965\n",
      "Iteration: 5750 loss: 0.0000000000 time: 0.029401540756225586\n",
      "Iteration: 5760 loss: 0.0000000000 time: 0.03345155715942383\n",
      "Iteration: 5770 loss: 0.0000000000 time: 0.0346529483795166\n",
      "Iteration: 5780 loss: 0.0000000000 time: 0.029459238052368164\n",
      "Iteration: 5790 loss: 0.0000000000 time: 0.03206276893615723\n",
      "Iteration: 5800 loss: 0.0000000000 time: 0.03241682052612305\n",
      "Iteration: 5810 loss: 0.0000000000 time: 0.029803991317749023\n",
      "Iteration: 5820 loss: 0.0000000000 time: 0.029825448989868164\n",
      "Iteration: 5830 loss: 0.0000000000 time: 0.03455638885498047\n",
      "Iteration: 5840 loss: 0.0000000000 time: 0.03147006034851074\n",
      "Iteration: 5850 loss: 0.0000000000 time: 0.03147459030151367\n",
      "Iteration: 5860 loss: 0.0000000000 time: 0.03360939025878906\n",
      "Iteration: 5870 loss: 0.0000000000 time: 0.034830331802368164\n",
      "Iteration: 5880 loss: 0.0000000000 time: 0.027359485626220703\n",
      "Iteration: 5890 loss: 0.0000000000 time: 0.03157210350036621\n",
      "Iteration: 5900 loss: 0.0000000000 time: 0.03502345085144043\n",
      "Iteration: 5910 loss: 0.0000000000 time: 0.03472614288330078\n",
      "Iteration: 5920 loss: 0.0000000000 time: 0.03164386749267578\n",
      "Iteration: 5930 loss: 0.0000000000 time: 0.031095266342163086\n",
      "Iteration: 5940 loss: 0.0000000000 time: 0.03181195259094238\n",
      "Iteration: 5950 loss: 0.0000000000 time: 0.02958202362060547\n",
      "Iteration: 5960 loss: 0.0000000000 time: 0.03026294708251953\n",
      "Iteration: 5970 loss: 0.0000000000 time: 0.03308749198913574\n",
      "Iteration: 5980 loss: 0.0000000000 time: 0.03728795051574707\n",
      "Iteration: 5990 loss: 0.0000000000 time: 0.03653264045715332\n",
      "Iteration: 6000 loss: 0.0000000000 time: 0.0313720703125\n",
      "Iteration: 6010 loss: 0.0000000000 time: 0.03579521179199219\n",
      "Iteration: 6020 loss: 0.0000000000 time: 0.029952287673950195\n",
      "Iteration: 6030 loss: 0.0000000000 time: 0.030334949493408203\n",
      "Iteration: 6040 loss: 0.0000000000 time: 0.031000852584838867\n",
      "Iteration: 6050 loss: 0.0000000000 time: 0.03371238708496094\n",
      "Iteration: 6060 loss: 0.0000000000 time: 0.030344486236572266\n",
      "Iteration: 6070 loss: 0.0000000000 time: 0.031087875366210938\n",
      "Iteration: 6080 loss: 0.0000000000 time: 0.028861045837402344\n",
      "Iteration: 6090 loss: 0.0000000000 time: 0.032354116439819336\n",
      "Iteration: 6100 loss: 0.0000000000 time: 0.03201723098754883\n",
      "Iteration: 6110 loss: 0.0000000000 time: 0.03670930862426758\n",
      "Iteration: 6120 loss: 0.0000000000 time: 0.02951502799987793\n",
      "Iteration: 6130 loss: 0.0000000000 time: 0.030380725860595703\n",
      "Iteration: 6140 loss: 0.0000000000 time: 0.031047344207763672\n",
      "Iteration: 6150 loss: 0.0000000000 time: 0.03671407699584961\n",
      "Iteration: 6160 loss: 0.0000000000 time: 0.03248953819274902\n",
      "Iteration: 6170 loss: 0.0000000000 time: 0.03758502006530762\n",
      "Iteration: 6180 loss: 0.0000000000 time: 0.03473615646362305\n",
      "Iteration: 6190 loss: 0.0000000000 time: 0.03159189224243164\n",
      "Iteration: 6200 loss: 0.0000000000 time: 0.03212904930114746\n",
      "Iteration: 6210 loss: 0.0000000000 time: 0.03156089782714844\n",
      "Iteration: 6220 loss: 0.0000000000 time: 0.028432130813598633\n",
      "Iteration: 6230 loss: 0.0000000000 time: 0.028635740280151367\n",
      "Iteration: 6240 loss: 0.0000000000 time: 0.028498172760009766\n",
      "Iteration: 6250 loss: 0.0000000000 time: 0.03076338768005371\n",
      "Iteration: 6260 loss: 0.0000000000 time: 0.02898263931274414\n",
      "Iteration: 6270 loss: 0.0000000000 time: 0.0275266170501709\n",
      "Iteration: 6280 loss: 0.0000000000 time: 0.03251981735229492\n",
      "Iteration: 6290 loss: 0.0000000000 time: 0.03571128845214844\n",
      "Iteration: 6300 loss: 0.0000000000 time: 0.031256675720214844\n",
      "Iteration: 6310 loss: 0.0000000000 time: 0.03058314323425293\n",
      "Iteration: 6320 loss: 0.0000000000 time: 0.033568382263183594\n",
      "Iteration: 6330 loss: 0.0000000000 time: 0.034616947174072266\n",
      "Iteration: 6340 loss: 0.0000000000 time: 0.0324249267578125\n",
      "Iteration: 6350 loss: 0.0000000000 time: 0.03157186508178711\n",
      "Iteration: 6360 loss: 0.0000000000 time: 0.03041672706604004\n",
      "Iteration: 6370 loss: 0.0000000000 time: 0.03109574317932129\n",
      "Iteration: 6380 loss: 0.0000000000 time: 0.03289651870727539\n",
      "Iteration: 6390 loss: 0.0000000000 time: 0.033078908920288086\n",
      "Iteration: 6400 loss: 0.0000000004 time: 0.02970147132873535\n",
      "Iteration: 6410 loss: 0.0000009285 time: 0.026912212371826172\n",
      "Iteration: 6420 loss: 0.0004452531 time: 0.02966022491455078\n",
      "Iteration: 6430 loss: 0.0000033322 time: 0.03157806396484375\n",
      "Iteration: 6440 loss: 0.0000002563 time: 0.031189680099487305\n",
      "Iteration: 6450 loss: 0.0000001882 time: 0.029790401458740234\n",
      "Iteration: 6460 loss: 0.0000007730 time: 0.03655052185058594\n",
      "Iteration: 6470 loss: 0.0000006979 time: 0.029573917388916016\n",
      "Iteration: 6480 loss: 0.0000003775 time: 0.030405521392822266\n",
      "Iteration: 6490 loss: 0.0000001595 time: 0.03485703468322754\n",
      "Iteration: 6500 loss: 0.0000000588 time: 0.030820608139038086\n",
      "Iteration: 6510 loss: 0.0000000193 time: 0.02791738510131836\n",
      "Iteration: 6520 loss: 0.0000000053 time: 0.031351327896118164\n",
      "Iteration: 6530 loss: 0.0000000010 time: 0.03533601760864258\n",
      "Iteration: 6540 loss: 0.0000000001 time: 0.029929161071777344\n",
      "Iteration: 6550 loss: 0.0000000000 time: 0.030869483947753906\n",
      "Iteration: 6560 loss: 0.0000000001 time: 0.02839040756225586\n",
      "Iteration: 6570 loss: 0.0000000001 time: 0.030953407287597656\n",
      "Iteration: 6580 loss: 0.0000000000 time: 0.03251361846923828\n",
      "Iteration: 6590 loss: 0.0000000000 time: 0.03443193435668945\n",
      "Iteration: 6600 loss: 0.0000000000 time: 0.03336048126220703\n",
      "Iteration: 6610 loss: 0.0000000000 time: 0.032209157943725586\n",
      "Iteration: 6620 loss: 0.0000000000 time: 0.0295565128326416\n",
      "Iteration: 6630 loss: 0.0000000000 time: 0.02749919891357422\n",
      "Iteration: 6640 loss: 0.0000000000 time: 0.026741981506347656\n",
      "Iteration: 6650 loss: 0.0000000000 time: 0.03491020202636719\n",
      "Iteration: 6660 loss: 0.0000000000 time: 0.03574681282043457\n",
      "Iteration: 6670 loss: 0.0000000000 time: 0.033498525619506836\n",
      "Iteration: 6680 loss: 0.0000000000 time: 0.03133082389831543\n",
      "Iteration: 6690 loss: 0.0000000000 time: 0.028983116149902344\n",
      "Iteration: 6700 loss: 0.0000000000 time: 0.03109121322631836\n",
      "Iteration: 6710 loss: 0.0000000000 time: 0.03370785713195801\n",
      "Iteration: 6720 loss: 0.0000000000 time: 0.032372474670410156\n",
      "Iteration: 6730 loss: 0.0000000000 time: 0.03503584861755371\n",
      "Iteration: 6740 loss: 0.0000000000 time: 0.03105020523071289\n",
      "Iteration: 6750 loss: 0.0000000000 time: 0.030915260314941406\n",
      "Iteration: 6760 loss: 0.0000000000 time: 0.03196144104003906\n",
      "Iteration: 6770 loss: 0.0000000000 time: 0.03242039680480957\n",
      "Iteration: 6780 loss: 0.0000000000 time: 0.03232216835021973\n",
      "Iteration: 6790 loss: 0.0000000000 time: 0.03145003318786621\n",
      "Iteration: 6800 loss: 0.0000000000 time: 0.03400874137878418\n",
      "Iteration: 6810 loss: 0.0000000000 time: 0.043274879455566406\n",
      "Iteration: 6820 loss: 0.0000000000 time: 0.03266096115112305\n",
      "Iteration: 6830 loss: 0.0000000000 time: 0.03389406204223633\n",
      "Iteration: 6840 loss: 0.0000000000 time: 0.039885759353637695\n",
      "Iteration: 6850 loss: 0.0000000000 time: 0.03325247764587402\n",
      "Iteration: 6860 loss: 0.0000000000 time: 0.03283500671386719\n",
      "Iteration: 6870 loss: 0.0000000000 time: 0.03452777862548828\n",
      "Iteration: 6880 loss: 0.0000000000 time: 0.03197312355041504\n",
      "Iteration: 6890 loss: 0.0000000000 time: 0.03069138526916504\n",
      "Iteration: 6900 loss: 0.0000000000 time: 0.03152751922607422\n",
      "Iteration: 6910 loss: 0.0000000000 time: 0.03103804588317871\n",
      "Iteration: 6920 loss: 0.0000000000 time: 0.03154587745666504\n",
      "Iteration: 6930 loss: 0.0000000000 time: 0.030203580856323242\n",
      "Iteration: 6940 loss: 0.0000000000 time: 0.03183245658874512\n",
      "Iteration: 6950 loss: 0.0000000000 time: 0.030563831329345703\n",
      "Iteration: 6960 loss: 0.0000000000 time: 0.036364078521728516\n",
      "Iteration: 6970 loss: 0.0000000000 time: 0.029747724533081055\n",
      "Iteration: 6980 loss: 0.0000000000 time: 0.030724525451660156\n",
      "Iteration: 6990 loss: 0.0000000000 time: 0.034356117248535156\n",
      "Iteration: 7000 loss: 0.0000000000 time: 0.03587031364440918\n",
      "Iteration: 7010 loss: 0.0000000000 time: 0.030058622360229492\n",
      "Iteration: 7020 loss: 0.0000000000 time: 0.03392839431762695\n",
      "Iteration: 7030 loss: 0.0000000000 time: 0.03332710266113281\n",
      "Iteration: 7040 loss: 0.0000000000 time: 0.03321123123168945\n",
      "Iteration: 7050 loss: 0.0000000000 time: 0.032219886779785156\n",
      "Iteration: 7060 loss: 0.0000000213 time: 0.030933856964111328\n",
      "Iteration: 7070 loss: 0.0000674957 time: 0.0320439338684082\n",
      "Iteration: 7080 loss: 0.0000447467 time: 0.030059814453125\n",
      "Iteration: 7090 loss: 0.0000210215 time: 0.027964115142822266\n",
      "Iteration: 7100 loss: 0.0000007960 time: 0.02826690673828125\n",
      "Iteration: 7110 loss: 0.0000078157 time: 0.03570151329040527\n",
      "Iteration: 7120 loss: 0.0000021927 time: 0.029783964157104492\n",
      "Iteration: 7130 loss: 0.0000000028 time: 0.031859397888183594\n",
      "Iteration: 7140 loss: 0.0000002855 time: 0.03279519081115723\n",
      "Iteration: 7150 loss: 0.0000001335 time: 0.03218269348144531\n",
      "Iteration: 7160 loss: 0.0000000125 time: 0.028940200805664062\n",
      "Iteration: 7170 loss: 0.0000000005 time: 0.0316159725189209\n",
      "Iteration: 7180 loss: 0.0000000030 time: 0.03456521034240723\n",
      "Iteration: 7190 loss: 0.0000000020 time: 0.030202627182006836\n",
      "Iteration: 7200 loss: 0.0000000008 time: 0.03368353843688965\n",
      "Iteration: 7210 loss: 0.0000000002 time: 0.03658318519592285\n",
      "Iteration: 7220 loss: 0.0000000001 time: 0.029863595962524414\n",
      "Iteration: 7230 loss: 0.0000000000 time: 0.02860116958618164\n",
      "Iteration: 7240 loss: 0.0000000000 time: 0.029129743576049805\n",
      "Iteration: 7250 loss: 0.0000000000 time: 0.031714677810668945\n",
      "Iteration: 7260 loss: 0.0000000000 time: 0.03360438346862793\n",
      "Iteration: 7270 loss: 0.0000000000 time: 0.037914276123046875\n",
      "Iteration: 7280 loss: 0.0000000000 time: 0.03409695625305176\n",
      "Iteration: 7290 loss: 0.0000000000 time: 0.03371596336364746\n",
      "Iteration: 7300 loss: 0.0000000000 time: 0.035303592681884766\n",
      "Iteration: 7310 loss: 0.0000000000 time: 0.03271961212158203\n",
      "Iteration: 7320 loss: 0.0000000000 time: 0.033446311950683594\n",
      "Iteration: 7330 loss: 0.0000000000 time: 0.03334307670593262\n",
      "Iteration: 7340 loss: 0.0000000000 time: 0.03052687644958496\n",
      "Iteration: 7350 loss: 0.0000000000 time: 0.038802146911621094\n",
      "Iteration: 7360 loss: 0.0000000000 time: 0.03197312355041504\n",
      "Iteration: 7370 loss: 0.0000000000 time: 0.033441781997680664\n",
      "Iteration: 7380 loss: 0.0000000000 time: 0.02975320816040039\n",
      "Iteration: 7390 loss: 0.0000000000 time: 0.02821946144104004\n",
      "Iteration: 7400 loss: 0.0000000000 time: 0.030376434326171875\n",
      "Iteration: 7410 loss: 0.0000000000 time: 0.03303861618041992\n",
      "Iteration: 7420 loss: 0.0000000000 time: 0.03214526176452637\n",
      "Iteration: 7430 loss: 0.0000000000 time: 0.04155111312866211\n",
      "Iteration: 7440 loss: 0.0000000000 time: 0.031093835830688477\n",
      "Iteration: 7450 loss: 0.0000000000 time: 0.029502391815185547\n",
      "Iteration: 7460 loss: 0.0000000000 time: 0.03148341178894043\n",
      "Iteration: 7470 loss: 0.0000000000 time: 0.03889584541320801\n",
      "Iteration: 7480 loss: 0.0000000000 time: 0.03713083267211914\n",
      "Iteration: 7490 loss: 0.0000000000 time: 0.03901410102844238\n",
      "Iteration: 7500 loss: 0.0000000000 time: 0.03483319282531738\n",
      "Iteration: 7510 loss: 0.0000000000 time: 0.0318450927734375\n",
      "Iteration: 7520 loss: 0.0000000000 time: 0.03200364112854004\n",
      "Iteration: 7530 loss: 0.0000000000 time: 0.0321199893951416\n",
      "Iteration: 7540 loss: 0.0000000000 time: 0.03146958351135254\n",
      "Iteration: 7550 loss: 0.0000000000 time: 0.03631591796875\n",
      "Iteration: 7560 loss: 0.0000000000 time: 0.032712459564208984\n",
      "Iteration: 7570 loss: 0.0000000000 time: 0.031006336212158203\n",
      "Iteration: 7580 loss: 0.0000000000 time: 0.029059410095214844\n",
      "Iteration: 7590 loss: 0.0000000000 time: 0.030763864517211914\n",
      "Iteration: 7600 loss: 0.0000000000 time: 0.028675317764282227\n",
      "Iteration: 7610 loss: 0.0000000000 time: 0.0345458984375\n",
      "Iteration: 7620 loss: 0.0000000000 time: 0.032928466796875\n",
      "Iteration: 7630 loss: 0.0000000000 time: 0.032800912857055664\n",
      "Iteration: 7640 loss: 0.0000000000 time: 0.029025793075561523\n",
      "Iteration: 7650 loss: 0.0000000000 time: 0.030617952346801758\n",
      "Iteration: 7660 loss: 0.0000000000 time: 0.029091358184814453\n",
      "Iteration: 7670 loss: 0.0000000000 time: 0.03612494468688965\n",
      "Iteration: 7680 loss: 0.0000000000 time: 0.032713890075683594\n",
      "Iteration: 7690 loss: 0.0000000000 time: 0.03259468078613281\n",
      "Iteration: 7700 loss: 0.0000000000 time: 0.028980731964111328\n",
      "Iteration: 7710 loss: 0.0000000000 time: 0.03105330467224121\n",
      "Iteration: 7720 loss: 0.0000000000 time: 0.029708147048950195\n",
      "Iteration: 7730 loss: 0.0000000000 time: 0.02939581871032715\n",
      "Iteration: 7740 loss: 0.0000000000 time: 0.032273054122924805\n",
      "Iteration: 7750 loss: 0.0000000000 time: 0.03366804122924805\n",
      "Iteration: 7760 loss: 0.0000000000 time: 0.04016590118408203\n",
      "Iteration: 7770 loss: 0.0000000000 time: 0.038596153259277344\n",
      "Iteration: 7780 loss: 0.0000000000 time: 0.04164528846740723\n",
      "Iteration: 7790 loss: 0.0000000014 time: 0.03188061714172363\n",
      "Iteration: 7800 loss: 0.0000029607 time: 0.028435707092285156\n",
      "Iteration: 7810 loss: 0.0000506110 time: 0.030361413955688477\n",
      "Iteration: 7820 loss: 0.0001009581 time: 0.03133893013000488\n",
      "Iteration: 7830 loss: 0.0000237484 time: 0.028057575225830078\n",
      "Iteration: 7840 loss: 0.0000002843 time: 0.02901148796081543\n",
      "Iteration: 7850 loss: 0.0000020839 time: 0.029064655303955078\n",
      "Iteration: 7860 loss: 0.0000014729 time: 0.02785634994506836\n",
      "Iteration: 7870 loss: 0.0000001569 time: 0.03032374382019043\n",
      "Iteration: 7880 loss: 0.0000000082 time: 0.03291773796081543\n",
      "Iteration: 7890 loss: 0.0000000411 time: 0.037210702896118164\n",
      "Iteration: 7900 loss: 0.0000000229 time: 0.03684377670288086\n",
      "Iteration: 7910 loss: 0.0000000063 time: 0.03441166877746582\n",
      "Iteration: 7920 loss: 0.0000000010 time: 0.029434919357299805\n",
      "Iteration: 7930 loss: 0.0000000001 time: 0.028249025344848633\n",
      "Iteration: 7940 loss: 0.0000000000 time: 0.027036428451538086\n",
      "Iteration: 7950 loss: 0.0000000000 time: 0.03448963165283203\n",
      "Iteration: 7960 loss: 0.0000000000 time: 0.03142523765563965\n",
      "Iteration: 7970 loss: 0.0000000000 time: 0.029886484146118164\n",
      "Iteration: 7980 loss: 0.0000000000 time: 0.03455853462219238\n",
      "Iteration: 7990 loss: 0.0000000000 time: 0.028707504272460938\n",
      "Iteration: 8000 loss: 0.0000000000 time: 0.029185056686401367\n",
      "Iteration: 8010 loss: 0.0000000000 time: 0.032453060150146484\n",
      "Iteration: 8020 loss: 0.0000000000 time: 0.03320813179016113\n",
      "Iteration: 8030 loss: 0.0000000000 time: 0.03432917594909668\n",
      "Iteration: 8040 loss: 0.0000000000 time: 0.029297590255737305\n",
      "Iteration: 8050 loss: 0.0000000000 time: 0.034398555755615234\n",
      "Iteration: 8060 loss: 0.0000000000 time: 0.03408694267272949\n",
      "Iteration: 8070 loss: 0.0000000000 time: 0.029354333877563477\n",
      "Iteration: 8080 loss: 0.0000000000 time: 0.03168320655822754\n",
      "Iteration: 8090 loss: 0.0000000000 time: 0.03656506538391113\n",
      "Iteration: 8100 loss: 0.0000000000 time: 0.029951810836791992\n",
      "Iteration: 8110 loss: 0.0000000000 time: 0.030442476272583008\n",
      "Iteration: 8120 loss: 0.0000000000 time: 0.02780890464782715\n",
      "Iteration: 8130 loss: 0.0000000000 time: 0.02869725227355957\n",
      "Iteration: 8140 loss: 0.0000000000 time: 0.037999868392944336\n",
      "Iteration: 8150 loss: 0.0000000000 time: 0.03544259071350098\n",
      "Iteration: 8160 loss: 0.0000000000 time: 0.033873558044433594\n",
      "Iteration: 8170 loss: 0.0000000000 time: 0.02943730354309082\n",
      "Iteration: 8180 loss: 0.0000000000 time: 0.02836132049560547\n",
      "Iteration: 8190 loss: 0.0000000000 time: 0.028888702392578125\n",
      "Iteration: 8200 loss: 0.0000000000 time: 0.028389692306518555\n",
      "Iteration: 8210 loss: 0.0000000000 time: 0.0287168025970459\n",
      "Iteration: 8220 loss: 0.0000000000 time: 0.031072616577148438\n",
      "Iteration: 8230 loss: 0.0000000000 time: 0.03099679946899414\n",
      "Iteration: 8240 loss: 0.0000000000 time: 0.03314614295959473\n",
      "Iteration: 8250 loss: 0.0000000000 time: 0.029493093490600586\n",
      "Iteration: 8260 loss: 0.0000000000 time: 0.03202009201049805\n",
      "Iteration: 8270 loss: 0.0000000000 time: 0.029250621795654297\n",
      "Iteration: 8280 loss: 0.0000000000 time: 0.029022693634033203\n",
      "Iteration: 8290 loss: 0.0000000000 time: 0.03223395347595215\n",
      "Iteration: 8300 loss: 0.0000000000 time: 0.030343055725097656\n",
      "Iteration: 8310 loss: 0.0000000000 time: 0.030574560165405273\n",
      "Iteration: 8320 loss: 0.0000000000 time: 0.029532194137573242\n",
      "Iteration: 8330 loss: 0.0000000000 time: 0.029533863067626953\n",
      "Iteration: 8340 loss: 0.0000000000 time: 0.02812504768371582\n",
      "Iteration: 8350 loss: 0.0000000000 time: 0.031055212020874023\n",
      "Iteration: 8360 loss: 0.0000000000 time: 0.030481815338134766\n",
      "Iteration: 8370 loss: 0.0000000000 time: 0.03309965133666992\n",
      "Iteration: 8380 loss: 0.0000000000 time: 0.034768104553222656\n",
      "Iteration: 8390 loss: 0.0000000000 time: 0.030946016311645508\n",
      "Iteration: 8400 loss: 0.0000000000 time: 0.029184818267822266\n",
      "Iteration: 8410 loss: 0.0000000000 time: 0.026872634887695312\n",
      "Iteration: 8420 loss: 0.0000000000 time: 0.032097578048706055\n",
      "Iteration: 8430 loss: 0.0000000000 time: 0.034809112548828125\n",
      "Iteration: 8440 loss: 0.0000000000 time: 0.03194689750671387\n",
      "Iteration: 8450 loss: 0.0000000000 time: 0.030251264572143555\n",
      "Iteration: 8460 loss: 0.0000000000 time: 0.0325925350189209\n",
      "Iteration: 8470 loss: 0.0000000000 time: 0.02911663055419922\n",
      "Iteration: 8480 loss: 0.0000000000 time: 0.030575037002563477\n",
      "Iteration: 8490 loss: 0.0000000000 time: 0.03362298011779785\n",
      "Iteration: 8500 loss: 0.0000000000 time: 0.029632568359375\n",
      "Iteration: 8510 loss: 0.0000000000 time: 0.03340744972229004\n",
      "Iteration: 8520 loss: 0.0000000763 time: 0.03049302101135254\n",
      "Iteration: 8530 loss: 0.0001904597 time: 0.030675172805786133\n",
      "Iteration: 8540 loss: 0.0001144566 time: 0.03401327133178711\n",
      "Iteration: 8550 loss: 0.0000424247 time: 0.03147554397583008\n",
      "Iteration: 8560 loss: 0.0000147939 time: 0.030291080474853516\n",
      "Iteration: 8570 loss: 0.0000000388 time: 0.029730796813964844\n",
      "Iteration: 8580 loss: 0.0000015595 time: 0.034752607345581055\n",
      "Iteration: 8590 loss: 0.0000005197 time: 0.031334638595581055\n",
      "Iteration: 8600 loss: 0.0000000001 time: 0.027814149856567383\n",
      "Iteration: 8610 loss: 0.0000000604 time: 0.03169655799865723\n",
      "Iteration: 8620 loss: 0.0000000298 time: 0.029403209686279297\n",
      "Iteration: 8630 loss: 0.0000000031 time: 0.033396005630493164\n",
      "Iteration: 8640 loss: 0.0000000001 time: 0.032129764556884766\n",
      "Iteration: 8650 loss: 0.0000000006 time: 0.03423666954040527\n",
      "Iteration: 8660 loss: 0.0000000004 time: 0.03268694877624512\n",
      "Iteration: 8670 loss: 0.0000000002 time: 0.03028726577758789\n",
      "Iteration: 8680 loss: 0.0000000001 time: 0.030886411666870117\n",
      "Iteration: 8690 loss: 0.0000000000 time: 0.033502817153930664\n",
      "Iteration: 8700 loss: 0.0000000000 time: 0.033895254135131836\n",
      "Iteration: 8710 loss: 0.0000000000 time: 0.03068399429321289\n",
      "Iteration: 8720 loss: 0.0000000000 time: 0.03319597244262695\n",
      "Iteration: 8730 loss: 0.0000000000 time: 0.037067413330078125\n",
      "Iteration: 8740 loss: 0.0000000000 time: 0.02906966209411621\n",
      "Iteration: 8750 loss: 0.0000000000 time: 0.03151679039001465\n",
      "Iteration: 8760 loss: 0.0000000000 time: 0.03213644027709961\n",
      "Iteration: 8770 loss: 0.0000000000 time: 0.028738021850585938\n",
      "Iteration: 8780 loss: 0.0000000000 time: 0.029938220977783203\n",
      "Iteration: 8790 loss: 0.0000000000 time: 0.033151865005493164\n",
      "Iteration: 8800 loss: 0.0000000000 time: 0.03525686264038086\n",
      "Iteration: 8810 loss: 0.0000000000 time: 0.03224468231201172\n",
      "Iteration: 8820 loss: 0.0000000000 time: 0.03364157676696777\n",
      "Iteration: 8830 loss: 0.0000000000 time: 0.03401780128479004\n",
      "Iteration: 8840 loss: 0.0000000000 time: 0.0385282039642334\n",
      "Iteration: 8850 loss: 0.0000000000 time: 0.033747196197509766\n",
      "Iteration: 8860 loss: 0.0000000000 time: 0.03297281265258789\n",
      "Iteration: 8870 loss: 0.0000000000 time: 0.03136157989501953\n",
      "Iteration: 8880 loss: 0.0000000000 time: 0.030635595321655273\n",
      "Iteration: 8890 loss: 0.0000000000 time: 0.0301363468170166\n",
      "Iteration: 8900 loss: 0.0000000000 time: 0.030282258987426758\n",
      "Iteration: 8910 loss: 0.0000000000 time: 0.029552936553955078\n",
      "Iteration: 8920 loss: 0.0000000000 time: 0.03383779525756836\n",
      "Iteration: 8930 loss: 0.0000000000 time: 0.033593177795410156\n",
      "Iteration: 8940 loss: 0.0000000000 time: 0.027823448181152344\n",
      "Iteration: 8950 loss: 0.0000000000 time: 0.02935624122619629\n",
      "Iteration: 8960 loss: 0.0000000000 time: 0.03175497055053711\n",
      "Iteration: 8970 loss: 0.0000000000 time: 0.026839017868041992\n",
      "Iteration: 8980 loss: 0.0000000000 time: 0.029819250106811523\n",
      "Iteration: 8990 loss: 0.0000000000 time: 0.03297305107116699\n",
      "Iteration: 9000 loss: 0.0000000000 time: 0.03367185592651367\n",
      "Iteration: 9010 loss: 0.0000000000 time: 0.03322124481201172\n",
      "Iteration: 9020 loss: 0.0000000000 time: 0.03389263153076172\n",
      "Iteration: 9030 loss: 0.0000000000 time: 0.030635833740234375\n",
      "Iteration: 9040 loss: 0.0000000000 time: 0.027902841567993164\n",
      "Iteration: 9050 loss: 0.0000000000 time: 0.02919769287109375\n",
      "Iteration: 9060 loss: 0.0000000000 time: 0.03125190734863281\n",
      "Iteration: 9070 loss: 0.0000000000 time: 0.03263282775878906\n",
      "Iteration: 9080 loss: 0.0000000000 time: 0.02936267852783203\n",
      "Iteration: 9090 loss: 0.0000000000 time: 0.02987837791442871\n",
      "Iteration: 9100 loss: 0.0000000000 time: 0.030176639556884766\n",
      "Iteration: 9110 loss: 0.0000000000 time: 0.02991342544555664\n",
      "Iteration: 9120 loss: 0.0000000000 time: 0.031000852584838867\n",
      "Iteration: 9130 loss: 0.0000000000 time: 0.0311279296875\n",
      "Iteration: 9140 loss: 0.0000000000 time: 0.03203129768371582\n",
      "Iteration: 9150 loss: 0.0000000000 time: 0.03331565856933594\n",
      "Iteration: 9160 loss: 0.0000000000 time: 0.031916141510009766\n",
      "Iteration: 9170 loss: 0.0000000000 time: 0.03639578819274902\n",
      "Iteration: 9180 loss: 0.0000000000 time: 0.032423973083496094\n",
      "Iteration: 9190 loss: 0.0000000000 time: 0.033392906188964844\n",
      "Iteration: 9200 loss: 0.0000000000 time: 0.03269147872924805\n",
      "Iteration: 9210 loss: 0.0000000000 time: 0.030248641967773438\n",
      "Iteration: 9220 loss: 0.0000000000 time: 0.03317141532897949\n",
      "Iteration: 9230 loss: 0.0000000000 time: 0.030775070190429688\n",
      "Iteration: 9240 loss: 0.0000000000 time: 0.02994823455810547\n",
      "Iteration: 9250 loss: 0.0000000000 time: 0.031195878982543945\n",
      "Iteration: 9260 loss: 0.0000000000 time: 0.0335848331451416\n",
      "Iteration: 9270 loss: 0.0000000008 time: 0.03040623664855957\n",
      "Iteration: 9280 loss: 0.0000027488 time: 0.0324246883392334\n",
      "Iteration: 9290 loss: 0.0000000030 time: 0.030412912368774414\n",
      "Iteration: 9300 loss: 0.0000129817 time: 0.03096747398376465\n",
      "Iteration: 9310 loss: 0.0000117308 time: 0.032758235931396484\n",
      "Iteration: 9320 loss: 0.0000077863 time: 0.029837846755981445\n",
      "Iteration: 9330 loss: 0.0000011511 time: 0.03456759452819824\n",
      "Iteration: 9340 loss: 0.0000008896 time: 0.03695178031921387\n",
      "Iteration: 9350 loss: 0.0000001436 time: 0.03531312942504883\n",
      "Iteration: 9360 loss: 0.0000001256 time: 0.03139352798461914\n",
      "Iteration: 9370 loss: 0.0000000062 time: 0.03285622596740723\n",
      "Iteration: 9380 loss: 0.0000000201 time: 0.028107881546020508\n",
      "Iteration: 9390 loss: 0.0000000004 time: 0.03157782554626465\n",
      "Iteration: 9400 loss: 0.0000000016 time: 0.029606103897094727\n",
      "Iteration: 9410 loss: 0.0000000007 time: 0.030803442001342773\n",
      "Iteration: 9420 loss: 0.0000000000 time: 0.03348684310913086\n",
      "Iteration: 9430 loss: 0.0000000000 time: 0.03046417236328125\n",
      "Iteration: 9440 loss: 0.0000000000 time: 0.029987096786499023\n",
      "Iteration: 9450 loss: 0.0000000000 time: 0.03350949287414551\n",
      "Iteration: 9460 loss: 0.0000000000 time: 0.030583858489990234\n",
      "Iteration: 9470 loss: 0.0000000000 time: 0.03246140480041504\n",
      "Iteration: 9480 loss: 0.0000000000 time: 0.03661036491394043\n",
      "Iteration: 9490 loss: 0.0000000000 time: 0.037744998931884766\n",
      "Iteration: 9500 loss: 0.0000000000 time: 0.03731131553649902\n",
      "Iteration: 9510 loss: 0.0000000000 time: 0.03838968276977539\n",
      "Iteration: 9520 loss: 0.0000000000 time: 0.03635001182556152\n",
      "Iteration: 9530 loss: 0.0000000000 time: 0.03473162651062012\n",
      "Iteration: 9540 loss: 0.0000000000 time: 0.03899192810058594\n",
      "Iteration: 9550 loss: 0.0000000000 time: 0.036658525466918945\n",
      "Iteration: 9560 loss: 0.0000000000 time: 0.0346987247467041\n",
      "Iteration: 9570 loss: 0.0000000000 time: 0.035369157791137695\n",
      "Iteration: 9580 loss: 0.0000000000 time: 0.03565859794616699\n",
      "Iteration: 9590 loss: 0.0000000000 time: 0.03443503379821777\n",
      "Iteration: 9600 loss: 0.0000000000 time: 0.035552978515625\n",
      "Iteration: 9610 loss: 0.0000000000 time: 0.0336606502532959\n",
      "Iteration: 9620 loss: 0.0000000000 time: 0.02821207046508789\n",
      "Iteration: 9630 loss: 0.0000000000 time: 0.0367434024810791\n",
      "Iteration: 9640 loss: 0.0000000000 time: 0.0340886116027832\n",
      "Iteration: 9650 loss: 0.0000000000 time: 0.033327341079711914\n",
      "Iteration: 9660 loss: 0.0000000000 time: 0.03411459922790527\n",
      "Iteration: 9670 loss: 0.0000000000 time: 0.035324811935424805\n",
      "Iteration: 9680 loss: 0.0000000000 time: 0.032850027084350586\n",
      "Iteration: 9690 loss: 0.0000000000 time: 0.029007673263549805\n",
      "Iteration: 9700 loss: 0.0000000000 time: 0.032117605209350586\n",
      "Iteration: 9710 loss: 0.0000000000 time: 0.029711246490478516\n",
      "Iteration: 9720 loss: 0.0000000000 time: 0.0307157039642334\n",
      "Iteration: 9730 loss: 0.0000000000 time: 0.032093048095703125\n",
      "Iteration: 9740 loss: 0.0000000000 time: 0.03283500671386719\n",
      "Iteration: 9750 loss: 0.0000000000 time: 0.033901214599609375\n",
      "Iteration: 9760 loss: 0.0000000000 time: 0.03338623046875\n",
      "Iteration: 9770 loss: 0.0000000000 time: 0.031052827835083008\n",
      "Iteration: 9780 loss: 0.0000000000 time: 0.03392505645751953\n",
      "Iteration: 9790 loss: 0.0000000000 time: 0.03415989875793457\n",
      "Iteration: 9800 loss: 0.0000000000 time: 0.032132625579833984\n",
      "Iteration: 9810 loss: 0.0000000000 time: 0.03561997413635254\n",
      "Iteration: 9820 loss: 0.0000000000 time: 0.037622690200805664\n",
      "Iteration: 9830 loss: 0.0000000000 time: 0.029297828674316406\n",
      "Iteration: 9840 loss: 0.0000000000 time: 0.028400897979736328\n",
      "Iteration: 9850 loss: 0.0000000000 time: 0.028529882431030273\n",
      "Iteration: 9860 loss: 0.0000000000 time: 0.029024124145507812\n",
      "Iteration: 9870 loss: 0.0000000000 time: 0.029183387756347656\n",
      "Iteration: 9880 loss: 0.0000000000 time: 0.02954554557800293\n",
      "Iteration: 9890 loss: 0.0000000000 time: 0.03270077705383301\n",
      "Iteration: 9900 loss: 0.0000000000 time: 0.03242206573486328\n",
      "Iteration: 9910 loss: 0.0000000000 time: 0.02914261817932129\n",
      "Iteration: 9920 loss: 0.0000000000 time: 0.03173208236694336\n",
      "Iteration: 9930 loss: 0.0000000000 time: 0.030592918395996094\n",
      "Iteration: 9940 loss: 0.0000000000 time: 0.035028696060180664\n",
      "Iteration: 9950 loss: 0.0000000000 time: 0.038019418716430664\n",
      "Iteration: 9960 loss: 0.0000000000 time: 0.035068511962890625\n",
      "Iteration: 9970 loss: 0.0000000000 time: 0.029992341995239258\n",
      "Iteration: 9980 loss: 0.0000000000 time: 0.031426191329956055\n",
      "Iteration: 9990 loss: 0.0000000000 time: 0.030369997024536133\n",
      "Iteration: 10000 loss: 0.0000000000 time: 0.03120589256286621\n",
      "Iteration: 10010 loss: 0.0000000000 time: 0.029881000518798828\n",
      "Iteration: 10020 loss: 0.0000000000 time: 0.032137393951416016\n",
      "Iteration: 10030 loss: 0.0000000000 time: 0.032682180404663086\n",
      "Iteration: 10040 loss: 0.0000000000 time: 0.03031325340270996\n",
      "Iteration: 10050 loss: 0.0000000000 time: 0.02998208999633789\n",
      "Iteration: 10060 loss: 0.0000000000 time: 0.03397011756896973\n",
      "Iteration: 10070 loss: 0.0000000000 time: 0.03126192092895508\n",
      "Iteration: 10080 loss: 0.0000000062 time: 0.029909133911132812\n",
      "Iteration: 10090 loss: 0.0000177315 time: 0.030625581741333008\n",
      "Iteration: 10100 loss: 0.0001873833 time: 0.03703594207763672\n",
      "Iteration: 10110 loss: 0.0000593007 time: 0.029172658920288086\n",
      "Iteration: 10120 loss: 0.0000089601 time: 0.030979633331298828\n",
      "Iteration: 10130 loss: 0.0000010308 time: 0.03650379180908203\n",
      "Iteration: 10140 loss: 0.0000023111 time: 0.035989999771118164\n",
      "Iteration: 10150 loss: 0.0000000763 time: 0.03147602081298828\n",
      "Iteration: 10160 loss: 0.0000001561 time: 0.03923845291137695\n",
      "Iteration: 10170 loss: 0.0000000911 time: 0.029108762741088867\n",
      "Iteration: 10180 loss: 0.0000000025 time: 0.028692960739135742\n",
      "Iteration: 10190 loss: 0.0000000043 time: 0.031094789505004883\n",
      "Iteration: 10200 loss: 0.0000000043 time: 0.031676530838012695\n",
      "Iteration: 10210 loss: 0.0000000011 time: 0.03133583068847656\n",
      "Iteration: 10220 loss: 0.0000000001 time: 0.034348487854003906\n",
      "Iteration: 10230 loss: 0.0000000000 time: 0.031212568283081055\n",
      "Iteration: 10240 loss: 0.0000000000 time: 0.02940201759338379\n",
      "Iteration: 10250 loss: 0.0000000000 time: 0.03432869911193848\n",
      "Iteration: 10260 loss: 0.0000000000 time: 0.033438920974731445\n",
      "Iteration: 10270 loss: 0.0000000000 time: 0.03122687339782715\n",
      "Iteration: 10280 loss: 0.0000000000 time: 0.030739545822143555\n",
      "Iteration: 10290 loss: 0.0000000000 time: 0.037158966064453125\n",
      "Iteration: 10300 loss: 0.0000000000 time: 0.033033132553100586\n",
      "Iteration: 10310 loss: 0.0000000000 time: 0.029773712158203125\n",
      "Iteration: 10320 loss: 0.0000000000 time: 0.03218674659729004\n",
      "Iteration: 10330 loss: 0.0000000000 time: 0.02888941764831543\n",
      "Iteration: 10340 loss: 0.0000000000 time: 0.03142690658569336\n",
      "Iteration: 10350 loss: 0.0000000000 time: 0.030959606170654297\n",
      "Iteration: 10360 loss: 0.0000000000 time: 0.03031182289123535\n",
      "Iteration: 10370 loss: 0.0000000000 time: 0.03428530693054199\n",
      "Iteration: 10380 loss: 0.0000000000 time: 0.03162741661071777\n",
      "Iteration: 10390 loss: 0.0000000000 time: 0.03242945671081543\n",
      "Iteration: 10400 loss: 0.0000000000 time: 0.031323909759521484\n",
      "Iteration: 10410 loss: 0.0000000000 time: 0.036895036697387695\n",
      "Iteration: 10420 loss: 0.0000000000 time: 0.029110431671142578\n",
      "Iteration: 10430 loss: 0.0000000000 time: 0.03955245018005371\n",
      "Iteration: 10440 loss: 0.0000000000 time: 0.0298919677734375\n",
      "Iteration: 10450 loss: 0.0000000000 time: 0.027835369110107422\n",
      "Iteration: 10460 loss: 0.0000000000 time: 0.030466794967651367\n",
      "Iteration: 10470 loss: 0.0000000000 time: 0.029851675033569336\n",
      "Iteration: 10480 loss: 0.0000000000 time: 0.03016209602355957\n",
      "Iteration: 10490 loss: 0.0000000000 time: 0.0353703498840332\n",
      "Iteration: 10500 loss: 0.0000000000 time: 0.03099513053894043\n",
      "Iteration: 10510 loss: 0.0000000000 time: 0.035134077072143555\n",
      "Iteration: 10520 loss: 0.0000000000 time: 0.03037095069885254\n",
      "Iteration: 10530 loss: 0.0000000000 time: 0.02883172035217285\n",
      "Iteration: 10540 loss: 0.0000000000 time: 0.03187417984008789\n",
      "Iteration: 10550 loss: 0.0000000000 time: 0.02938985824584961\n",
      "Iteration: 10560 loss: 0.0000000000 time: 0.033432722091674805\n",
      "Iteration: 10570 loss: 0.0000000000 time: 0.03833317756652832\n",
      "Iteration: 10580 loss: 0.0000000000 time: 0.0325009822845459\n",
      "Iteration: 10590 loss: 0.0000000000 time: 0.03082418441772461\n",
      "Iteration: 10600 loss: 0.0000000000 time: 0.030524730682373047\n",
      "Iteration: 10610 loss: 0.0000000000 time: 0.026920557022094727\n",
      "Iteration: 10620 loss: 0.0000000000 time: 0.03694915771484375\n",
      "Iteration: 10630 loss: 0.0000000000 time: 0.03227972984313965\n",
      "Iteration: 10640 loss: 0.0000000000 time: 0.03329730033874512\n",
      "Iteration: 10650 loss: 0.0000000000 time: 0.03181767463684082\n",
      "Iteration: 10660 loss: 0.0000000000 time: 0.0301666259765625\n",
      "Iteration: 10670 loss: 0.0000000000 time: 0.030881643295288086\n",
      "Iteration: 10680 loss: 0.0000000000 time: 0.029077529907226562\n",
      "Iteration: 10690 loss: 0.0000000000 time: 0.03185296058654785\n",
      "Iteration: 10700 loss: 0.0000000000 time: 0.030350923538208008\n",
      "Iteration: 10710 loss: 0.0000000000 time: 0.03048086166381836\n",
      "Iteration: 10720 loss: 0.0000000000 time: 0.033930063247680664\n",
      "Iteration: 10730 loss: 0.0000000000 time: 0.03444194793701172\n",
      "Iteration: 10740 loss: 0.0000000000 time: 0.028264522552490234\n",
      "Iteration: 10750 loss: 0.0000000000 time: 0.030951738357543945\n",
      "Iteration: 10760 loss: 0.0000000000 time: 0.030049800872802734\n",
      "Iteration: 10770 loss: 0.0000000000 time: 0.028132915496826172\n",
      "Iteration: 10780 loss: 0.0000000000 time: 0.0304868221282959\n",
      "Iteration: 10790 loss: 0.0000000000 time: 0.03574991226196289\n",
      "Iteration: 10800 loss: 0.0000000000 time: 0.03284096717834473\n",
      "Iteration: 10810 loss: 0.0000000000 time: 0.03454256057739258\n",
      "Iteration: 10820 loss: 0.0000000000 time: 0.03480172157287598\n",
      "Iteration: 10830 loss: 0.0000000000 time: 0.03474998474121094\n",
      "Iteration: 10840 loss: 0.0000000000 time: 0.030147075653076172\n",
      "Iteration: 10850 loss: 0.0000000002 time: 0.0334622859954834\n",
      "Iteration: 10860 loss: 0.0000009755 time: 0.029313325881958008\n",
      "Iteration: 10870 loss: 0.0000874300 time: 0.037238121032714844\n",
      "Iteration: 10880 loss: 0.0000523295 time: 0.0338740348815918\n",
      "Iteration: 10890 loss: 0.0000035759 time: 0.0281829833984375\n",
      "Iteration: 10900 loss: 0.0000075204 time: 0.028423070907592773\n",
      "Iteration: 10910 loss: 0.0000019123 time: 0.032074928283691406\n",
      "Iteration: 10920 loss: 0.0000001857 time: 0.02997589111328125\n",
      "Iteration: 10930 loss: 0.0000004183 time: 0.03062915802001953\n",
      "Iteration: 10940 loss: 0.0000000017 time: 0.028449058532714844\n",
      "Iteration: 10950 loss: 0.0000000492 time: 0.029703617095947266\n",
      "Iteration: 10960 loss: 0.0000000019 time: 0.02646946907043457\n",
      "Iteration: 10970 loss: 0.0000000059 time: 0.03210091590881348\n",
      "Iteration: 10980 loss: 0.0000000001 time: 0.0317990779876709\n",
      "Iteration: 10990 loss: 0.0000000008 time: 0.033093929290771484\n",
      "Iteration: 11000 loss: 0.0000000000 time: 0.029340028762817383\n",
      "Iteration: 11010 loss: 0.0000000001 time: 0.029840469360351562\n",
      "Iteration: 11020 loss: 0.0000000000 time: 0.03235912322998047\n",
      "Iteration: 11030 loss: 0.0000000000 time: 0.02966785430908203\n",
      "Iteration: 11040 loss: 0.0000000000 time: 0.03120255470275879\n",
      "Iteration: 11050 loss: 0.0000000000 time: 0.034578561782836914\n",
      "Iteration: 11060 loss: 0.0000000000 time: 0.032157182693481445\n",
      "Iteration: 11070 loss: 0.0000000000 time: 0.030900239944458008\n",
      "Iteration: 11080 loss: 0.0000000000 time: 0.03224301338195801\n",
      "Iteration: 11090 loss: 0.0000000000 time: 0.03375959396362305\n",
      "Iteration: 11100 loss: 0.0000000000 time: 0.031070947647094727\n",
      "Iteration: 11110 loss: 0.0000000000 time: 0.031177997589111328\n",
      "Iteration: 11120 loss: 0.0000000000 time: 0.03569769859313965\n",
      "Iteration: 11130 loss: 0.0000000000 time: 0.03189826011657715\n",
      "Iteration: 11140 loss: 0.0000000000 time: 0.02874898910522461\n",
      "Iteration: 11150 loss: 0.0000000000 time: 0.0326080322265625\n",
      "Iteration: 11160 loss: 0.0000000000 time: 0.03191876411437988\n",
      "Iteration: 11170 loss: 0.0000000000 time: 0.035303592681884766\n",
      "Iteration: 11180 loss: 0.0000000000 time: 0.028590917587280273\n",
      "Iteration: 11190 loss: 0.0000000000 time: 0.03619837760925293\n",
      "Iteration: 11200 loss: 0.0000000000 time: 0.03443479537963867\n",
      "Iteration: 11210 loss: 0.0000000000 time: 0.033006906509399414\n",
      "Iteration: 11220 loss: 0.0000000000 time: 0.029345035552978516\n",
      "Iteration: 11230 loss: 0.0000000000 time: 0.028841733932495117\n",
      "Iteration: 11240 loss: 0.0000000000 time: 0.02718353271484375\n",
      "Iteration: 11250 loss: 0.0000000000 time: 0.02890300750732422\n",
      "Iteration: 11260 loss: 0.0000000000 time: 0.031266212463378906\n",
      "Iteration: 11270 loss: 0.0000000000 time: 0.03667902946472168\n",
      "Iteration: 11280 loss: 0.0000000000 time: 0.029214859008789062\n",
      "Iteration: 11290 loss: 0.0000000000 time: 0.02807784080505371\n",
      "Iteration: 11300 loss: 0.0000000000 time: 0.029468774795532227\n",
      "Iteration: 11310 loss: 0.0000000000 time: 0.03058600425720215\n",
      "Iteration: 11320 loss: 0.0000000000 time: 0.029615163803100586\n",
      "Iteration: 11330 loss: 0.0000000000 time: 0.029882431030273438\n",
      "Iteration: 11340 loss: 0.0000000000 time: 0.03725600242614746\n",
      "Iteration: 11350 loss: 0.0000000000 time: 0.032384395599365234\n",
      "Iteration: 11360 loss: 0.0000000000 time: 0.03149271011352539\n",
      "Iteration: 11370 loss: 0.0000000000 time: 0.033449411392211914\n",
      "Iteration: 11380 loss: 0.0000000000 time: 0.03624677658081055\n",
      "Iteration: 11390 loss: 0.0000000000 time: 0.03175544738769531\n",
      "Iteration: 11400 loss: 0.0000000000 time: 0.03478288650512695\n",
      "Iteration: 11410 loss: 0.0000000000 time: 0.030739307403564453\n",
      "Iteration: 11420 loss: 0.0000000000 time: 0.029944896697998047\n",
      "Iteration: 11430 loss: 0.0000000000 time: 0.030218839645385742\n",
      "Iteration: 11440 loss: 0.0000000000 time: 0.03181147575378418\n",
      "Iteration: 11450 loss: 0.0000000000 time: 0.031119823455810547\n",
      "Iteration: 11460 loss: 0.0000000000 time: 0.0297391414642334\n",
      "Iteration: 11470 loss: 0.0000000000 time: 0.03669118881225586\n",
      "Iteration: 11480 loss: 0.0000000000 time: 0.03230881690979004\n",
      "Iteration: 11490 loss: 0.0000000000 time: 0.03542900085449219\n",
      "Iteration: 11500 loss: 0.0000000000 time: 0.034715890884399414\n",
      "Iteration: 11510 loss: 0.0000000000 time: 0.032442569732666016\n",
      "Iteration: 11520 loss: 0.0000000000 time: 0.03415203094482422\n",
      "Iteration: 11530 loss: 0.0000000000 time: 0.0354161262512207\n",
      "Iteration: 11540 loss: 0.0000000000 time: 0.03503608703613281\n",
      "Iteration: 11550 loss: 0.0000000000 time: 0.03392505645751953\n",
      "Iteration: 11560 loss: 0.0000000000 time: 0.033948421478271484\n",
      "Iteration: 11570 loss: 0.0000000000 time: 0.034874916076660156\n",
      "Iteration: 11580 loss: 0.0000000000 time: 0.035108327865600586\n",
      "Iteration: 11590 loss: 0.0000000000 time: 0.04004263877868652\n",
      "Iteration: 11600 loss: 0.0000000000 time: 0.03612780570983887\n",
      "Iteration: 11610 loss: 0.0000000000 time: 0.03439736366271973\n",
      "Iteration: 11620 loss: 0.0000000000 time: 0.029995203018188477\n",
      "Iteration: 11630 loss: 0.0000000000 time: 0.03231668472290039\n",
      "Iteration: 11640 loss: 0.0000000000 time: 0.03182697296142578\n",
      "Iteration: 11650 loss: 0.0000000000 time: 0.03266477584838867\n",
      "Iteration: 11660 loss: 0.0000000000 time: 0.03127002716064453\n",
      "Iteration: 11670 loss: 0.0000000000 time: 0.038834571838378906\n",
      "Iteration: 11680 loss: 0.0000000000 time: 0.03345298767089844\n",
      "Iteration: 11690 loss: 0.0000000002 time: 0.031685829162597656\n",
      "Iteration: 11700 loss: 0.0000004327 time: 0.0316004753112793\n",
      "Iteration: 11710 loss: 0.0002756855 time: 0.03099513053894043\n",
      "Iteration: 11720 loss: 0.0000027212 time: 0.03451800346374512\n",
      "Iteration: 11730 loss: 0.0000040568 time: 0.03023052215576172\n",
      "Iteration: 11740 loss: 0.0000067149 time: 0.03028583526611328\n",
      "Iteration: 11750 loss: 0.0000020585 time: 0.028426408767700195\n",
      "Iteration: 11760 loss: 0.0000000169 time: 0.029491901397705078\n",
      "Iteration: 11770 loss: 0.0000001528 time: 0.030894994735717773\n",
      "Iteration: 11780 loss: 0.0000001203 time: 0.03060436248779297\n",
      "Iteration: 11790 loss: 0.0000000266 time: 0.033892154693603516\n",
      "Iteration: 11800 loss: 0.0000000013 time: 0.02889108657836914\n",
      "Iteration: 11810 loss: 0.0000000003 time: 0.028715133666992188\n",
      "Iteration: 11820 loss: 0.0000000007 time: 0.030033588409423828\n",
      "Iteration: 11830 loss: 0.0000000005 time: 0.03214716911315918\n",
      "Iteration: 11840 loss: 0.0000000002 time: 0.033052921295166016\n",
      "Iteration: 11850 loss: 0.0000000001 time: 0.030975341796875\n",
      "Iteration: 11860 loss: 0.0000000000 time: 0.03312373161315918\n",
      "Iteration: 11870 loss: 0.0000000000 time: 0.03374600410461426\n",
      "Iteration: 11880 loss: 0.0000000000 time: 0.03213810920715332\n",
      "Iteration: 11890 loss: 0.0000000000 time: 0.03081965446472168\n",
      "Iteration: 11900 loss: 0.0000000000 time: 0.030103683471679688\n",
      "Iteration: 11910 loss: 0.0000000000 time: 0.032144784927368164\n",
      "Iteration: 11920 loss: 0.0000000000 time: 0.034252166748046875\n",
      "Iteration: 11930 loss: 0.0000000000 time: 0.0353999137878418\n",
      "Iteration: 11940 loss: 0.0000000000 time: 0.028700828552246094\n",
      "Iteration: 11950 loss: 0.0000000000 time: 0.03159046173095703\n",
      "Iteration: 11960 loss: 0.0000000000 time: 0.031595706939697266\n",
      "Iteration: 11970 loss: 0.0000000000 time: 0.0301973819732666\n",
      "Iteration: 11980 loss: 0.0000000000 time: 0.028881311416625977\n",
      "Iteration: 11990 loss: 0.0000000000 time: 0.036209821701049805\n",
      "Iteration: 12000 loss: 0.0000000000 time: 0.032401323318481445\n",
      "Iteration: 12010 loss: 0.0000000000 time: 0.02973651885986328\n",
      "Iteration: 12020 loss: 0.0000000000 time: 0.03058338165283203\n",
      "Iteration: 12030 loss: 0.0000000000 time: 0.02827596664428711\n",
      "Iteration: 12040 loss: 0.0000000000 time: 0.030587434768676758\n",
      "Iteration: 12050 loss: 0.0000000000 time: 0.029937267303466797\n",
      "Iteration: 12060 loss: 0.0000000000 time: 0.031197309494018555\n",
      "Iteration: 12070 loss: 0.0000000000 time: 0.036248207092285156\n",
      "Iteration: 12080 loss: 0.0000000000 time: 0.028656005859375\n",
      "Iteration: 12090 loss: 0.0000000000 time: 0.02849745750427246\n",
      "Iteration: 12100 loss: 0.0000000000 time: 0.03298592567443848\n",
      "Iteration: 12110 loss: 0.0000000000 time: 0.030376434326171875\n",
      "Iteration: 12120 loss: 0.0000000000 time: 0.026772260665893555\n",
      "Iteration: 12130 loss: 0.0000000000 time: 0.0351412296295166\n",
      "Iteration: 12140 loss: 0.0000000000 time: 0.040387868881225586\n",
      "Iteration: 12150 loss: 0.0000000000 time: 0.03647327423095703\n",
      "Iteration: 12160 loss: 0.0000000000 time: 0.038417816162109375\n",
      "Iteration: 12170 loss: 0.0000000000 time: 0.028920412063598633\n",
      "Iteration: 12180 loss: 0.0000000000 time: 0.027850866317749023\n",
      "Iteration: 12190 loss: 0.0000000000 time: 0.027344465255737305\n",
      "Iteration: 12200 loss: 0.0000000000 time: 0.03011178970336914\n",
      "Iteration: 12210 loss: 0.0000000000 time: 0.028145313262939453\n",
      "Iteration: 12220 loss: 0.0000000000 time: 0.027115821838378906\n",
      "Iteration: 12230 loss: 0.0000000000 time: 0.029826641082763672\n",
      "Iteration: 12240 loss: 0.0000000000 time: 0.027037620544433594\n",
      "Iteration: 12250 loss: 0.0000000000 time: 0.02955317497253418\n",
      "Iteration: 12260 loss: 0.0000000000 time: 0.02774977684020996\n",
      "Iteration: 12270 loss: 0.0000000000 time: 0.03577017784118652\n",
      "Iteration: 12280 loss: 0.0000000000 time: 0.03281378746032715\n",
      "Iteration: 12290 loss: 0.0000000000 time: 0.029219627380371094\n",
      "Iteration: 12300 loss: 0.0000000000 time: 0.032605648040771484\n",
      "Iteration: 12310 loss: 0.0000000000 time: 0.03405404090881348\n",
      "Iteration: 12320 loss: 0.0000000000 time: 0.02791428565979004\n",
      "Iteration: 12330 loss: 0.0000000000 time: 0.029714107513427734\n",
      "Iteration: 12340 loss: 0.0000000000 time: 0.033823251724243164\n",
      "Iteration: 12350 loss: 0.0000000000 time: 0.03223156929016113\n",
      "Iteration: 12360 loss: 0.0000000000 time: 0.027651548385620117\n",
      "Iteration: 12370 loss: 0.0000000000 time: 0.030827760696411133\n",
      "Iteration: 12380 loss: 0.0000000000 time: 0.0303497314453125\n",
      "Iteration: 12390 loss: 0.0000000000 time: 0.030503511428833008\n",
      "Iteration: 12400 loss: 0.0000000000 time: 0.029260635375976562\n",
      "Iteration: 12410 loss: 0.0000000000 time: 0.031003713607788086\n",
      "Iteration: 12420 loss: 0.0000000000 time: 0.03471565246582031\n",
      "Iteration: 12430 loss: 0.0000000632 time: 0.03122258186340332\n",
      "Iteration: 12440 loss: 0.0001968119 time: 0.0323030948638916\n",
      "Iteration: 12450 loss: 0.0000732508 time: 0.02947545051574707\n",
      "Iteration: 12460 loss: 0.0000056989 time: 0.02986884117126465\n",
      "Iteration: 12470 loss: 0.0000059670 time: 0.04196596145629883\n",
      "Iteration: 12480 loss: 0.0000017796 time: 0.03657054901123047\n",
      "Iteration: 12490 loss: 0.0000006157 time: 0.028693437576293945\n",
      "Iteration: 12500 loss: 0.0000002586 time: 0.028353452682495117\n",
      "Iteration: 12510 loss: 0.0000000471 time: 0.027887582778930664\n",
      "Iteration: 12520 loss: 0.0000000475 time: 0.02820730209350586\n",
      "Iteration: 12530 loss: 0.0000000002 time: 0.028872251510620117\n",
      "Iteration: 12540 loss: 0.0000000064 time: 0.03129172325134277\n",
      "Iteration: 12550 loss: 0.0000000010 time: 0.02888798713684082\n",
      "Iteration: 12560 loss: 0.0000000001 time: 0.03331494331359863\n",
      "Iteration: 12570 loss: 0.0000000003 time: 0.029766559600830078\n",
      "Iteration: 12580 loss: 0.0000000001 time: 0.0304110050201416\n",
      "Iteration: 12590 loss: 0.0000000000 time: 0.03232622146606445\n",
      "Iteration: 12600 loss: 0.0000000000 time: 0.029555797576904297\n",
      "Iteration: 12610 loss: 0.0000000000 time: 0.032877445220947266\n",
      "Iteration: 12620 loss: 0.0000000000 time: 0.034712791442871094\n",
      "Iteration: 12630 loss: 0.0000000000 time: 0.03767514228820801\n",
      "Iteration: 12640 loss: 0.0000000000 time: 0.03161740303039551\n",
      "Iteration: 12650 loss: 0.0000000000 time: 0.030514240264892578\n",
      "Iteration: 12660 loss: 0.0000000000 time: 0.031058073043823242\n",
      "Iteration: 12670 loss: 0.0000000000 time: 0.032744646072387695\n",
      "Iteration: 12680 loss: 0.0000000000 time: 0.03061819076538086\n",
      "Iteration: 12690 loss: 0.0000000000 time: 0.0302731990814209\n",
      "Iteration: 12700 loss: 0.0000000000 time: 0.03174185752868652\n",
      "Iteration: 12710 loss: 0.0000000000 time: 0.029813051223754883\n",
      "Iteration: 12720 loss: 0.0000000000 time: 0.028148174285888672\n",
      "Iteration: 12730 loss: 0.0000000000 time: 0.02859640121459961\n",
      "Iteration: 12740 loss: 0.0000000000 time: 0.03238677978515625\n",
      "Iteration: 12750 loss: 0.0000000000 time: 0.03043341636657715\n",
      "Iteration: 12760 loss: 0.0000000000 time: 0.030018329620361328\n",
      "Iteration: 12770 loss: 0.0000000000 time: 0.03769397735595703\n",
      "Iteration: 12780 loss: 0.0000000000 time: 0.03348493576049805\n",
      "Iteration: 12790 loss: 0.0000000000 time: 0.03340339660644531\n",
      "Iteration: 12800 loss: 0.0000000000 time: 0.03189420700073242\n",
      "Iteration: 12810 loss: 0.0000000000 time: 0.033919334411621094\n",
      "Iteration: 12820 loss: 0.0000000000 time: 0.033846139907836914\n",
      "Iteration: 12830 loss: 0.0000000000 time: 0.035402536392211914\n",
      "Iteration: 12840 loss: 0.0000000000 time: 0.03294825553894043\n",
      "Iteration: 12850 loss: 0.0000000000 time: 0.028305768966674805\n",
      "Iteration: 12860 loss: 0.0000000000 time: 0.028288841247558594\n",
      "Iteration: 12870 loss: 0.0000000000 time: 0.02865743637084961\n",
      "Iteration: 12880 loss: 0.0000000000 time: 0.02727532386779785\n",
      "Iteration: 12890 loss: 0.0000000000 time: 0.02717900276184082\n",
      "Iteration: 12900 loss: 0.0000000000 time: 0.03587174415588379\n",
      "Iteration: 12910 loss: 0.0000000000 time: 0.030378103256225586\n",
      "Iteration: 12920 loss: 0.0000000000 time: 0.028887033462524414\n",
      "Iteration: 12930 loss: 0.0000000000 time: 0.0298004150390625\n",
      "Iteration: 12940 loss: 0.0000000000 time: 0.027721405029296875\n",
      "Iteration: 12950 loss: 0.0000000000 time: 0.027124881744384766\n",
      "Iteration: 12960 loss: 0.0000000000 time: 0.03578448295593262\n",
      "Iteration: 12970 loss: 0.0000000000 time: 0.03583216667175293\n",
      "Iteration: 12980 loss: 0.0000000000 time: 0.027647733688354492\n",
      "Iteration: 12990 loss: 0.0000000000 time: 0.02828359603881836\n",
      "Iteration: 13000 loss: 0.0000000000 time: 0.029323339462280273\n",
      "Iteration: 13010 loss: 0.0000000000 time: 0.029866456985473633\n",
      "Iteration: 13020 loss: 0.0000000000 time: 0.027990102767944336\n",
      "Iteration: 13030 loss: 0.0000000000 time: 0.02879810333251953\n",
      "Iteration: 13040 loss: 0.0000000000 time: 0.03264665603637695\n",
      "Iteration: 13050 loss: 0.0000000000 time: 0.029551029205322266\n",
      "Iteration: 13060 loss: 0.0000000000 time: 0.03060746192932129\n",
      "Iteration: 13070 loss: 0.0000000000 time: 0.029165029525756836\n",
      "Iteration: 13080 loss: 0.0000000000 time: 0.03134918212890625\n",
      "Iteration: 13090 loss: 0.0000000000 time: 0.02883124351501465\n",
      "Iteration: 13100 loss: 0.0000000000 time: 0.03224968910217285\n",
      "Iteration: 13110 loss: 0.0000000000 time: 0.031159162521362305\n",
      "Iteration: 13120 loss: 0.0000000000 time: 0.0305783748626709\n",
      "Iteration: 13130 loss: 0.0000000000 time: 0.03060460090637207\n",
      "Iteration: 13140 loss: 0.0000000000 time: 0.030010461807250977\n",
      "Iteration: 13150 loss: 0.0000000000 time: 0.02861189842224121\n",
      "Iteration: 13160 loss: 0.0000000000 time: 0.028334617614746094\n",
      "Iteration: 13170 loss: 0.0000000000 time: 0.031005144119262695\n",
      "Iteration: 13180 loss: 0.0000000000 time: 0.0391383171081543\n",
      "Iteration: 13190 loss: 0.0000000000 time: 0.02959156036376953\n",
      "Iteration: 13200 loss: 0.0000000000 time: 0.0276641845703125\n",
      "Iteration: 13210 loss: 0.0000000000 time: 0.02778792381286621\n",
      "Iteration: 13220 loss: 0.0000000001 time: 0.02713632583618164\n",
      "Iteration: 13230 loss: 0.0000003102 time: 0.0270540714263916\n",
      "Iteration: 13240 loss: 0.0002268180 time: 0.029869556427001953\n",
      "Iteration: 13250 loss: 0.0000032567 time: 0.0348360538482666\n",
      "Iteration: 13260 loss: 0.0000048161 time: 0.02950763702392578\n",
      "Iteration: 13270 loss: 0.0000059819 time: 0.030519723892211914\n",
      "Iteration: 13280 loss: 0.0000013121 time: 0.03231358528137207\n",
      "Iteration: 13290 loss: 0.0000000044 time: 0.03351020812988281\n",
      "Iteration: 13300 loss: 0.0000001905 time: 0.03115367889404297\n",
      "Iteration: 13310 loss: 0.0000000938 time: 0.033135175704956055\n",
      "Iteration: 13320 loss: 0.0000000113 time: 0.032201528549194336\n",
      "Iteration: 13330 loss: 0.0000000000 time: 0.030236244201660156\n",
      "Iteration: 13340 loss: 0.0000000014 time: 0.03189444541931152\n",
      "Iteration: 13350 loss: 0.0000000012 time: 0.030727624893188477\n",
      "Iteration: 13360 loss: 0.0000000005 time: 0.031188488006591797\n",
      "Iteration: 13370 loss: 0.0000000002 time: 0.027990341186523438\n",
      "Iteration: 13380 loss: 0.0000000001 time: 0.03328752517700195\n",
      "Iteration: 13390 loss: 0.0000000000 time: 0.03424882888793945\n",
      "Iteration: 13400 loss: 0.0000000000 time: 0.030562639236450195\n",
      "Iteration: 13410 loss: 0.0000000000 time: 0.028597354888916016\n",
      "Iteration: 13420 loss: 0.0000000000 time: 0.029547452926635742\n",
      "Iteration: 13430 loss: 0.0000000000 time: 0.029619932174682617\n",
      "Iteration: 13440 loss: 0.0000000000 time: 0.030181407928466797\n",
      "Iteration: 13450 loss: 0.0000000000 time: 0.03554248809814453\n",
      "Iteration: 13460 loss: 0.0000000000 time: 0.035233259201049805\n",
      "Iteration: 13470 loss: 0.0000000000 time: 0.031063556671142578\n",
      "Iteration: 13480 loss: 0.0000000000 time: 0.028475522994995117\n",
      "Iteration: 13490 loss: 0.0000000000 time: 0.03129744529724121\n",
      "Iteration: 13500 loss: 0.0000000000 time: 0.031691551208496094\n",
      "Iteration: 13510 loss: 0.0000000000 time: 0.03248190879821777\n",
      "Iteration: 13520 loss: 0.0000000000 time: 0.03956151008605957\n",
      "Iteration: 13530 loss: 0.0000000000 time: 0.033194541931152344\n",
      "Iteration: 13540 loss: 0.0000000000 time: 0.029893159866333008\n",
      "Iteration: 13550 loss: 0.0000000000 time: 0.030081748962402344\n",
      "Iteration: 13560 loss: 0.0000000000 time: 0.028901338577270508\n",
      "Iteration: 13570 loss: 0.0000000000 time: 0.02704143524169922\n",
      "Iteration: 13580 loss: 0.0000000000 time: 0.02804422378540039\n",
      "Iteration: 13590 loss: 0.0000000000 time: 0.03025674819946289\n",
      "Iteration: 13600 loss: 0.0000000000 time: 0.03136301040649414\n",
      "Iteration: 13610 loss: 0.0000000000 time: 0.03259396553039551\n",
      "Iteration: 13620 loss: 0.0000000000 time: 0.03244304656982422\n",
      "Iteration: 13630 loss: 0.0000000000 time: 0.027616024017333984\n",
      "Iteration: 13640 loss: 0.0000000000 time: 0.03010416030883789\n",
      "Iteration: 13650 loss: 0.0000000000 time: 0.029997825622558594\n",
      "Iteration: 13660 loss: 0.0000000000 time: 0.03472185134887695\n",
      "Iteration: 13670 loss: 0.0000000000 time: 0.03163933753967285\n",
      "Iteration: 13680 loss: 0.0000000000 time: 0.033411264419555664\n",
      "Iteration: 13690 loss: 0.0000000000 time: 0.03373146057128906\n",
      "Iteration: 13700 loss: 0.0000000000 time: 0.030597686767578125\n",
      "Iteration: 13710 loss: 0.0000000000 time: 0.027463197708129883\n",
      "Iteration: 13720 loss: 0.0000000000 time: 0.03224825859069824\n",
      "Iteration: 13730 loss: 0.0000000000 time: 0.032549142837524414\n",
      "Iteration: 13740 loss: 0.0000000000 time: 0.03532671928405762\n",
      "Iteration: 13750 loss: 0.0000000000 time: 0.031016111373901367\n",
      "Iteration: 13760 loss: 0.0000000000 time: 0.03262495994567871\n",
      "Iteration: 13770 loss: 0.0000000000 time: 0.03185772895812988\n",
      "Iteration: 13780 loss: 0.0000000000 time: 0.030257463455200195\n",
      "Iteration: 13790 loss: 0.0000000000 time: 0.029204130172729492\n",
      "Iteration: 13800 loss: 0.0000000000 time: 0.032012939453125\n",
      "Iteration: 13810 loss: 0.0000000000 time: 0.03478240966796875\n",
      "Iteration: 13820 loss: 0.0000000000 time: 0.03052973747253418\n",
      "Iteration: 13830 loss: 0.0000000000 time: 0.03415513038635254\n",
      "Iteration: 13840 loss: 0.0000000000 time: 0.02985835075378418\n",
      "Iteration: 13850 loss: 0.0000000000 time: 0.029117345809936523\n",
      "Iteration: 13860 loss: 0.0000000000 time: 0.03664350509643555\n",
      "Iteration: 13870 loss: 0.0000000000 time: 0.038666486740112305\n",
      "Iteration: 13880 loss: 0.0000000000 time: 0.03279995918273926\n",
      "Iteration: 13890 loss: 0.0000000000 time: 0.03209066390991211\n",
      "Iteration: 13900 loss: 0.0000000000 time: 0.028441429138183594\n",
      "Iteration: 13910 loss: 0.0000000000 time: 0.030633926391601562\n",
      "Iteration: 13920 loss: 0.0000000000 time: 0.030997753143310547\n",
      "Iteration: 13930 loss: 0.0000000000 time: 0.03345608711242676\n",
      "Iteration: 13940 loss: 0.0000000000 time: 0.03633403778076172\n",
      "Iteration: 13950 loss: 0.0000000013 time: 0.03583717346191406\n",
      "Iteration: 13960 loss: 0.0000032540 time: 0.034542083740234375\n",
      "Iteration: 13970 loss: 0.0000409596 time: 0.0316009521484375\n",
      "Iteration: 13980 loss: 0.0000000881 time: 0.027566194534301758\n",
      "Iteration: 13990 loss: 0.0000022962 time: 0.029605627059936523\n",
      "Iteration: 14000 loss: 0.0000038942 time: 0.03058338165283203\n",
      "Iteration: 14010 loss: 0.0000009598 time: 0.032701730728149414\n",
      "Iteration: 14020 loss: 0.0000000017 time: 0.031793832778930664\n",
      "Iteration: 14030 loss: 0.0000000859 time: 0.027383089065551758\n",
      "Iteration: 14040 loss: 0.0000000623 time: 0.031029701232910156\n",
      "Iteration: 14050 loss: 0.0000000149 time: 0.030678987503051758\n",
      "Iteration: 14060 loss: 0.0000000011 time: 0.029254436492919922\n",
      "Iteration: 14070 loss: 0.0000000000 time: 0.031107664108276367\n",
      "Iteration: 14080 loss: 0.0000000002 time: 0.03028726577758789\n",
      "Iteration: 14090 loss: 0.0000000002 time: 0.0414280891418457\n",
      "Iteration: 14100 loss: 0.0000000001 time: 0.03076624870300293\n",
      "Iteration: 14110 loss: 0.0000000000 time: 0.027721405029296875\n",
      "Iteration: 14120 loss: 0.0000000000 time: 0.030310392379760742\n",
      "Iteration: 14130 loss: 0.0000000000 time: 0.030441761016845703\n",
      "Iteration: 14140 loss: 0.0000000000 time: 0.029562950134277344\n",
      "Iteration: 14150 loss: 0.0000000000 time: 0.0342404842376709\n",
      "Iteration: 14160 loss: 0.0000000000 time: 0.033265113830566406\n",
      "Iteration: 14170 loss: 0.0000000000 time: 0.03054356575012207\n",
      "Iteration: 14180 loss: 0.0000000000 time: 0.036919593811035156\n",
      "Iteration: 14190 loss: 0.0000000000 time: 0.033440351486206055\n",
      "Iteration: 14200 loss: 0.0000000000 time: 0.0358426570892334\n",
      "Iteration: 14210 loss: 0.0000000000 time: 0.02983713150024414\n",
      "Iteration: 14220 loss: 0.0000000000 time: 0.03128385543823242\n",
      "Iteration: 14230 loss: 0.0000000000 time: 0.028628826141357422\n",
      "Iteration: 14240 loss: 0.0000000000 time: 0.028201580047607422\n",
      "Iteration: 14250 loss: 0.0000000000 time: 0.034572601318359375\n",
      "Iteration: 14260 loss: 0.0000000000 time: 0.03366518020629883\n",
      "Iteration: 14270 loss: 0.0000000000 time: 0.03248715400695801\n",
      "Iteration: 14280 loss: 0.0000000000 time: 0.032540082931518555\n",
      "Iteration: 14290 loss: 0.0000000000 time: 0.033220767974853516\n",
      "Iteration: 14300 loss: 0.0000000000 time: 0.03354334831237793\n",
      "Iteration: 14310 loss: 0.0000000000 time: 0.03088998794555664\n",
      "Iteration: 14320 loss: 0.0000000000 time: 0.029656648635864258\n",
      "Iteration: 14330 loss: 0.0000000000 time: 0.029369592666625977\n",
      "Iteration: 14340 loss: 0.0000000000 time: 0.028371334075927734\n",
      "Iteration: 14350 loss: 0.0000000000 time: 0.03267645835876465\n",
      "Iteration: 14360 loss: 0.0000000000 time: 0.03449892997741699\n",
      "Iteration: 14370 loss: 0.0000000000 time: 0.031116485595703125\n",
      "Iteration: 14380 loss: 0.0000000000 time: 0.03057384490966797\n",
      "Iteration: 14390 loss: 0.0000000000 time: 0.02956247329711914\n",
      "Iteration: 14400 loss: 0.0000000000 time: 0.030628681182861328\n",
      "Iteration: 14410 loss: 0.0000000000 time: 0.03741765022277832\n",
      "Iteration: 14420 loss: 0.0000000000 time: 0.03070235252380371\n",
      "Iteration: 14430 loss: 0.0000000000 time: 0.030889272689819336\n",
      "Iteration: 14440 loss: 0.0000000000 time: 0.030833959579467773\n",
      "Iteration: 14450 loss: 0.0000000000 time: 0.03276777267456055\n",
      "Iteration: 14460 loss: 0.0000000000 time: 0.028476953506469727\n",
      "Iteration: 14470 loss: 0.0000000000 time: 0.02790069580078125\n",
      "Iteration: 14480 loss: 0.0000000000 time: 0.029937028884887695\n",
      "Iteration: 14490 loss: 0.0000000000 time: 0.02773308753967285\n",
      "Iteration: 14500 loss: 0.0000000000 time: 0.0334019660949707\n",
      "Iteration: 14510 loss: 0.0000000000 time: 0.033154964447021484\n",
      "Iteration: 14520 loss: 0.0000000000 time: 0.029009580612182617\n",
      "Iteration: 14530 loss: 0.0000000000 time: 0.030344486236572266\n",
      "Iteration: 14540 loss: 0.0000000000 time: 0.030585765838623047\n",
      "Iteration: 14550 loss: 0.0000000000 time: 0.030705690383911133\n",
      "Iteration: 14560 loss: 0.0000000000 time: 0.028859615325927734\n",
      "Iteration: 14570 loss: 0.0000000000 time: 0.03369879722595215\n",
      "Iteration: 14580 loss: 0.0000000000 time: 0.03624415397644043\n",
      "Iteration: 14590 loss: 0.0000000000 time: 0.028235435485839844\n",
      "Iteration: 14600 loss: 0.0000000000 time: 0.028850555419921875\n",
      "Iteration: 14610 loss: 0.0000000000 time: 0.02823162078857422\n",
      "Iteration: 14620 loss: 0.0000000000 time: 0.029163599014282227\n",
      "Iteration: 14630 loss: 0.0000000000 time: 0.03057694435119629\n",
      "Iteration: 14640 loss: 0.0000000000 time: 0.032952070236206055\n",
      "Iteration: 14650 loss: 0.0000000000 time: 0.030932188034057617\n",
      "Iteration: 14660 loss: 0.0000000000 time: 0.028435468673706055\n",
      "Iteration: 14670 loss: 0.0000000000 time: 0.028914213180541992\n",
      "Iteration: 14680 loss: 0.0000000267 time: 0.02892589569091797\n",
      "Iteration: 14690 loss: 0.0000780796 time: 0.02984619140625\n",
      "Iteration: 14700 loss: 0.0000537746 time: 0.02817559242248535\n",
      "Iteration: 14710 loss: 0.0000204927 time: 0.030695199966430664\n",
      "Iteration: 14720 loss: 0.0000048808 time: 0.033599853515625\n",
      "Iteration: 14730 loss: 0.0000000338 time: 0.03671383857727051\n",
      "Iteration: 14740 loss: 0.0000007539 time: 0.034893035888671875\n",
      "Iteration: 14750 loss: 0.0000001688 time: 0.02931690216064453\n",
      "Iteration: 14760 loss: 0.0000000020 time: 0.029685258865356445\n",
      "Iteration: 14770 loss: 0.0000000276 time: 0.03519940376281738\n",
      "Iteration: 14780 loss: 0.0000000117 time: 0.03249979019165039\n",
      "Iteration: 14790 loss: 0.0000000011 time: 0.03138613700866699\n",
      "Iteration: 14800 loss: 0.0000000000 time: 0.02896857261657715\n",
      "Iteration: 14810 loss: 0.0000000002 time: 0.02792525291442871\n",
      "Iteration: 14820 loss: 0.0000000002 time: 0.031245946884155273\n",
      "Iteration: 14830 loss: 0.0000000001 time: 0.028582334518432617\n",
      "Iteration: 14840 loss: 0.0000000000 time: 0.03413891792297363\n",
      "Iteration: 14850 loss: 0.0000000000 time: 0.03054499626159668\n",
      "Iteration: 14860 loss: 0.0000000000 time: 0.03589797019958496\n",
      "Iteration: 14870 loss: 0.0000000000 time: 0.04095268249511719\n",
      "Iteration: 14880 loss: 0.0000000000 time: 0.03288388252258301\n",
      "Iteration: 14890 loss: 0.0000000000 time: 0.036725759506225586\n",
      "Iteration: 14900 loss: 0.0000000000 time: 0.03194284439086914\n",
      "Iteration: 14910 loss: 0.0000000000 time: 0.030434370040893555\n",
      "Iteration: 14920 loss: 0.0000000000 time: 0.03120899200439453\n",
      "Iteration: 14930 loss: 0.0000000000 time: 0.02931523323059082\n",
      "Iteration: 14940 loss: 0.0000000000 time: 0.030889511108398438\n",
      "Iteration: 14950 loss: 0.0000000000 time: 0.028970956802368164\n",
      "Iteration: 14960 loss: 0.0000000000 time: 0.030124425888061523\n",
      "Iteration: 14970 loss: 0.0000000000 time: 0.028998374938964844\n",
      "Iteration: 14980 loss: 0.0000000000 time: 0.030249834060668945\n",
      "Iteration: 14990 loss: 0.0000000000 time: 0.03465890884399414\n",
      "Iteration: 15000 loss: 0.0000000000 time: 0.028651952743530273\n",
      "-->mesh : \n",
      "     n_triangles :  16\n",
      "     n_vertices  :  13\n",
      "     n_edges     :  28\n",
      "     h_max           :  0.5000000000013305\n",
      "     h_min           :  0.353553390592333\n",
      "-->test_fun      : \n",
      "     order       :  1\n",
      "     dof         :  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 12:11:59.097286: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:11:59.100971: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:11:59.104029: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:11:59.105677: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:11:59.108168: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:11:59.110517: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:11:59.113167: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:11:59.114928: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:11:59.116743: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:11:59.119117: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:11:59.630609: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_30' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_30}}]]\n",
      "2023-12-26 12:11:59.630763: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_59' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_59}}]]\n",
      "2023-12-26 12:11:59.630820: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_77' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_77}}]]\n",
      "2023-12-26 12:11:59.630922: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_95' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_95}}]]\n",
      "2023-12-26 12:11:59.630996: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_104' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_104}}]]\n",
      "2023-12-26 12:11:59.631087: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_133' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_133}}]]\n",
      "2023-12-26 12:11:59.631148: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_151' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_151}}]]\n",
      "2023-12-26 12:11:59.631218: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_169' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_169}}]]\n",
      "2023-12-26 12:11:59.631312: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_178' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_178}}]]\n",
      "2023-12-26 12:11:59.631393: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_207' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_207}}]]\n",
      "2023-12-26 12:11:59.631499: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_225' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_225}}]]\n",
      "2023-12-26 12:11:59.631610: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_243' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_243}}]]\n",
      "2023-12-26 12:11:59.631774: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_252' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_252}}]]\n",
      "2023-12-26 12:11:59.631918: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_280' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_280}}]]\n",
      "2023-12-26 12:11:59.632011: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_291' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_291}}]]\n",
      "2023-12-26 12:11:59.632147: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_298' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_298}}]]\n",
      "2023-12-26 12:11:59.632220: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_303' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_303}}]]\n",
      "2023-12-26 12:11:59.632370: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_306' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_306}}]]\n",
      "2023-12-26 12:11:59.632476: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_309' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_309}}]]\n",
      "2023-12-26 12:11:59.632584: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_312' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_312}}]]\n",
      "2023-12-26 12:11:59.632717: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_315' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_315}}]]\n",
      "2023-12-26 12:11:59.632794: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_318' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_318}}]]\n",
      "2023-12-26 12:11:59.632915: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_321' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_321}}]]\n",
      "2023-12-26 12:11:59.633050: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_324' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_324}}]]\n",
      "2023-12-26 12:11:59.633126: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_327' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_327}}]]\n",
      "2023-12-26 12:11:59.633250: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_330' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_330}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 loss: 0.1100002730 time: 2.9545247554779053\n",
      "Iteration: 10 loss: 0.0185641847 time: 0.09357953071594238\n",
      "Iteration: 20 loss: 0.0104962785 time: 0.09421873092651367\n",
      "Iteration: 30 loss: 0.0051915833 time: 0.0966343879699707\n",
      "Iteration: 40 loss: 0.0016350525 time: 0.08510518074035645\n",
      "Iteration: 50 loss: 0.0007420293 time: 0.08286881446838379\n",
      "Iteration: 60 loss: 0.0004640379 time: 0.0824289321899414\n",
      "Iteration: 70 loss: 0.0002519273 time: 0.09079885482788086\n",
      "Iteration: 80 loss: 0.0001036577 time: 0.0850684642791748\n",
      "Iteration: 90 loss: 0.0000461426 time: 0.08551955223083496\n",
      "Iteration: 100 loss: 0.0000222299 time: 0.08413457870483398\n",
      "Iteration: 110 loss: 0.0000113820 time: 0.07390999794006348\n",
      "Iteration: 120 loss: 0.0000083684 time: 0.0738072395324707\n",
      "Iteration: 130 loss: 0.0000075156 time: 0.07315444946289062\n",
      "Iteration: 140 loss: 0.0000073169 time: 0.07073736190795898\n",
      "Iteration: 150 loss: 0.0000073053 time: 0.07214236259460449\n",
      "Iteration: 160 loss: 0.0000072769 time: 0.06914234161376953\n",
      "Iteration: 170 loss: 0.0000072490 time: 0.062142133712768555\n",
      "Iteration: 180 loss: 0.0000072191 time: 0.06380033493041992\n",
      "Iteration: 190 loss: 0.0000071907 time: 0.06852340698242188\n",
      "Iteration: 200 loss: 0.0000071633 time: 0.06754422187805176\n",
      "Iteration: 210 loss: 0.0000071356 time: 0.06720852851867676\n",
      "Iteration: 220 loss: 0.0000071074 time: 0.0659494400024414\n",
      "Iteration: 230 loss: 0.0000070786 time: 0.06971955299377441\n",
      "Iteration: 240 loss: 0.0000070491 time: 0.06302666664123535\n",
      "Iteration: 250 loss: 0.0000070189 time: 0.05700945854187012\n",
      "Iteration: 260 loss: 0.0000069881 time: 0.06652021408081055\n",
      "Iteration: 270 loss: 0.0000069566 time: 0.06808042526245117\n",
      "Iteration: 280 loss: 0.0000069245 time: 0.06684613227844238\n",
      "Iteration: 290 loss: 0.0000068919 time: 0.07123756408691406\n",
      "Iteration: 300 loss: 0.0000068586 time: 0.07177209854125977\n",
      "Iteration: 310 loss: 0.0000068248 time: 0.07375311851501465\n",
      "Iteration: 320 loss: 0.0000067903 time: 0.07222914695739746\n",
      "Iteration: 330 loss: 0.0000067553 time: 0.07405853271484375\n",
      "Iteration: 340 loss: 0.0000067198 time: 0.06351184844970703\n",
      "Iteration: 350 loss: 0.0000066837 time: 0.06139183044433594\n",
      "Iteration: 360 loss: 0.0000066470 time: 0.06756162643432617\n",
      "Iteration: 370 loss: 0.0000066098 time: 0.07155156135559082\n",
      "Iteration: 380 loss: 0.0000065721 time: 0.06572914123535156\n",
      "Iteration: 390 loss: 0.0000065339 time: 0.06432914733886719\n",
      "Iteration: 400 loss: 0.0000064952 time: 0.06473875045776367\n",
      "Iteration: 410 loss: 0.0000064560 time: 0.06647539138793945\n",
      "Iteration: 420 loss: 0.0000064162 time: 0.06351637840270996\n",
      "Iteration: 430 loss: 0.0000063760 time: 0.06330180168151855\n",
      "Iteration: 440 loss: 0.0000063354 time: 0.06996870040893555\n",
      "Iteration: 450 loss: 0.0000062942 time: 0.06728792190551758\n",
      "Iteration: 460 loss: 0.0000062526 time: 0.0666494369506836\n",
      "Iteration: 470 loss: 0.0000062106 time: 0.06618785858154297\n",
      "Iteration: 480 loss: 0.0000061681 time: 0.06667685508728027\n",
      "Iteration: 490 loss: 0.0000061251 time: 0.06745243072509766\n",
      "Iteration: 500 loss: 0.0000060817 time: 0.06713652610778809\n",
      "Iteration: 510 loss: 0.0000060379 time: 0.0676572322845459\n",
      "Iteration: 520 loss: 0.0000059937 time: 0.07120585441589355\n",
      "Iteration: 530 loss: 0.0000059491 time: 0.0716557502746582\n",
      "Iteration: 540 loss: 0.0000059041 time: 0.07261800765991211\n",
      "Iteration: 550 loss: 0.0000058587 time: 0.06055283546447754\n",
      "Iteration: 560 loss: 0.0000058129 time: 0.07013392448425293\n",
      "Iteration: 570 loss: 0.0000057667 time: 0.06971883773803711\n",
      "Iteration: 580 loss: 0.0000057202 time: 0.0697791576385498\n",
      "Iteration: 590 loss: 0.0000056733 time: 0.07048845291137695\n",
      "Iteration: 600 loss: 0.0000056260 time: 0.0689845085144043\n",
      "Iteration: 610 loss: 0.0000055784 time: 0.06965804100036621\n",
      "Iteration: 620 loss: 0.0000055304 time: 0.06862235069274902\n",
      "Iteration: 630 loss: 0.0000054822 time: 0.060431718826293945\n",
      "Iteration: 640 loss: 0.0000054336 time: 0.06192517280578613\n",
      "Iteration: 650 loss: 0.0000053847 time: 0.06428766250610352\n",
      "Iteration: 660 loss: 0.0000053355 time: 0.06356239318847656\n",
      "Iteration: 670 loss: 0.0000052860 time: 0.0641334056854248\n",
      "Iteration: 680 loss: 0.0000052362 time: 0.06398940086364746\n",
      "Iteration: 690 loss: 0.0000051861 time: 0.06271100044250488\n",
      "Iteration: 700 loss: 0.0000051358 time: 0.0599672794342041\n",
      "Iteration: 710 loss: 0.0000050852 time: 0.06693410873413086\n",
      "Iteration: 720 loss: 0.0000050343 time: 0.06995844841003418\n",
      "Iteration: 730 loss: 0.0000049832 time: 0.0730743408203125\n",
      "Iteration: 740 loss: 0.0000049319 time: 0.0713651180267334\n",
      "Iteration: 750 loss: 0.0000048803 time: 0.06952738761901855\n",
      "Iteration: 760 loss: 0.0000048286 time: 0.06981229782104492\n",
      "Iteration: 770 loss: 0.0000047766 time: 0.07038211822509766\n",
      "Iteration: 780 loss: 0.0000047244 time: 0.06933450698852539\n",
      "Iteration: 790 loss: 0.0000046721 time: 0.06969261169433594\n",
      "Iteration: 800 loss: 0.0000046196 time: 0.07085394859313965\n",
      "Iteration: 810 loss: 0.0000045669 time: 0.06936168670654297\n",
      "Iteration: 820 loss: 0.0000045141 time: 0.07312917709350586\n",
      "Iteration: 830 loss: 0.0000044611 time: 0.07311153411865234\n",
      "Iteration: 840 loss: 0.0000044080 time: 0.07239890098571777\n",
      "Iteration: 850 loss: 0.0000043547 time: 0.0740959644317627\n",
      "Iteration: 860 loss: 0.0000043014 time: 0.07504391670227051\n",
      "Iteration: 870 loss: 0.0000042479 time: 0.07234787940979004\n",
      "Iteration: 880 loss: 0.0000041944 time: 0.0734257698059082\n",
      "Iteration: 890 loss: 0.0000041408 time: 0.07426643371582031\n",
      "Iteration: 900 loss: 0.0000040871 time: 0.07293176651000977\n",
      "Iteration: 910 loss: 0.0000040334 time: 0.07282209396362305\n",
      "Iteration: 920 loss: 0.0000039796 time: 0.0738365650177002\n",
      "Iteration: 930 loss: 0.0000039258 time: 0.07381963729858398\n",
      "Iteration: 940 loss: 0.0000038720 time: 0.07542538642883301\n",
      "Iteration: 950 loss: 0.0000038182 time: 0.07526254653930664\n",
      "Iteration: 960 loss: 0.0000037643 time: 0.06154465675354004\n",
      "Iteration: 970 loss: 0.0000037105 time: 0.06175875663757324\n",
      "Iteration: 980 loss: 0.0000036567 time: 0.06801795959472656\n",
      "Iteration: 990 loss: 0.0000036030 time: 0.07256174087524414\n",
      "Iteration: 1000 loss: 0.0000035493 time: 0.06694221496582031\n",
      "Iteration: 1010 loss: 0.0000034957 time: 0.06736302375793457\n",
      "Iteration: 1020 loss: 0.0000034421 time: 0.0695953369140625\n",
      "Iteration: 1030 loss: 0.0000033887 time: 0.0731959342956543\n",
      "Iteration: 1040 loss: 0.0000033354 time: 0.0720822811126709\n",
      "Iteration: 1050 loss: 0.0000032821 time: 0.0722498893737793\n",
      "Iteration: 1060 loss: 0.0000032290 time: 0.07283258438110352\n",
      "Iteration: 1070 loss: 0.0000031761 time: 0.07212495803833008\n",
      "Iteration: 1080 loss: 0.0000031233 time: 0.07241678237915039\n",
      "Iteration: 1090 loss: 0.0000030707 time: 0.07196688652038574\n",
      "Iteration: 1100 loss: 0.0000030182 time: 0.07280468940734863\n",
      "Iteration: 1110 loss: 0.0000029659 time: 0.06583786010742188\n",
      "Iteration: 1120 loss: 0.0000029139 time: 0.06148886680603027\n",
      "Iteration: 1130 loss: 0.0000028621 time: 0.06023764610290527\n",
      "Iteration: 1140 loss: 0.0000028105 time: 0.0639486312866211\n",
      "Iteration: 1150 loss: 0.0000027591 time: 0.06282806396484375\n",
      "Iteration: 1160 loss: 0.0000027081 time: 0.06522703170776367\n",
      "Iteration: 1170 loss: 0.0000026572 time: 0.06087207794189453\n",
      "Iteration: 1180 loss: 0.0000026067 time: 0.060769081115722656\n",
      "Iteration: 1190 loss: 0.0000025565 time: 0.06943345069885254\n",
      "Iteration: 1200 loss: 0.0000025066 time: 0.06902837753295898\n",
      "Iteration: 1210 loss: 0.0000024570 time: 0.071929931640625\n",
      "Iteration: 1220 loss: 0.0000024077 time: 0.06578874588012695\n",
      "Iteration: 1230 loss: 0.0000023588 time: 0.057535648345947266\n",
      "Iteration: 1240 loss: 0.0000023102 time: 0.06699538230895996\n",
      "Iteration: 1250 loss: 0.0000022620 time: 0.0693049430847168\n",
      "Iteration: 1260 loss: 0.0000022142 time: 0.0666348934173584\n",
      "Iteration: 1270 loss: 0.0000021668 time: 0.06583285331726074\n",
      "Iteration: 1280 loss: 0.0000021198 time: 0.0654902458190918\n",
      "Iteration: 1290 loss: 0.0000020732 time: 0.06815862655639648\n",
      "Iteration: 1300 loss: 0.0000020271 time: 0.0722053050994873\n",
      "Iteration: 1310 loss: 0.0000019813 time: 0.06401705741882324\n",
      "Iteration: 1320 loss: 0.0000019361 time: 0.0634922981262207\n",
      "Iteration: 1330 loss: 0.0000018913 time: 0.06549310684204102\n",
      "Iteration: 1340 loss: 0.0000018469 time: 0.062487125396728516\n",
      "Iteration: 1350 loss: 0.0000018031 time: 0.06262707710266113\n",
      "Iteration: 1360 loss: 0.0000017597 time: 0.06139969825744629\n",
      "Iteration: 1370 loss: 0.0000017169 time: 0.064208984375\n",
      "Iteration: 1380 loss: 0.0000016745 time: 0.06288599967956543\n",
      "Iteration: 1390 loss: 0.0000016327 time: 0.06334996223449707\n",
      "Iteration: 1400 loss: 0.0000015914 time: 0.06326460838317871\n",
      "Iteration: 1410 loss: 0.0000015506 time: 0.06672382354736328\n",
      "Iteration: 1420 loss: 0.0000015104 time: 0.06281495094299316\n",
      "Iteration: 1430 loss: 0.0000014707 time: 0.0621495246887207\n",
      "Iteration: 1440 loss: 0.0000014316 time: 0.06366157531738281\n",
      "Iteration: 1450 loss: 0.0000013930 time: 0.06507444381713867\n",
      "Iteration: 1460 loss: 0.0000013550 time: 0.06429839134216309\n",
      "Iteration: 1470 loss: 0.0000013176 time: 0.06338310241699219\n",
      "Iteration: 1480 loss: 0.0000012808 time: 0.0653538703918457\n",
      "Iteration: 1490 loss: 0.0000012446 time: 0.06422638893127441\n",
      "Iteration: 1500 loss: 0.0000012089 time: 0.0632028579711914\n",
      "Iteration: 1510 loss: 0.0000011739 time: 0.06212949752807617\n",
      "Iteration: 1520 loss: 0.0000011394 time: 0.0633993148803711\n",
      "Iteration: 1530 loss: 0.0000011056 time: 0.06523489952087402\n",
      "Iteration: 1540 loss: 0.0000010723 time: 0.062494516372680664\n",
      "Iteration: 1550 loss: 0.0000010397 time: 0.06450819969177246\n",
      "Iteration: 1560 loss: 0.0000010077 time: 0.06257271766662598\n",
      "Iteration: 1570 loss: 0.0000009763 time: 0.06539607048034668\n",
      "Iteration: 1580 loss: 0.0000009455 time: 0.06330132484436035\n",
      "Iteration: 1590 loss: 0.0000009154 time: 0.062244415283203125\n",
      "Iteration: 1600 loss: 0.0000008858 time: 0.06418824195861816\n",
      "Iteration: 1610 loss: 0.0000008569 time: 0.06400251388549805\n",
      "Iteration: 1620 loss: 0.0000008286 time: 0.0652925968170166\n",
      "Iteration: 1630 loss: 0.0000008009 time: 0.07099151611328125\n",
      "Iteration: 1640 loss: 0.0000007738 time: 0.07399559020996094\n",
      "Iteration: 1650 loss: 0.0000007473 time: 0.07181310653686523\n",
      "Iteration: 1660 loss: 0.0000007215 time: 0.06790924072265625\n",
      "Iteration: 1670 loss: 0.0000006962 time: 0.06732296943664551\n",
      "Iteration: 1680 loss: 0.0000006716 time: 0.07030344009399414\n",
      "Iteration: 1690 loss: 0.0000006476 time: 0.07042098045349121\n",
      "Iteration: 1700 loss: 0.0000006241 time: 0.0678863525390625\n",
      "Iteration: 1710 loss: 0.0000006013 time: 0.07123517990112305\n",
      "Iteration: 1720 loss: 0.0000005790 time: 0.0673220157623291\n",
      "Iteration: 1730 loss: 0.0000005574 time: 0.06323099136352539\n",
      "Iteration: 1740 loss: 0.0000005363 time: 0.06486105918884277\n",
      "Iteration: 1750 loss: 0.0000005158 time: 0.062479257583618164\n",
      "Iteration: 1760 loss: 0.0000004959 time: 0.06449031829833984\n",
      "Iteration: 1770 loss: 0.0000004765 time: 0.06557226181030273\n",
      "Iteration: 1780 loss: 0.0000004577 time: 0.06265020370483398\n",
      "Iteration: 1790 loss: 0.0000004394 time: 0.06456470489501953\n",
      "Iteration: 1800 loss: 0.0000004217 time: 0.06386828422546387\n",
      "Iteration: 1810 loss: 0.0000004045 time: 0.06560182571411133\n",
      "Iteration: 1820 loss: 0.0000003878 time: 0.06296229362487793\n",
      "Iteration: 1830 loss: 0.0000003717 time: 0.061456918716430664\n",
      "Iteration: 1840 loss: 0.0000003560 time: 0.06343412399291992\n",
      "Iteration: 1850 loss: 0.0000003409 time: 0.06572198867797852\n",
      "Iteration: 1860 loss: 0.0000003263 time: 0.06291794776916504\n",
      "Iteration: 1870 loss: 0.0000003121 time: 0.06332135200500488\n",
      "Iteration: 1880 loss: 0.0000002985 time: 0.06258869171142578\n",
      "Iteration: 1890 loss: 0.0000002853 time: 0.06674933433532715\n",
      "Iteration: 1900 loss: 0.0000002725 time: 0.06277108192443848\n",
      "Iteration: 1910 loss: 0.0000002602 time: 0.06390023231506348\n",
      "Iteration: 1920 loss: 0.0000002484 time: 0.05738520622253418\n",
      "Iteration: 1930 loss: 0.0000002369 time: 0.060396671295166016\n",
      "Iteration: 1940 loss: 0.0000002259 time: 0.058998823165893555\n",
      "Iteration: 1950 loss: 0.0000002153 time: 0.06250762939453125\n",
      "Iteration: 1960 loss: 0.0000002051 time: 0.06850814819335938\n",
      "Iteration: 1970 loss: 0.0000001953 time: 0.06382322311401367\n",
      "Iteration: 1980 loss: 0.0000001859 time: 0.06539011001586914\n",
      "Iteration: 1990 loss: 0.0000001769 time: 0.06343197822570801\n",
      "Iteration: 2000 loss: 0.0000001682 time: 0.06360220909118652\n",
      "Iteration: 2010 loss: 0.0000001598 time: 0.06614136695861816\n",
      "Iteration: 2020 loss: 0.0000001518 time: 0.06252932548522949\n",
      "Iteration: 2030 loss: 0.0000001441 time: 0.059011220932006836\n",
      "Iteration: 2040 loss: 0.0000001368 time: 0.06981182098388672\n",
      "Iteration: 2050 loss: 0.0000001298 time: 0.06992578506469727\n",
      "Iteration: 2060 loss: 0.0000001230 time: 0.06896710395812988\n",
      "Iteration: 2070 loss: 0.0000001166 time: 0.06858110427856445\n",
      "Iteration: 2080 loss: 0.0000001104 time: 0.06745481491088867\n",
      "Iteration: 2090 loss: 0.0000001046 time: 0.06978082656860352\n",
      "Iteration: 2100 loss: 0.0000000989 time: 0.06668901443481445\n",
      "Iteration: 2110 loss: 0.0000000936 time: 0.06786251068115234\n",
      "Iteration: 2120 loss: 0.0000000885 time: 0.07051277160644531\n",
      "Iteration: 2130 loss: 0.0000000836 time: 0.06813383102416992\n",
      "Iteration: 2140 loss: 0.0000000789 time: 0.06660175323486328\n",
      "Iteration: 2150 loss: 0.0000000745 time: 0.06962275505065918\n",
      "Iteration: 2160 loss: 0.0000000703 time: 0.06758904457092285\n",
      "Iteration: 2170 loss: 0.0000000663 time: 0.06838536262512207\n",
      "Iteration: 2180 loss: 0.0000000625 time: 0.06961512565612793\n",
      "Iteration: 2190 loss: 0.0000000589 time: 0.056160926818847656\n",
      "Iteration: 2200 loss: 0.0000000554 time: 0.061669111251831055\n",
      "Iteration: 2210 loss: 0.0000000522 time: 0.06062507629394531\n",
      "Iteration: 2220 loss: 0.0000000491 time: 0.05907797813415527\n",
      "Iteration: 2230 loss: 0.0000000462 time: 0.060600996017456055\n",
      "Iteration: 2240 loss: 0.0000000434 time: 0.059293270111083984\n",
      "Iteration: 2250 loss: 0.0000000408 time: 0.061431169509887695\n",
      "Iteration: 2260 loss: 0.0000000383 time: 0.061605215072631836\n",
      "Iteration: 2270 loss: 0.0000000359 time: 0.0664210319519043\n",
      "Iteration: 2280 loss: 0.0000000337 time: 0.06313705444335938\n",
      "Iteration: 2290 loss: 0.0000000316 time: 0.0671546459197998\n",
      "Iteration: 2300 loss: 0.0000000296 time: 0.06632161140441895\n",
      "Iteration: 2310 loss: 0.0000000277 time: 0.0649716854095459\n",
      "Iteration: 2320 loss: 0.0000000259 time: 0.06336140632629395\n",
      "Iteration: 2330 loss: 0.0000000243 time: 0.06519651412963867\n",
      "Iteration: 2340 loss: 0.0000000227 time: 0.06502008438110352\n",
      "Iteration: 2350 loss: 0.0000000212 time: 0.06275534629821777\n",
      "Iteration: 2360 loss: 0.0000000198 time: 0.06697440147399902\n",
      "Iteration: 2370 loss: 0.0000000185 time: 0.06281900405883789\n",
      "Iteration: 2380 loss: 0.0000000173 time: 0.05945611000061035\n",
      "Iteration: 2390 loss: 0.0000000161 time: 0.06077098846435547\n",
      "Iteration: 2400 loss: 0.0000000150 time: 0.05941462516784668\n",
      "Iteration: 2410 loss: 0.0000000140 time: 0.0715174674987793\n",
      "Iteration: 2420 loss: 0.0000000130 time: 0.0703122615814209\n",
      "Iteration: 2430 loss: 0.0000000121 time: 0.07053208351135254\n",
      "Iteration: 2440 loss: 0.0000000113 time: 0.07177615165710449\n",
      "Iteration: 2450 loss: 0.0000000105 time: 0.07213020324707031\n",
      "Iteration: 2460 loss: 0.0000000098 time: 0.0710599422454834\n",
      "Iteration: 2470 loss: 0.0000000091 time: 0.07080411911010742\n",
      "Iteration: 2480 loss: 0.0000000084 time: 0.07332801818847656\n",
      "Iteration: 2490 loss: 0.0000000078 time: 0.06970643997192383\n",
      "Iteration: 2500 loss: 0.0000000073 time: 0.07030630111694336\n",
      "Iteration: 2510 loss: 0.0000000067 time: 0.0728006362915039\n",
      "Iteration: 2520 loss: 0.0000000062 time: 0.07040762901306152\n",
      "Iteration: 2530 loss: 0.0000000058 time: 0.07108592987060547\n",
      "Iteration: 2540 loss: 0.0000000053 time: 0.07435917854309082\n",
      "Iteration: 2550 loss: 0.0000000049 time: 0.07189083099365234\n",
      "Iteration: 2560 loss: 0.0000000046 time: 0.07285332679748535\n",
      "Iteration: 2570 loss: 0.0000000042 time: 0.06103253364562988\n",
      "Iteration: 2580 loss: 0.0000000039 time: 0.05885148048400879\n",
      "Iteration: 2590 loss: 0.0000000036 time: 0.06463861465454102\n",
      "Iteration: 2600 loss: 0.0000000033 time: 0.06905508041381836\n",
      "Iteration: 2610 loss: 0.0000000031 time: 0.06699395179748535\n",
      "Iteration: 2620 loss: 0.0000000028 time: 0.06392407417297363\n",
      "Iteration: 2630 loss: 0.0000000026 time: 0.06902313232421875\n",
      "Iteration: 2640 loss: 0.0000000024 time: 0.0709068775177002\n",
      "Iteration: 2650 loss: 0.0000000022 time: 0.0676736831665039\n",
      "Iteration: 2660 loss: 0.0000000020 time: 0.06882619857788086\n",
      "Iteration: 2670 loss: 0.0000000019 time: 0.06859326362609863\n",
      "Iteration: 2680 loss: 0.0000000017 time: 0.0698859691619873\n",
      "Iteration: 2690 loss: 0.0000000016 time: 0.0694277286529541\n",
      "Iteration: 2700 loss: 0.0000000015 time: 0.069671630859375\n",
      "Iteration: 2710 loss: 0.0000000013 time: 0.07117104530334473\n",
      "Iteration: 2720 loss: 0.0000000012 time: 0.06682109832763672\n",
      "Iteration: 2730 loss: 0.0000000011 time: 0.06744694709777832\n",
      "Iteration: 2740 loss: 0.0000000010 time: 0.07090163230895996\n",
      "Iteration: 2750 loss: 0.0000000009 time: 0.06872677803039551\n",
      "Iteration: 2760 loss: 0.0000000009 time: 0.06949925422668457\n",
      "Iteration: 2770 loss: 0.0000000008 time: 0.07007002830505371\n",
      "Iteration: 2780 loss: 0.0000000007 time: 0.0690910816192627\n",
      "Iteration: 2790 loss: 0.0000000007 time: 0.06253409385681152\n",
      "Iteration: 2800 loss: 0.0000000006 time: 0.06208968162536621\n",
      "Iteration: 2810 loss: 0.0000000006 time: 0.05879354476928711\n",
      "Iteration: 2820 loss: 0.0000000005 time: 0.05904340744018555\n",
      "Iteration: 2830 loss: 0.0000000005 time: 0.05954146385192871\n",
      "Iteration: 2840 loss: 0.0000000004 time: 0.05952763557434082\n",
      "Iteration: 2850 loss: 0.0000000004 time: 0.05867433547973633\n",
      "Iteration: 2860 loss: 0.0000000003 time: 0.059317827224731445\n",
      "Iteration: 2870 loss: 0.0000000003 time: 0.0614621639251709\n",
      "Iteration: 2880 loss: 0.0000000003 time: 0.06129765510559082\n",
      "Iteration: 2890 loss: 0.0000000003 time: 0.05828595161437988\n",
      "Iteration: 2900 loss: 0.0000000002 time: 0.058699607849121094\n",
      "Iteration: 2910 loss: 0.0000000002 time: 0.05903339385986328\n",
      "Iteration: 2920 loss: 0.0000000002 time: 0.060987234115600586\n",
      "Iteration: 2930 loss: 0.0000000002 time: 0.06858444213867188\n",
      "Iteration: 2940 loss: 0.0000000002 time: 0.07200384140014648\n",
      "Iteration: 2950 loss: 0.0000000001 time: 0.07207536697387695\n",
      "Iteration: 2960 loss: 0.0000000001 time: 0.0661478042602539\n",
      "Iteration: 2970 loss: 0.0000000001 time: 0.07211160659790039\n",
      "Iteration: 2980 loss: 0.0000000001 time: 0.0746612548828125\n",
      "Iteration: 2990 loss: 0.0000000001 time: 0.06979060173034668\n",
      "Iteration: 3000 loss: 0.0000000001 time: 0.07375121116638184\n",
      "Iteration: 3010 loss: 0.0000000001 time: 0.07312798500061035\n",
      "Iteration: 3020 loss: 0.0000000001 time: 0.06874227523803711\n",
      "Iteration: 3030 loss: 0.0000000001 time: 0.06022453308105469\n",
      "Iteration: 3040 loss: 0.0000000001 time: 0.06395554542541504\n",
      "Iteration: 3050 loss: 0.0000000001 time: 0.0577547550201416\n",
      "Iteration: 3060 loss: 0.0000000000 time: 0.06256508827209473\n",
      "Iteration: 3070 loss: 0.0000000000 time: 0.06572699546813965\n",
      "Iteration: 3080 loss: 0.0000000000 time: 0.06537485122680664\n",
      "Iteration: 3090 loss: 0.0000000000 time: 0.06589651107788086\n",
      "Iteration: 3100 loss: 0.0000000000 time: 0.06406545639038086\n",
      "Iteration: 3110 loss: 0.0000000000 time: 0.05985689163208008\n",
      "Iteration: 3120 loss: 0.0000000000 time: 0.05863595008850098\n",
      "Iteration: 3130 loss: 0.0000000000 time: 0.05762314796447754\n",
      "Iteration: 3140 loss: 0.0000000000 time: 0.059252262115478516\n",
      "Iteration: 3150 loss: 0.0000000000 time: 0.062268972396850586\n",
      "Iteration: 3160 loss: 0.0000000000 time: 0.06316828727722168\n",
      "Iteration: 3170 loss: 0.0000000000 time: 0.06228065490722656\n",
      "Iteration: 3180 loss: 0.0000000000 time: 0.06440186500549316\n",
      "Iteration: 3190 loss: 0.0000000000 time: 0.06440114974975586\n",
      "Iteration: 3200 loss: 0.0000000000 time: 0.06219768524169922\n",
      "Iteration: 3210 loss: 0.0000000000 time: 0.06302928924560547\n",
      "Iteration: 3220 loss: 0.0000000000 time: 0.06256532669067383\n",
      "Iteration: 3230 loss: 0.0000000000 time: 0.06890487670898438\n",
      "Iteration: 3240 loss: 0.0000000000 time: 0.06034064292907715\n",
      "Iteration: 3250 loss: 0.0000000000 time: 0.06015801429748535\n",
      "Iteration: 3260 loss: 0.0000000000 time: 0.06845259666442871\n",
      "Iteration: 3270 loss: 0.0000000000 time: 0.06856465339660645\n",
      "Iteration: 3280 loss: 0.0000000000 time: 0.06534171104431152\n",
      "Iteration: 3290 loss: 0.0000000000 time: 0.06697225570678711\n",
      "Iteration: 3300 loss: 0.0000000000 time: 0.06555771827697754\n",
      "Iteration: 3310 loss: 0.0000000000 time: 0.06807589530944824\n",
      "Iteration: 3320 loss: 0.0000000000 time: 0.06686949729919434\n",
      "Iteration: 3330 loss: 0.0000000000 time: 0.06510782241821289\n",
      "Iteration: 3340 loss: 0.0000000000 time: 0.0669248104095459\n",
      "Iteration: 3350 loss: 0.0000000000 time: 0.06634688377380371\n",
      "Iteration: 3360 loss: 0.0000000000 time: 0.06435585021972656\n",
      "Iteration: 3370 loss: 0.0000000000 time: 0.06520891189575195\n",
      "Iteration: 3380 loss: 0.0000000000 time: 0.06677865982055664\n",
      "Iteration: 3390 loss: 0.0000000000 time: 0.06642651557922363\n",
      "Iteration: 3400 loss: 0.0000000000 time: 0.06653857231140137\n",
      "Iteration: 3410 loss: 0.0000000000 time: 0.06595897674560547\n",
      "Iteration: 3420 loss: 0.0000000000 time: 0.06614112854003906\n",
      "Iteration: 3430 loss: 0.0000000000 time: 0.06841206550598145\n",
      "Iteration: 3440 loss: 0.0000000000 time: 0.06647729873657227\n",
      "Iteration: 3450 loss: 0.0000000000 time: 0.06424760818481445\n",
      "Iteration: 3460 loss: 0.0000000000 time: 0.06719517707824707\n",
      "Iteration: 3470 loss: 0.0000000000 time: 0.06580734252929688\n",
      "Iteration: 3480 loss: 0.0000000000 time: 0.06762075424194336\n",
      "Iteration: 3490 loss: 0.0000000000 time: 0.06820988655090332\n",
      "Iteration: 3500 loss: 0.0000000000 time: 0.061772823333740234\n",
      "Iteration: 3510 loss: 0.0000000000 time: 0.057572126388549805\n",
      "Iteration: 3520 loss: 0.0000000000 time: 0.05585646629333496\n",
      "Iteration: 3530 loss: 0.0000000000 time: 0.05812668800354004\n",
      "Iteration: 3540 loss: 0.0000000000 time: 0.05942368507385254\n",
      "Iteration: 3550 loss: 0.0000000000 time: 0.06032252311706543\n",
      "Iteration: 3560 loss: 0.0000000000 time: 0.0609898567199707\n",
      "Iteration: 3570 loss: 0.0000000000 time: 0.0651085376739502\n",
      "Iteration: 3580 loss: 0.0000000000 time: 0.06832337379455566\n",
      "Iteration: 3590 loss: 0.0000000000 time: 0.07154011726379395\n",
      "Iteration: 3600 loss: 0.0000000000 time: 0.06269097328186035\n",
      "Iteration: 3610 loss: 0.0000000000 time: 0.06433892250061035\n",
      "Iteration: 3620 loss: 0.0000000000 time: 0.06206941604614258\n",
      "Iteration: 3630 loss: 0.0000000000 time: 0.0678720474243164\n",
      "Iteration: 3640 loss: 0.0000000000 time: 0.06343555450439453\n",
      "Iteration: 3650 loss: 0.0000000000 time: 0.06661558151245117\n",
      "Iteration: 3660 loss: 0.0000000000 time: 0.06853389739990234\n",
      "Iteration: 3670 loss: 0.0000000000 time: 0.0697178840637207\n",
      "Iteration: 3680 loss: 0.0000000000 time: 0.06720709800720215\n",
      "Iteration: 3690 loss: 0.0000000000 time: 0.06756281852722168\n",
      "Iteration: 3700 loss: 0.0000000000 time: 0.07005023956298828\n",
      "Iteration: 3710 loss: 0.0000000000 time: 0.07081198692321777\n",
      "Iteration: 3720 loss: 0.0000000000 time: 0.06762528419494629\n",
      "Iteration: 3730 loss: 0.0000000000 time: 0.059792518615722656\n",
      "Iteration: 3740 loss: 0.0000000000 time: 0.06422185897827148\n",
      "Iteration: 3750 loss: 0.0000000000 time: 0.07095599174499512\n",
      "Iteration: 3760 loss: 0.0000000000 time: 0.0678093433380127\n",
      "Iteration: 3770 loss: 0.0000000000 time: 0.06980466842651367\n",
      "Iteration: 3780 loss: 0.0000000000 time: 0.07087302207946777\n",
      "Iteration: 3790 loss: 0.0000000000 time: 0.06805181503295898\n",
      "Iteration: 3800 loss: 0.0000000000 time: 0.07009029388427734\n",
      "Iteration: 3810 loss: 0.0000000000 time: 0.0720367431640625\n",
      "Iteration: 3820 loss: 0.0000000000 time: 0.06698870658874512\n",
      "Iteration: 3830 loss: 0.0000000000 time: 0.06999397277832031\n",
      "Iteration: 3840 loss: 0.0000000000 time: 0.07087254524230957\n",
      "Iteration: 3850 loss: 0.0000000000 time: 0.06961774826049805\n",
      "Iteration: 3860 loss: 0.0000000000 time: 0.07004380226135254\n",
      "Iteration: 3870 loss: 0.0000000000 time: 0.07299065589904785\n",
      "Iteration: 3880 loss: 0.0000000000 time: 0.06811857223510742\n",
      "Iteration: 3890 loss: 0.0000000000 time: 0.07159876823425293\n",
      "Iteration: 3900 loss: 0.0000000000 time: 0.06916618347167969\n",
      "Iteration: 3910 loss: 0.0000000000 time: 0.06477570533752441\n",
      "Iteration: 3920 loss: 0.0000000000 time: 0.06261157989501953\n",
      "Iteration: 3930 loss: 0.0000000000 time: 0.06490325927734375\n",
      "Iteration: 3940 loss: 0.0000000000 time: 0.06543588638305664\n",
      "Iteration: 3950 loss: 0.0000000000 time: 0.0708627700805664\n",
      "Iteration: 3960 loss: 0.0000000000 time: 0.06640243530273438\n",
      "Iteration: 3970 loss: 0.0000000000 time: 0.06836462020874023\n",
      "Iteration: 3980 loss: 0.0000000000 time: 0.06592917442321777\n",
      "Iteration: 3990 loss: 0.0000000000 time: 0.06547355651855469\n",
      "Iteration: 4000 loss: 0.0000000000 time: 0.06857466697692871\n",
      "Iteration: 4010 loss: 0.0000000000 time: 0.06623983383178711\n",
      "Iteration: 4020 loss: 0.0000000000 time: 0.06601476669311523\n",
      "Iteration: 4030 loss: 0.0000000000 time: 0.06640434265136719\n",
      "Iteration: 4040 loss: 0.0000000000 time: 0.06551885604858398\n",
      "Iteration: 4050 loss: 0.0000000000 time: 0.056957244873046875\n",
      "Iteration: 4060 loss: 0.0000000000 time: 0.056741952896118164\n",
      "Iteration: 4070 loss: 0.0000000000 time: 0.05717897415161133\n",
      "Iteration: 4080 loss: 0.0000000000 time: 0.06398129463195801\n",
      "Iteration: 4090 loss: 0.0000000000 time: 0.05946040153503418\n",
      "Iteration: 4100 loss: 0.0000000000 time: 0.06196856498718262\n",
      "Iteration: 4110 loss: 0.0000000000 time: 0.055394649505615234\n",
      "Iteration: 4120 loss: 0.0000000000 time: 0.05825304985046387\n",
      "Iteration: 4130 loss: 0.0000000000 time: 0.0635690689086914\n",
      "Iteration: 4140 loss: 0.0000000000 time: 0.06280732154846191\n",
      "Iteration: 4150 loss: 0.0000000000 time: 0.061333656311035156\n",
      "Iteration: 4160 loss: 0.0000000000 time: 0.061331748962402344\n",
      "Iteration: 4170 loss: 0.0000000000 time: 0.061473846435546875\n",
      "Iteration: 4180 loss: 0.0000000000 time: 0.061113595962524414\n",
      "Iteration: 4190 loss: 0.0000000000 time: 0.06818103790283203\n",
      "Iteration: 4200 loss: 0.0000000000 time: 0.0684194564819336\n",
      "Iteration: 4210 loss: 0.0000000000 time: 0.06906723976135254\n",
      "Iteration: 4220 loss: 0.0000000000 time: 0.06920528411865234\n",
      "Iteration: 4230 loss: 0.0000000000 time: 0.07038402557373047\n",
      "Iteration: 4240 loss: 0.0000000000 time: 0.06469202041625977\n",
      "Iteration: 4250 loss: 0.0000000000 time: 0.0693504810333252\n",
      "Iteration: 4260 loss: 0.0000000000 time: 0.07168769836425781\n",
      "Iteration: 4270 loss: 0.0000000000 time: 0.0690760612487793\n",
      "Iteration: 4280 loss: 0.0000000000 time: 0.06871962547302246\n",
      "Iteration: 4290 loss: 0.0000000000 time: 0.06667232513427734\n",
      "Iteration: 4300 loss: 0.0000000000 time: 0.06987953186035156\n",
      "Iteration: 4310 loss: 0.0000000000 time: 0.06838083267211914\n",
      "Iteration: 4320 loss: 0.0000000000 time: 0.06672024726867676\n",
      "Iteration: 4330 loss: 0.0000000000 time: 0.07187581062316895\n",
      "Iteration: 4340 loss: 0.0000000000 time: 0.07187318801879883\n",
      "Iteration: 4350 loss: 0.0000000000 time: 0.07320094108581543\n",
      "Iteration: 4360 loss: 0.0000000000 time: 0.07550978660583496\n",
      "Iteration: 4370 loss: 0.0000000000 time: 0.06542229652404785\n",
      "Iteration: 4380 loss: 0.0000000000 time: 0.0632777214050293\n",
      "Iteration: 4390 loss: 0.0000000000 time: 0.06599593162536621\n",
      "Iteration: 4400 loss: 0.0000000000 time: 0.0626680850982666\n",
      "Iteration: 4410 loss: 0.0000000000 time: 0.06850337982177734\n",
      "Iteration: 4420 loss: 0.0000000000 time: 0.06699037551879883\n",
      "Iteration: 4430 loss: 0.0000000000 time: 0.06645584106445312\n",
      "Iteration: 4440 loss: 0.0000000000 time: 0.06412482261657715\n",
      "Iteration: 4450 loss: 0.0000000000 time: 0.06704831123352051\n",
      "Iteration: 4460 loss: 0.0000000000 time: 0.06709551811218262\n",
      "Iteration: 4470 loss: 0.0000000000 time: 0.0654752254486084\n",
      "Iteration: 4480 loss: 0.0000000000 time: 0.06606721878051758\n",
      "Iteration: 4490 loss: 0.0000000000 time: 0.06545400619506836\n",
      "Iteration: 4500 loss: 0.0000000000 time: 0.0685577392578125\n",
      "Iteration: 4510 loss: 0.0000000000 time: 0.06709456443786621\n",
      "Iteration: 4520 loss: 0.0000000000 time: 0.06492853164672852\n",
      "Iteration: 4530 loss: 0.0000000000 time: 0.07011151313781738\n",
      "Iteration: 4540 loss: 0.0000000000 time: 0.06938052177429199\n",
      "Iteration: 4550 loss: 0.0000000000 time: 0.06623625755310059\n",
      "Iteration: 4560 loss: 0.0000000000 time: 0.0643925666809082\n",
      "Iteration: 4570 loss: 0.0000000000 time: 0.06951236724853516\n",
      "Iteration: 4580 loss: 0.0000000000 time: 0.05867147445678711\n",
      "Iteration: 4590 loss: 0.0000000000 time: 0.05917549133300781\n",
      "Iteration: 4600 loss: 0.0000000000 time: 0.059705495834350586\n",
      "Iteration: 4610 loss: 0.0000000000 time: 0.06144833564758301\n",
      "Iteration: 4620 loss: 0.0000000000 time: 0.06484007835388184\n",
      "Iteration: 4630 loss: 0.0000000000 time: 0.06452250480651855\n",
      "Iteration: 4640 loss: 0.0000000000 time: 0.06648898124694824\n",
      "Iteration: 4650 loss: 0.0000000000 time: 0.06843447685241699\n",
      "Iteration: 4660 loss: 0.0000000000 time: 0.06639814376831055\n",
      "Iteration: 4670 loss: 0.0000000000 time: 0.06588625907897949\n",
      "Iteration: 4680 loss: 0.0000000000 time: 0.06699132919311523\n",
      "Iteration: 4690 loss: 0.0000000000 time: 0.06629204750061035\n",
      "Iteration: 4700 loss: 0.0000000000 time: 0.06658053398132324\n",
      "Iteration: 4710 loss: 0.0000000000 time: 0.06526398658752441\n",
      "Iteration: 4720 loss: 0.0000000000 time: 0.06376910209655762\n",
      "Iteration: 4730 loss: 0.0000000000 time: 0.06766152381896973\n",
      "Iteration: 4740 loss: 0.0000000000 time: 0.0702199935913086\n",
      "Iteration: 4750 loss: 0.0000000000 time: 0.06945252418518066\n",
      "Iteration: 4760 loss: 0.0000000000 time: 0.06990289688110352\n",
      "Iteration: 4770 loss: 0.0000000000 time: 0.07067728042602539\n",
      "Iteration: 4780 loss: 0.0000000000 time: 0.06992483139038086\n",
      "Iteration: 4790 loss: 0.0000000000 time: 0.07024741172790527\n",
      "Iteration: 4800 loss: 0.0000000000 time: 0.07242155075073242\n",
      "Iteration: 4810 loss: 0.0000000000 time: 0.06957507133483887\n",
      "Iteration: 4820 loss: 0.0000000000 time: 0.0684363842010498\n",
      "Iteration: 4830 loss: 0.0000000000 time: 0.06401753425598145\n",
      "Iteration: 4840 loss: 0.0000000000 time: 0.06538701057434082\n",
      "Iteration: 4850 loss: 0.0000000000 time: 0.06819415092468262\n",
      "Iteration: 4860 loss: 0.0000000000 time: 0.06757998466491699\n",
      "Iteration: 4870 loss: 0.0000000000 time: 0.06642913818359375\n",
      "Iteration: 4880 loss: 0.0000000000 time: 0.06316924095153809\n",
      "Iteration: 4890 loss: 0.0000000000 time: 0.06100153923034668\n",
      "Iteration: 4900 loss: 0.0000000000 time: 0.06278324127197266\n",
      "Iteration: 4910 loss: 0.0000000000 time: 0.06084036827087402\n",
      "Iteration: 4920 loss: 0.0000000000 time: 0.0608828067779541\n",
      "Iteration: 4930 loss: 0.0000000000 time: 0.06024336814880371\n",
      "Iteration: 4940 loss: 0.0000000000 time: 0.06232714653015137\n",
      "Iteration: 4950 loss: 0.0000000000 time: 0.060506582260131836\n",
      "Iteration: 4960 loss: 0.0000000000 time: 0.06219124794006348\n",
      "Iteration: 4970 loss: 0.0000000000 time: 0.061183929443359375\n",
      "Iteration: 4980 loss: 0.0000000000 time: 0.06458258628845215\n",
      "Iteration: 4990 loss: 0.0000000000 time: 0.06289076805114746\n",
      "Iteration: 5000 loss: 0.0000000000 time: 0.06130647659301758\n",
      "Iteration: 5010 loss: 0.0000000000 time: 0.06206536293029785\n",
      "Iteration: 5020 loss: 0.0000000000 time: 0.06536483764648438\n",
      "Iteration: 5030 loss: 0.0000000000 time: 0.06588125228881836\n",
      "Iteration: 5040 loss: 0.0000000000 time: 0.07131791114807129\n",
      "Iteration: 5050 loss: 0.0000000000 time: 0.07202339172363281\n",
      "Iteration: 5060 loss: 0.0000000000 time: 0.06807470321655273\n",
      "Iteration: 5070 loss: 0.0000000000 time: 0.060450077056884766\n",
      "Iteration: 5080 loss: 0.0000000000 time: 0.05745220184326172\n",
      "Iteration: 5090 loss: 0.0000000000 time: 0.059967756271362305\n",
      "Iteration: 5100 loss: 0.0000000000 time: 0.06464505195617676\n",
      "Iteration: 5110 loss: 0.0000000000 time: 0.06984138488769531\n",
      "Iteration: 5120 loss: 0.0000000000 time: 0.06911325454711914\n",
      "Iteration: 5130 loss: 0.0000000000 time: 0.07025647163391113\n",
      "Iteration: 5140 loss: 0.0000000000 time: 0.06886672973632812\n",
      "Iteration: 5150 loss: 0.0000000000 time: 0.06778550148010254\n",
      "Iteration: 5160 loss: 0.0000000000 time: 0.07113146781921387\n",
      "Iteration: 5170 loss: 0.0000000000 time: 0.07140994071960449\n",
      "Iteration: 5180 loss: 0.0000000000 time: 0.06541919708251953\n",
      "Iteration: 5190 loss: 0.0000000000 time: 0.06610822677612305\n",
      "Iteration: 5200 loss: 0.0000000000 time: 0.0733785629272461\n",
      "Iteration: 5210 loss: 0.0000000000 time: 0.07390165328979492\n",
      "Iteration: 5220 loss: 0.0000000000 time: 0.07362937927246094\n",
      "Iteration: 5230 loss: 0.0000000000 time: 0.07197880744934082\n",
      "Iteration: 5240 loss: 0.0000000000 time: 0.0740046501159668\n",
      "Iteration: 5250 loss: 0.0000000000 time: 0.0632171630859375\n",
      "Iteration: 5260 loss: 0.0000000000 time: 0.06013131141662598\n",
      "Iteration: 5270 loss: 0.0000000000 time: 0.05902266502380371\n",
      "Iteration: 5280 loss: 0.0000000000 time: 0.07043671607971191\n",
      "Iteration: 5290 loss: 0.0000000000 time: 0.06950926780700684\n",
      "Iteration: 5300 loss: 0.0000000000 time: 0.06762313842773438\n",
      "Iteration: 5310 loss: 0.0000000000 time: 0.06972217559814453\n",
      "Iteration: 5320 loss: 0.0000000000 time: 0.07229137420654297\n",
      "Iteration: 5330 loss: 0.0000000000 time: 0.06820893287658691\n",
      "Iteration: 5340 loss: 0.0000000000 time: 0.06963729858398438\n",
      "Iteration: 5350 loss: 0.0000000000 time: 0.0707387924194336\n",
      "Iteration: 5360 loss: 0.0000000000 time: 0.06931138038635254\n",
      "Iteration: 5370 loss: 0.0000000000 time: 0.06988048553466797\n",
      "Iteration: 5380 loss: 0.0000000001 time: 0.07078886032104492\n",
      "Iteration: 5390 loss: 0.0000003103 time: 0.07068586349487305\n",
      "Iteration: 5400 loss: 0.0000023890 time: 0.06291079521179199\n",
      "Iteration: 5410 loss: 0.0000044840 time: 0.07289791107177734\n",
      "Iteration: 5420 loss: 0.0000003725 time: 0.05963277816772461\n",
      "Iteration: 5430 loss: 0.0000009526 time: 0.07154726982116699\n",
      "Iteration: 5440 loss: 0.0000002086 time: 0.06841707229614258\n",
      "Iteration: 5450 loss: 0.0000000037 time: 0.05982613563537598\n",
      "Iteration: 5460 loss: 0.0000000415 time: 0.060094356536865234\n",
      "Iteration: 5470 loss: 0.0000000052 time: 0.06533694267272949\n",
      "Iteration: 5480 loss: 0.0000000027 time: 0.06549739837646484\n",
      "Iteration: 5490 loss: 0.0000000013 time: 0.06541728973388672\n",
      "Iteration: 5500 loss: 0.0000000003 time: 0.06974124908447266\n",
      "Iteration: 5510 loss: 0.0000000001 time: 0.07027721405029297\n",
      "Iteration: 5520 loss: 0.0000000001 time: 0.06604409217834473\n",
      "Iteration: 5530 loss: 0.0000000000 time: 0.06604218482971191\n",
      "Iteration: 5540 loss: 0.0000000000 time: 0.06802105903625488\n",
      "Iteration: 5550 loss: 0.0000000000 time: 0.06792640686035156\n",
      "Iteration: 5560 loss: 0.0000000000 time: 0.06681394577026367\n",
      "Iteration: 5570 loss: 0.0000000000 time: 0.06511855125427246\n",
      "Iteration: 5580 loss: 0.0000000000 time: 0.06963038444519043\n",
      "Iteration: 5590 loss: 0.0000000000 time: 0.06769847869873047\n",
      "Iteration: 5600 loss: 0.0000000000 time: 0.06293869018554688\n",
      "Iteration: 5610 loss: 0.0000000000 time: 0.06151437759399414\n",
      "Iteration: 5620 loss: 0.0000000000 time: 0.06761312484741211\n",
      "Iteration: 5630 loss: 0.0000000000 time: 0.06333446502685547\n",
      "Iteration: 5640 loss: 0.0000000000 time: 0.07135510444641113\n",
      "Iteration: 5650 loss: 0.0000000000 time: 0.07282137870788574\n",
      "Iteration: 5660 loss: 0.0000000000 time: 0.0753786563873291\n",
      "Iteration: 5670 loss: 0.0000000000 time: 0.07096505165100098\n",
      "Iteration: 5680 loss: 0.0000000000 time: 0.0723257064819336\n",
      "Iteration: 5690 loss: 0.0000000000 time: 0.07099533081054688\n",
      "Iteration: 5700 loss: 0.0000000000 time: 0.0678853988647461\n",
      "Iteration: 5710 loss: 0.0000000000 time: 0.0737755298614502\n",
      "Iteration: 5720 loss: 0.0000000000 time: 0.0736696720123291\n",
      "Iteration: 5730 loss: 0.0000000000 time: 0.07299399375915527\n",
      "Iteration: 5740 loss: 0.0000000000 time: 0.07374954223632812\n",
      "Iteration: 5750 loss: 0.0000000000 time: 0.0690305233001709\n",
      "Iteration: 5760 loss: 0.0000000000 time: 0.06736040115356445\n",
      "Iteration: 5770 loss: 0.0000000000 time: 0.06286287307739258\n",
      "Iteration: 5780 loss: 0.0000000000 time: 0.06030631065368652\n",
      "Iteration: 5790 loss: 0.0000000000 time: 0.05935931205749512\n",
      "Iteration: 5800 loss: 0.0000000000 time: 0.058670759201049805\n",
      "Iteration: 5810 loss: 0.0000000000 time: 0.05842852592468262\n",
      "Iteration: 5820 loss: 0.0000000000 time: 0.06538009643554688\n",
      "Iteration: 5830 loss: 0.0000000000 time: 0.06488537788391113\n",
      "Iteration: 5840 loss: 0.0000000000 time: 0.0633840560913086\n",
      "Iteration: 5850 loss: 0.0000000000 time: 0.07086324691772461\n",
      "Iteration: 5860 loss: 0.0000000000 time: 0.06990361213684082\n",
      "Iteration: 5870 loss: 0.0000000000 time: 0.07029438018798828\n",
      "Iteration: 5880 loss: 0.0000000000 time: 0.06885552406311035\n",
      "Iteration: 5890 loss: 0.0000000063 time: 0.0696561336517334\n",
      "Iteration: 5900 loss: 0.0000074368 time: 0.06452417373657227\n",
      "Iteration: 5910 loss: 0.0000193157 time: 0.05912208557128906\n",
      "Iteration: 5920 loss: 0.0000013130 time: 0.06902050971984863\n",
      "Iteration: 5930 loss: 0.0000006952 time: 0.06989073753356934\n",
      "Iteration: 5940 loss: 0.0000008591 time: 0.06922769546508789\n",
      "Iteration: 5950 loss: 0.0000001826 time: 0.06911277770996094\n",
      "Iteration: 5960 loss: 0.0000000013 time: 0.0708615779876709\n",
      "Iteration: 5970 loss: 0.0000000329 time: 0.07185912132263184\n",
      "Iteration: 5980 loss: 0.0000000075 time: 0.07052898406982422\n",
      "Iteration: 5990 loss: 0.0000000011 time: 0.07156920433044434\n",
      "Iteration: 6000 loss: 0.0000000016 time: 0.07102656364440918\n",
      "Iteration: 6010 loss: 0.0000000001 time: 0.0688009262084961\n",
      "Iteration: 6020 loss: 0.0000000002 time: 0.0690760612487793\n",
      "Iteration: 6030 loss: 0.0000000000 time: 0.07148480415344238\n",
      "Iteration: 6040 loss: 0.0000000000 time: 0.06821823120117188\n",
      "Iteration: 6050 loss: 0.0000000000 time: 0.06983208656311035\n",
      "Iteration: 6060 loss: 0.0000000000 time: 0.07276201248168945\n",
      "Iteration: 6070 loss: 0.0000000000 time: 0.06651091575622559\n",
      "Iteration: 6080 loss: 0.0000000000 time: 0.05886197090148926\n",
      "Iteration: 6090 loss: 0.0000000000 time: 0.061090707778930664\n",
      "Iteration: 6100 loss: 0.0000000000 time: 0.059324026107788086\n",
      "Iteration: 6110 loss: 0.0000000000 time: 0.06209683418273926\n",
      "Iteration: 6120 loss: 0.0000000000 time: 0.06849288940429688\n",
      "Iteration: 6130 loss: 0.0000000000 time: 0.0707249641418457\n",
      "Iteration: 6140 loss: 0.0000000000 time: 0.07038545608520508\n",
      "Iteration: 6150 loss: 0.0000000000 time: 0.060952186584472656\n",
      "Iteration: 6160 loss: 0.0000000000 time: 0.06309199333190918\n",
      "Iteration: 6170 loss: 0.0000000000 time: 0.06111025810241699\n",
      "Iteration: 6180 loss: 0.0000000000 time: 0.06101393699645996\n",
      "Iteration: 6190 loss: 0.0000000000 time: 0.06275820732116699\n",
      "Iteration: 6200 loss: 0.0000000000 time: 0.06406950950622559\n",
      "Iteration: 6210 loss: 0.0000000000 time: 0.06601786613464355\n",
      "Iteration: 6220 loss: 0.0000000000 time: 0.06799674034118652\n",
      "Iteration: 6230 loss: 0.0000000000 time: 0.07033729553222656\n",
      "Iteration: 6240 loss: 0.0000000000 time: 0.07199239730834961\n",
      "Iteration: 6250 loss: 0.0000000000 time: 0.07236742973327637\n",
      "Iteration: 6260 loss: 0.0000000000 time: 0.07159185409545898\n",
      "Iteration: 6270 loss: 0.0000000000 time: 0.07279372215270996\n",
      "Iteration: 6280 loss: 0.0000000000 time: 0.07335615158081055\n",
      "Iteration: 6290 loss: 0.0000000000 time: 0.07120442390441895\n",
      "Iteration: 6300 loss: 0.0000000000 time: 0.0605013370513916\n",
      "Iteration: 6310 loss: 0.0000000000 time: 0.06289839744567871\n",
      "Iteration: 6320 loss: 0.0000000000 time: 0.06949973106384277\n",
      "Iteration: 6330 loss: 0.0000000000 time: 0.06791496276855469\n",
      "Iteration: 6340 loss: 0.0000000000 time: 0.07051563262939453\n",
      "Iteration: 6350 loss: 0.0000000000 time: 0.07085227966308594\n",
      "Iteration: 6360 loss: 0.0000000000 time: 0.06979918479919434\n",
      "Iteration: 6370 loss: 0.0000000000 time: 0.07421994209289551\n",
      "Iteration: 6380 loss: 0.0000000000 time: 0.07330107688903809\n",
      "Iteration: 6390 loss: 0.0000000000 time: 0.0726158618927002\n",
      "Iteration: 6400 loss: 0.0000000000 time: 0.07215023040771484\n",
      "Iteration: 6410 loss: 0.0000000099 time: 0.06702589988708496\n",
      "Iteration: 6420 loss: 0.0000132909 time: 0.061409711837768555\n",
      "Iteration: 6430 loss: 0.0000061085 time: 0.0621030330657959\n",
      "Iteration: 6440 loss: 0.0000080989 time: 0.06687808036804199\n",
      "Iteration: 6450 loss: 0.0000017364 time: 0.06539082527160645\n",
      "Iteration: 6460 loss: 0.0000002095 time: 0.06662178039550781\n",
      "Iteration: 6470 loss: 0.0000000008 time: 0.07221436500549316\n",
      "Iteration: 6480 loss: 0.0000000300 time: 0.07380557060241699\n",
      "Iteration: 6490 loss: 0.0000000357 time: 0.07151269912719727\n",
      "Iteration: 6500 loss: 0.0000000139 time: 0.07453036308288574\n",
      "Iteration: 6510 loss: 0.0000000010 time: 0.07305383682250977\n",
      "Iteration: 6520 loss: 0.0000000005 time: 0.07213211059570312\n",
      "Iteration: 6530 loss: 0.0000000007 time: 0.07323479652404785\n",
      "Iteration: 6540 loss: 0.0000000001 time: 0.06649136543273926\n",
      "Iteration: 6550 loss: 0.0000000001 time: 0.06436657905578613\n",
      "Iteration: 6560 loss: 0.0000000000 time: 0.057393550872802734\n",
      "Iteration: 6570 loss: 0.0000000000 time: 0.0592648983001709\n",
      "Iteration: 6580 loss: 0.0000000000 time: 0.06015920639038086\n",
      "Iteration: 6590 loss: 0.0000000000 time: 0.06527948379516602\n",
      "Iteration: 6600 loss: 0.0000000000 time: 0.06627702713012695\n",
      "Iteration: 6610 loss: 0.0000000000 time: 0.06782102584838867\n",
      "Iteration: 6620 loss: 0.0000000000 time: 0.06691122055053711\n",
      "Iteration: 6630 loss: 0.0000000000 time: 0.06607866287231445\n",
      "Iteration: 6640 loss: 0.0000000000 time: 0.06645870208740234\n",
      "Iteration: 6650 loss: 0.0000000000 time: 0.0668647289276123\n",
      "Iteration: 6660 loss: 0.0000000000 time: 0.06633400917053223\n",
      "Iteration: 6670 loss: 0.0000000000 time: 0.07134246826171875\n",
      "Iteration: 6680 loss: 0.0000000000 time: 0.0711824893951416\n",
      "Iteration: 6690 loss: 0.0000000000 time: 0.07273674011230469\n",
      "Iteration: 6700 loss: 0.0000000000 time: 0.07188177108764648\n",
      "Iteration: 6710 loss: 0.0000000000 time: 0.0718083381652832\n",
      "Iteration: 6720 loss: 0.0000000000 time: 0.06630992889404297\n",
      "Iteration: 6730 loss: 0.0000000000 time: 0.06179928779602051\n",
      "Iteration: 6740 loss: 0.0000000000 time: 0.06671428680419922\n",
      "Iteration: 6750 loss: 0.0000000000 time: 0.06610298156738281\n",
      "Iteration: 6760 loss: 0.0000000000 time: 0.06816387176513672\n",
      "Iteration: 6770 loss: 0.0000000000 time: 0.06401681900024414\n",
      "Iteration: 6780 loss: 0.0000000000 time: 0.06598615646362305\n",
      "Iteration: 6790 loss: 0.0000000000 time: 0.06820225715637207\n",
      "Iteration: 6800 loss: 0.0000000000 time: 0.06629395484924316\n",
      "Iteration: 6810 loss: 0.0000000000 time: 0.0646514892578125\n",
      "Iteration: 6820 loss: 0.0000000000 time: 0.06569552421569824\n",
      "Iteration: 6830 loss: 0.0000000000 time: 0.0668632984161377\n",
      "Iteration: 6840 loss: 0.0000000000 time: 0.06535077095031738\n",
      "Iteration: 6850 loss: 0.0000000000 time: 0.0650322437286377\n",
      "Iteration: 6860 loss: 0.0000000000 time: 0.06654548645019531\n",
      "Iteration: 6870 loss: 0.0000000000 time: 0.0674278736114502\n",
      "Iteration: 6880 loss: 0.0000000000 time: 0.0677480697631836\n",
      "Iteration: 6890 loss: 0.0000000000 time: 0.07021808624267578\n",
      "Iteration: 6900 loss: 0.0000000000 time: 0.07141876220703125\n",
      "Iteration: 6910 loss: 0.0000000000 time: 0.07284021377563477\n",
      "Iteration: 6920 loss: 0.0000000000 time: 0.07177329063415527\n",
      "Iteration: 6930 loss: 0.0000000000 time: 0.07213354110717773\n",
      "Iteration: 6940 loss: 0.0000000000 time: 0.07439470291137695\n",
      "Iteration: 6950 loss: 0.0000000000 time: 0.06325054168701172\n",
      "Iteration: 6960 loss: 0.0000000000 time: 0.0643777847290039\n",
      "Iteration: 6970 loss: 0.0000000001 time: 0.06470179557800293\n",
      "Iteration: 6980 loss: 0.0000000928 time: 0.062393903732299805\n",
      "Iteration: 6990 loss: 0.0000706708 time: 0.06332588195800781\n",
      "Iteration: 7000 loss: 0.0000074309 time: 0.0641012191772461\n",
      "Iteration: 7010 loss: 0.0000049836 time: 0.06508779525756836\n",
      "Iteration: 7020 loss: 0.0000017083 time: 0.0709381103515625\n",
      "Iteration: 7030 loss: 0.0000005479 time: 0.07291293144226074\n",
      "Iteration: 7040 loss: 0.0000001861 time: 0.06894254684448242\n",
      "Iteration: 7050 loss: 0.0000000701 time: 0.07105731964111328\n",
      "Iteration: 7060 loss: 0.0000000284 time: 0.07304978370666504\n",
      "Iteration: 7070 loss: 0.0000000116 time: 0.07210016250610352\n",
      "Iteration: 7080 loss: 0.0000000042 time: 0.07124829292297363\n",
      "Iteration: 7090 loss: 0.0000000011 time: 0.061157941818237305\n",
      "Iteration: 7100 loss: 0.0000000002 time: 0.06346511840820312\n",
      "Iteration: 7110 loss: 0.0000000000 time: 0.06608247756958008\n",
      "Iteration: 7120 loss: 0.0000000000 time: 0.06302142143249512\n",
      "Iteration: 7130 loss: 0.0000000000 time: 0.06214332580566406\n",
      "Iteration: 7140 loss: 0.0000000000 time: 0.060979604721069336\n",
      "Iteration: 7150 loss: 0.0000000000 time: 0.06230497360229492\n",
      "Iteration: 7160 loss: 0.0000000000 time: 0.06138324737548828\n",
      "Iteration: 7170 loss: 0.0000000000 time: 0.06173563003540039\n",
      "Iteration: 7180 loss: 0.0000000000 time: 0.06332540512084961\n",
      "Iteration: 7190 loss: 0.0000000000 time: 0.06311368942260742\n",
      "Iteration: 7200 loss: 0.0000000000 time: 0.06486368179321289\n",
      "Iteration: 7210 loss: 0.0000000000 time: 0.0673227310180664\n",
      "Iteration: 7220 loss: 0.0000000000 time: 0.06509566307067871\n",
      "Iteration: 7230 loss: 0.0000000000 time: 0.06499886512756348\n",
      "Iteration: 7240 loss: 0.0000000000 time: 0.06560945510864258\n",
      "Iteration: 7250 loss: 0.0000000000 time: 0.0677347183227539\n",
      "Iteration: 7260 loss: 0.0000000000 time: 0.0665433406829834\n",
      "Iteration: 7270 loss: 0.0000000000 time: 0.0665888786315918\n",
      "Iteration: 7280 loss: 0.0000000000 time: 0.06549191474914551\n",
      "Iteration: 7290 loss: 0.0000000000 time: 0.06711435317993164\n",
      "Iteration: 7300 loss: 0.0000000000 time: 0.06587028503417969\n",
      "Iteration: 7310 loss: 0.0000000000 time: 0.06477665901184082\n",
      "Iteration: 7320 loss: 0.0000000000 time: 0.0661773681640625\n",
      "Iteration: 7330 loss: 0.0000000000 time: 0.06899714469909668\n",
      "Iteration: 7340 loss: 0.0000000000 time: 0.06781959533691406\n",
      "Iteration: 7350 loss: 0.0000000000 time: 0.0586085319519043\n",
      "Iteration: 7360 loss: 0.0000000000 time: 0.06414318084716797\n",
      "Iteration: 7370 loss: 0.0000000000 time: 0.06303548812866211\n",
      "Iteration: 7380 loss: 0.0000000000 time: 0.060666799545288086\n",
      "Iteration: 7390 loss: 0.0000000000 time: 0.06200909614562988\n",
      "Iteration: 7400 loss: 0.0000000000 time: 0.06579399108886719\n",
      "Iteration: 7410 loss: 0.0000000000 time: 0.07237386703491211\n",
      "Iteration: 7420 loss: 0.0000000000 time: 0.07312750816345215\n",
      "Iteration: 7430 loss: 0.0000000000 time: 0.07293534278869629\n",
      "Iteration: 7440 loss: 0.0000000000 time: 0.07414913177490234\n",
      "Iteration: 7450 loss: 0.0000000000 time: 0.07337713241577148\n",
      "Iteration: 7460 loss: 0.0000000000 time: 0.07288217544555664\n",
      "Iteration: 7470 loss: 0.0000000000 time: 0.07256722450256348\n",
      "Iteration: 7480 loss: 0.0000000000 time: 0.074859619140625\n",
      "Iteration: 7490 loss: 0.0000000000 time: 0.07206869125366211\n",
      "Iteration: 7500 loss: 0.0000000000 time: 0.07489800453186035\n",
      "Iteration: 7510 loss: 0.0000000000 time: 0.07444214820861816\n",
      "Iteration: 7520 loss: 0.0000000000 time: 0.07431459426879883\n",
      "Iteration: 7530 loss: 0.0000000000 time: 0.07343268394470215\n",
      "Iteration: 7540 loss: 0.0000000000 time: 0.06227684020996094\n",
      "Iteration: 7550 loss: 0.0000000000 time: 0.0669856071472168\n",
      "Iteration: 7560 loss: 0.0000000000 time: 0.07255339622497559\n",
      "Iteration: 7570 loss: 0.0000000000 time: 0.06807327270507812\n",
      "Iteration: 7580 loss: 0.0000000000 time: 0.06591653823852539\n",
      "Iteration: 7590 loss: 0.0000000000 time: 0.06542205810546875\n",
      "Iteration: 7600 loss: 0.0000000013 time: 0.0679783821105957\n",
      "Iteration: 7610 loss: 0.0000030835 time: 0.06707954406738281\n",
      "Iteration: 7620 loss: 0.0000393313 time: 0.06622529029846191\n",
      "Iteration: 7630 loss: 0.0000067821 time: 0.06655430793762207\n",
      "Iteration: 7640 loss: 0.0000032892 time: 0.06461048126220703\n",
      "Iteration: 7650 loss: 0.0000014358 time: 0.06104016304016113\n",
      "Iteration: 7660 loss: 0.0000003798 time: 0.06375432014465332\n",
      "Iteration: 7670 loss: 0.0000000580 time: 0.05934739112854004\n",
      "Iteration: 7680 loss: 0.0000000036 time: 0.06411266326904297\n",
      "Iteration: 7690 loss: 0.0000000003 time: 0.07182884216308594\n",
      "Iteration: 7700 loss: 0.0000000008 time: 0.0698390007019043\n",
      "Iteration: 7710 loss: 0.0000000005 time: 0.07493376731872559\n",
      "Iteration: 7720 loss: 0.0000000002 time: 0.06978130340576172\n",
      "Iteration: 7730 loss: 0.0000000000 time: 0.06350469589233398\n",
      "Iteration: 7740 loss: 0.0000000000 time: 0.06490325927734375\n",
      "Iteration: 7750 loss: 0.0000000000 time: 0.06645822525024414\n",
      "Iteration: 7760 loss: 0.0000000000 time: 0.06660771369934082\n",
      "Iteration: 7770 loss: 0.0000000000 time: 0.06404900550842285\n",
      "Iteration: 7780 loss: 0.0000000000 time: 0.06303930282592773\n",
      "Iteration: 7790 loss: 0.0000000000 time: 0.06364083290100098\n",
      "Iteration: 7800 loss: 0.0000000000 time: 0.06270551681518555\n",
      "Iteration: 7810 loss: 0.0000000000 time: 0.062432050704956055\n",
      "Iteration: 7820 loss: 0.0000000000 time: 0.06183290481567383\n",
      "Iteration: 7830 loss: 0.0000000000 time: 0.06373381614685059\n",
      "Iteration: 7840 loss: 0.0000000000 time: 0.062058210372924805\n",
      "Iteration: 7850 loss: 0.0000000000 time: 0.059914350509643555\n",
      "Iteration: 7860 loss: 0.0000000000 time: 0.06130695343017578\n",
      "Iteration: 7870 loss: 0.0000000000 time: 0.06556558609008789\n",
      "Iteration: 7880 loss: 0.0000000000 time: 0.06526613235473633\n",
      "Iteration: 7890 loss: 0.0000000000 time: 0.06586885452270508\n",
      "Iteration: 7900 loss: 0.0000000000 time: 0.06499075889587402\n",
      "Iteration: 7910 loss: 0.0000000000 time: 0.06793355941772461\n",
      "Iteration: 7920 loss: 0.0000000000 time: 0.06644392013549805\n",
      "Iteration: 7930 loss: 0.0000000000 time: 0.06598877906799316\n",
      "Iteration: 7940 loss: 0.0000000000 time: 0.06581783294677734\n",
      "Iteration: 7950 loss: 0.0000000000 time: 0.06818389892578125\n",
      "Iteration: 7960 loss: 0.0000000000 time: 0.06652665138244629\n",
      "Iteration: 7970 loss: 0.0000000000 time: 0.07032561302185059\n",
      "Iteration: 7980 loss: 0.0000000000 time: 0.0746152400970459\n",
      "Iteration: 7990 loss: 0.0000000000 time: 0.0734705924987793\n",
      "Iteration: 8000 loss: 0.0000000000 time: 0.07131171226501465\n",
      "Iteration: 8010 loss: 0.0000000000 time: 0.07279229164123535\n",
      "Iteration: 8020 loss: 0.0000000000 time: 0.07534003257751465\n",
      "Iteration: 8030 loss: 0.0000000000 time: 0.06958699226379395\n",
      "Iteration: 8040 loss: 0.0000000000 time: 0.06126904487609863\n",
      "Iteration: 8050 loss: 0.0000000000 time: 0.07366371154785156\n",
      "Iteration: 8060 loss: 0.0000000000 time: 0.06909799575805664\n",
      "Iteration: 8070 loss: 0.0000000000 time: 0.06973409652709961\n",
      "Iteration: 8080 loss: 0.0000000000 time: 0.07153463363647461\n",
      "Iteration: 8090 loss: 0.0000000000 time: 0.06809878349304199\n",
      "Iteration: 8100 loss: 0.0000000000 time: 0.07152748107910156\n",
      "Iteration: 8110 loss: 0.0000000000 time: 0.07261800765991211\n",
      "Iteration: 8120 loss: 0.0000000000 time: 0.07060837745666504\n",
      "Iteration: 8130 loss: 0.0000000000 time: 0.07095527648925781\n",
      "Iteration: 8140 loss: 0.0000000000 time: 0.07175230979919434\n",
      "Iteration: 8150 loss: 0.0000000000 time: 0.06900858879089355\n",
      "Iteration: 8160 loss: 0.0000000000 time: 0.0733489990234375\n",
      "Iteration: 8170 loss: 0.0000000000 time: 0.07840657234191895\n",
      "Iteration: 8180 loss: 0.0000000000 time: 0.0706636905670166\n",
      "Iteration: 8190 loss: 0.0000000000 time: 0.06137251853942871\n",
      "Iteration: 8200 loss: 0.0000000000 time: 0.06389331817626953\n",
      "Iteration: 8210 loss: 0.0000000000 time: 0.06051349639892578\n",
      "Iteration: 8220 loss: 0.0000000000 time: 0.06925559043884277\n",
      "Iteration: 8230 loss: 0.0000000000 time: 0.0699920654296875\n",
      "Iteration: 8240 loss: 0.0000000000 time: 0.06373095512390137\n",
      "Iteration: 8250 loss: 0.0000000000 time: 0.06065249443054199\n",
      "Iteration: 8260 loss: 0.0000000000 time: 0.06120181083679199\n",
      "Iteration: 8270 loss: 0.0000000000 time: 0.06317663192749023\n",
      "Iteration: 8280 loss: 0.0000000012 time: 0.060843467712402344\n",
      "Iteration: 8290 loss: 0.0000036704 time: 0.060698509216308594\n",
      "Iteration: 8300 loss: 0.0000371354 time: 0.0609431266784668\n",
      "Iteration: 8310 loss: 0.0000120004 time: 0.06225848197937012\n",
      "Iteration: 8320 loss: 0.0000028434 time: 0.06690096855163574\n",
      "Iteration: 8330 loss: 0.0000000400 time: 0.07037472724914551\n",
      "Iteration: 8340 loss: 0.0000002018 time: 0.07166337966918945\n",
      "Iteration: 8350 loss: 0.0000001745 time: 0.06508731842041016\n",
      "Iteration: 8360 loss: 0.0000000381 time: 0.06885886192321777\n",
      "Iteration: 8370 loss: 0.0000000014 time: 0.06739497184753418\n",
      "Iteration: 8380 loss: 0.0000000010 time: 0.06424498558044434\n",
      "Iteration: 8390 loss: 0.0000000015 time: 0.05813765525817871\n",
      "Iteration: 8400 loss: 0.0000000009 time: 0.0706186294555664\n",
      "Iteration: 8410 loss: 0.0000000003 time: 0.07159018516540527\n",
      "Iteration: 8420 loss: 0.0000000001 time: 0.07132601737976074\n",
      "Iteration: 8430 loss: 0.0000000000 time: 0.06891083717346191\n",
      "Iteration: 8440 loss: 0.0000000000 time: 0.06956839561462402\n",
      "Iteration: 8450 loss: 0.0000000000 time: 0.07109999656677246\n",
      "Iteration: 8460 loss: 0.0000000000 time: 0.06754589080810547\n",
      "Iteration: 8470 loss: 0.0000000000 time: 0.07019877433776855\n",
      "Iteration: 8480 loss: 0.0000000000 time: 0.0702660083770752\n",
      "Iteration: 8490 loss: 0.0000000000 time: 0.06871867179870605\n",
      "Iteration: 8500 loss: 0.0000000000 time: 0.06712746620178223\n",
      "Iteration: 8510 loss: 0.0000000000 time: 0.0698542594909668\n",
      "Iteration: 8520 loss: 0.0000000000 time: 0.06825900077819824\n",
      "Iteration: 8530 loss: 0.0000000000 time: 0.06806373596191406\n",
      "Iteration: 8540 loss: 0.0000000000 time: 0.07053804397583008\n",
      "Iteration: 8550 loss: 0.0000000000 time: 0.06014394760131836\n",
      "Iteration: 8560 loss: 0.0000000000 time: 0.05729508399963379\n",
      "Iteration: 8570 loss: 0.0000000000 time: 0.06129312515258789\n",
      "Iteration: 8580 loss: 0.0000000000 time: 0.05896592140197754\n",
      "Iteration: 8590 loss: 0.0000000000 time: 0.05826115608215332\n",
      "Iteration: 8600 loss: 0.0000000000 time: 0.05785036087036133\n",
      "Iteration: 8610 loss: 0.0000000000 time: 0.05931282043457031\n",
      "Iteration: 8620 loss: 0.0000000000 time: 0.06674504280090332\n",
      "Iteration: 8630 loss: 0.0000000000 time: 0.06121039390563965\n",
      "Iteration: 8640 loss: 0.0000000000 time: 0.059179067611694336\n",
      "Iteration: 8650 loss: 0.0000000000 time: 0.06195974349975586\n",
      "Iteration: 8660 loss: 0.0000000000 time: 0.0655817985534668\n",
      "Iteration: 8670 loss: 0.0000000000 time: 0.06992673873901367\n",
      "Iteration: 8680 loss: 0.0000000000 time: 0.0731954574584961\n",
      "Iteration: 8690 loss: 0.0000000000 time: 0.06795430183410645\n",
      "Iteration: 8700 loss: 0.0000000000 time: 0.06934499740600586\n",
      "Iteration: 8710 loss: 0.0000000000 time: 0.06791424751281738\n",
      "Iteration: 8720 loss: 0.0000000000 time: 0.07082343101501465\n",
      "Iteration: 8730 loss: 0.0000000000 time: 0.06940174102783203\n",
      "Iteration: 8740 loss: 0.0000000000 time: 0.0683281421661377\n",
      "Iteration: 8750 loss: 0.0000000000 time: 0.07014918327331543\n",
      "Iteration: 8760 loss: 0.0000000000 time: 0.0683736801147461\n",
      "Iteration: 8770 loss: 0.0000000000 time: 0.06789803504943848\n",
      "Iteration: 8780 loss: 0.0000000000 time: 0.06968545913696289\n",
      "Iteration: 8790 loss: 0.0000000000 time: 0.06839132308959961\n",
      "Iteration: 8800 loss: 0.0000000000 time: 0.06710147857666016\n",
      "Iteration: 8810 loss: 0.0000000000 time: 0.07106590270996094\n",
      "Iteration: 8820 loss: 0.0000000000 time: 0.06841087341308594\n",
      "Iteration: 8830 loss: 0.0000000000 time: 0.07390141487121582\n",
      "Iteration: 8840 loss: 0.0000000000 time: 0.0709085464477539\n",
      "Iteration: 8850 loss: 0.0000000000 time: 0.07313871383666992\n",
      "Iteration: 8860 loss: 0.0000000000 time: 0.07305097579956055\n",
      "Iteration: 8870 loss: 0.0000000000 time: 0.07421135902404785\n",
      "Iteration: 8880 loss: 0.0000000000 time: 0.07419896125793457\n",
      "Iteration: 8890 loss: 0.0000000000 time: 0.07338953018188477\n",
      "Iteration: 8900 loss: 0.0000000000 time: 0.06791019439697266\n",
      "Iteration: 8910 loss: 0.0000000000 time: 0.06698274612426758\n",
      "Iteration: 8920 loss: 0.0000000000 time: 0.06773257255554199\n",
      "Iteration: 8930 loss: 0.0000000000 time: 0.06768226623535156\n",
      "Iteration: 8940 loss: 0.0000000000 time: 0.06697344779968262\n",
      "Iteration: 8950 loss: 0.0000000000 time: 0.06692028045654297\n",
      "Iteration: 8960 loss: 0.0000000000 time: 0.06674075126647949\n",
      "Iteration: 8970 loss: 0.0000000000 time: 0.0595552921295166\n",
      "Iteration: 8980 loss: 0.0000000000 time: 0.06937980651855469\n",
      "Iteration: 8990 loss: 0.0000000000 time: 0.06855130195617676\n",
      "Iteration: 9000 loss: 0.0000000024 time: 0.06647229194641113\n",
      "Iteration: 9010 loss: 0.0000071237 time: 0.06550979614257812\n",
      "Iteration: 9020 loss: 0.0000099065 time: 0.06722617149353027\n",
      "Iteration: 9030 loss: 0.0000048248 time: 0.06839537620544434\n",
      "Iteration: 9040 loss: 0.0000001091 time: 0.06537628173828125\n",
      "Iteration: 9050 loss: 0.0000003871 time: 0.06557607650756836\n",
      "Iteration: 9060 loss: 0.0000004056 time: 0.06502127647399902\n",
      "Iteration: 9070 loss: 0.0000000985 time: 0.06932640075683594\n",
      "Iteration: 9080 loss: 0.0000000036 time: 0.06574082374572754\n",
      "Iteration: 9090 loss: 0.0000000025 time: 0.06473922729492188\n",
      "Iteration: 9100 loss: 0.0000000040 time: 0.06494665145874023\n",
      "Iteration: 9110 loss: 0.0000000021 time: 0.06808876991271973\n",
      "Iteration: 9120 loss: 0.0000000008 time: 0.06548762321472168\n",
      "Iteration: 9130 loss: 0.0000000002 time: 0.06502771377563477\n",
      "Iteration: 9140 loss: 0.0000000001 time: 0.0664219856262207\n",
      "Iteration: 9150 loss: 0.0000000000 time: 0.06818866729736328\n",
      "Iteration: 9160 loss: 0.0000000000 time: 0.05753898620605469\n",
      "Iteration: 9170 loss: 0.0000000000 time: 0.05772233009338379\n",
      "Iteration: 9180 loss: 0.0000000000 time: 0.05508112907409668\n",
      "Iteration: 9190 loss: 0.0000000000 time: 0.05943870544433594\n",
      "Iteration: 9200 loss: 0.0000000000 time: 0.06585550308227539\n",
      "Iteration: 9210 loss: 0.0000000000 time: 0.06423807144165039\n",
      "Iteration: 9220 loss: 0.0000000000 time: 0.06696701049804688\n",
      "Iteration: 9230 loss: 0.0000000000 time: 0.06873369216918945\n",
      "Iteration: 9240 loss: 0.0000000000 time: 0.06576085090637207\n",
      "Iteration: 9250 loss: 0.0000000000 time: 0.06522512435913086\n",
      "Iteration: 9260 loss: 0.0000000000 time: 0.06660628318786621\n",
      "Iteration: 9270 loss: 0.0000000000 time: 0.06748127937316895\n",
      "Iteration: 9280 loss: 0.0000000000 time: 0.06551098823547363\n",
      "Iteration: 9290 loss: 0.0000000000 time: 0.06567645072937012\n",
      "Iteration: 9300 loss: 0.0000000000 time: 0.07398843765258789\n",
      "Iteration: 9310 loss: 0.0000000000 time: 0.0749669075012207\n",
      "Iteration: 9320 loss: 0.0000000000 time: 0.05966019630432129\n",
      "Iteration: 9330 loss: 0.0000000000 time: 0.06111598014831543\n",
      "Iteration: 9340 loss: 0.0000000000 time: 0.06067466735839844\n",
      "Iteration: 9350 loss: 0.0000000000 time: 0.059751033782958984\n",
      "Iteration: 9360 loss: 0.0000000000 time: 0.05922222137451172\n",
      "Iteration: 9370 loss: 0.0000000000 time: 0.05742025375366211\n",
      "Iteration: 9380 loss: 0.0000000000 time: 0.0631406307220459\n",
      "Iteration: 9390 loss: 0.0000000000 time: 0.05663108825683594\n",
      "Iteration: 9400 loss: 0.0000000000 time: 0.06491804122924805\n",
      "Iteration: 9410 loss: 0.0000000000 time: 0.0707247257232666\n",
      "Iteration: 9420 loss: 0.0000000000 time: 0.07457113265991211\n",
      "Iteration: 9430 loss: 0.0000000000 time: 0.07246232032775879\n",
      "Iteration: 9440 loss: 0.0000000000 time: 0.07183456420898438\n",
      "Iteration: 9450 loss: 0.0000000000 time: 0.07439541816711426\n",
      "Iteration: 9460 loss: 0.0000000000 time: 0.07372426986694336\n",
      "Iteration: 9470 loss: 0.0000000000 time: 0.07250237464904785\n",
      "Iteration: 9480 loss: 0.0000000000 time: 0.07124447822570801\n",
      "Iteration: 9490 loss: 0.0000000000 time: 0.07337689399719238\n",
      "Iteration: 9500 loss: 0.0000000000 time: 0.07197308540344238\n",
      "Iteration: 9510 loss: 0.0000000000 time: 0.07175493240356445\n",
      "Iteration: 9520 loss: 0.0000000000 time: 0.07177472114562988\n",
      "Iteration: 9530 loss: 0.0000000000 time: 0.06209850311279297\n",
      "Iteration: 9540 loss: 0.0000000000 time: 0.06016969680786133\n",
      "Iteration: 9550 loss: 0.0000000000 time: 0.06460976600646973\n",
      "Iteration: 9560 loss: 0.0000000000 time: 0.06358790397644043\n",
      "Iteration: 9570 loss: 0.0000000000 time: 0.061075687408447266\n",
      "Iteration: 9580 loss: 0.0000000000 time: 0.06313276290893555\n",
      "Iteration: 9590 loss: 0.0000000000 time: 0.06430482864379883\n",
      "Iteration: 9600 loss: 0.0000000000 time: 0.0685884952545166\n",
      "Iteration: 9610 loss: 0.0000000000 time: 0.06688070297241211\n",
      "Iteration: 9620 loss: 0.0000000000 time: 0.0676872730255127\n",
      "Iteration: 9630 loss: 0.0000000000 time: 0.07180333137512207\n",
      "Iteration: 9640 loss: 0.0000000000 time: 0.0647737979888916\n",
      "Iteration: 9650 loss: 0.0000000000 time: 0.0654456615447998\n",
      "Iteration: 9660 loss: 0.0000000000 time: 0.06741499900817871\n",
      "Iteration: 9670 loss: 0.0000000000 time: 0.0648341178894043\n",
      "Iteration: 9680 loss: 0.0000000000 time: 0.07207226753234863\n",
      "Iteration: 9690 loss: 0.0000000000 time: 0.07326579093933105\n",
      "Iteration: 9700 loss: 0.0000000000 time: 0.07039737701416016\n",
      "Iteration: 9710 loss: 0.0000000010 time: 0.07388734817504883\n",
      "Iteration: 9720 loss: 0.0000024663 time: 0.07392740249633789\n",
      "Iteration: 9730 loss: 0.0000320672 time: 0.07265496253967285\n",
      "Iteration: 9740 loss: 0.0000059322 time: 0.07203817367553711\n",
      "Iteration: 9750 loss: 0.0000029187 time: 0.07257986068725586\n",
      "Iteration: 9760 loss: 0.0000011542 time: 0.07131600379943848\n",
      "Iteration: 9770 loss: 0.0000002288 time: 0.07218074798583984\n",
      "Iteration: 9780 loss: 0.0000000117 time: 0.07438278198242188\n",
      "Iteration: 9790 loss: 0.0000000026 time: 0.0719294548034668\n",
      "Iteration: 9800 loss: 0.0000000062 time: 0.07262253761291504\n",
      "Iteration: 9810 loss: 0.0000000041 time: 0.07280516624450684\n",
      "Iteration: 9820 loss: 0.0000000019 time: 0.0722801685333252\n",
      "Iteration: 9830 loss: 0.0000000007 time: 0.07161927223205566\n",
      "Iteration: 9840 loss: 0.0000000003 time: 0.07265424728393555\n",
      "Iteration: 9850 loss: 0.0000000001 time: 0.07347917556762695\n",
      "Iteration: 9860 loss: 0.0000000000 time: 0.07277607917785645\n",
      "Iteration: 9870 loss: 0.0000000000 time: 0.07535600662231445\n",
      "Iteration: 9880 loss: 0.0000000000 time: 0.07282304763793945\n",
      "Iteration: 9890 loss: 0.0000000000 time: 0.07572126388549805\n",
      "Iteration: 9900 loss: 0.0000000000 time: 0.07377052307128906\n",
      "Iteration: 9910 loss: 0.0000000000 time: 0.07301759719848633\n",
      "Iteration: 9920 loss: 0.0000000000 time: 0.0697486400604248\n",
      "Iteration: 9930 loss: 0.0000000000 time: 0.07103180885314941\n",
      "Iteration: 9940 loss: 0.0000000000 time: 0.0642549991607666\n",
      "Iteration: 9950 loss: 0.0000000000 time: 0.06354284286499023\n",
      "Iteration: 9960 loss: 0.0000000000 time: 0.06466221809387207\n",
      "Iteration: 9970 loss: 0.0000000000 time: 0.05876040458679199\n",
      "Iteration: 9980 loss: 0.0000000000 time: 0.05845952033996582\n",
      "Iteration: 9990 loss: 0.0000000000 time: 0.05905461311340332\n",
      "Iteration: 10000 loss: 0.0000000000 time: 0.060101985931396484\n",
      "Iteration: 10010 loss: 0.0000000000 time: 0.05950760841369629\n",
      "Iteration: 10020 loss: 0.0000000000 time: 0.05883598327636719\n",
      "Iteration: 10030 loss: 0.0000000000 time: 0.05895686149597168\n",
      "Iteration: 10040 loss: 0.0000000000 time: 0.06139564514160156\n",
      "Iteration: 10050 loss: 0.0000000000 time: 0.07051873207092285\n",
      "Iteration: 10060 loss: 0.0000000000 time: 0.07062339782714844\n",
      "Iteration: 10070 loss: 0.0000000000 time: 0.07211899757385254\n",
      "Iteration: 10080 loss: 0.0000000000 time: 0.07342219352722168\n",
      "Iteration: 10090 loss: 0.0000000000 time: 0.07196784019470215\n",
      "Iteration: 10100 loss: 0.0000000000 time: 0.07220101356506348\n",
      "Iteration: 10110 loss: 0.0000000000 time: 0.07337284088134766\n",
      "Iteration: 10120 loss: 0.0000000000 time: 0.06947875022888184\n",
      "Iteration: 10130 loss: 0.0000000000 time: 0.07431197166442871\n",
      "Iteration: 10140 loss: 0.0000000000 time: 0.07617521286010742\n",
      "Iteration: 10150 loss: 0.0000000000 time: 0.07172536849975586\n",
      "Iteration: 10160 loss: 0.0000000000 time: 0.07405948638916016\n",
      "Iteration: 10170 loss: 0.0000000000 time: 0.06493115425109863\n",
      "Iteration: 10180 loss: 0.0000000000 time: 0.06287717819213867\n",
      "Iteration: 10190 loss: 0.0000000000 time: 0.07187962532043457\n",
      "Iteration: 10200 loss: 0.0000000000 time: 0.06805133819580078\n",
      "Iteration: 10210 loss: 0.0000000000 time: 0.0696263313293457\n",
      "Iteration: 10220 loss: 0.0000000000 time: 0.0666499137878418\n",
      "Iteration: 10230 loss: 0.0000000000 time: 0.06662416458129883\n",
      "Iteration: 10240 loss: 0.0000000000 time: 0.06755542755126953\n",
      "Iteration: 10250 loss: 0.0000000000 time: 0.07342362403869629\n",
      "Iteration: 10260 loss: 0.0000000000 time: 0.06712484359741211\n",
      "Iteration: 10270 loss: 0.0000000000 time: 0.06076478958129883\n",
      "Iteration: 10280 loss: 0.0000000000 time: 0.05916452407836914\n",
      "Iteration: 10290 loss: 0.0000000000 time: 0.05880427360534668\n",
      "Iteration: 10300 loss: 0.0000000000 time: 0.06683039665222168\n",
      "Iteration: 10310 loss: 0.0000000000 time: 0.06421995162963867\n",
      "Iteration: 10320 loss: 0.0000000000 time: 0.06318283081054688\n",
      "Iteration: 10330 loss: 0.0000000000 time: 0.0655367374420166\n",
      "Iteration: 10340 loss: 0.0000000000 time: 0.0641636848449707\n",
      "Iteration: 10350 loss: 0.0000000000 time: 0.06597447395324707\n",
      "Iteration: 10360 loss: 0.0000000000 time: 0.06222271919250488\n",
      "Iteration: 10370 loss: 0.0000000000 time: 0.06460356712341309\n",
      "Iteration: 10380 loss: 0.0000000000 time: 0.06484675407409668\n",
      "Iteration: 10390 loss: 0.0000000000 time: 0.06367969512939453\n",
      "Iteration: 10400 loss: 0.0000000158 time: 0.06435942649841309\n",
      "Iteration: 10410 loss: 0.0000321455 time: 0.06465625762939453\n",
      "Iteration: 10420 loss: 0.0000179697 time: 0.07155728340148926\n",
      "Iteration: 10430 loss: 0.0000054228 time: 0.07191348075866699\n",
      "Iteration: 10440 loss: 0.0000021247 time: 0.07336783409118652\n",
      "Iteration: 10450 loss: 0.0000005295 time: 0.07335829734802246\n",
      "Iteration: 10460 loss: 0.0000000467 time: 0.07400798797607422\n",
      "Iteration: 10470 loss: 0.0000000017 time: 0.07306551933288574\n",
      "Iteration: 10480 loss: 0.0000000088 time: 0.0739889144897461\n",
      "Iteration: 10490 loss: 0.0000000070 time: 0.0739288330078125\n",
      "Iteration: 10500 loss: 0.0000000034 time: 0.061630964279174805\n",
      "Iteration: 10510 loss: 0.0000000013 time: 0.06303048133850098\n",
      "Iteration: 10520 loss: 0.0000000005 time: 0.06295609474182129\n",
      "Iteration: 10530 loss: 0.0000000002 time: 0.06085968017578125\n",
      "Iteration: 10540 loss: 0.0000000001 time: 0.061337947845458984\n",
      "Iteration: 10550 loss: 0.0000000000 time: 0.0613405704498291\n",
      "Iteration: 10560 loss: 0.0000000000 time: 0.06764054298400879\n",
      "Iteration: 10570 loss: 0.0000000000 time: 0.07271194458007812\n",
      "Iteration: 10580 loss: 0.0000000000 time: 0.06133723258972168\n",
      "Iteration: 10590 loss: 0.0000000000 time: 0.06204032897949219\n",
      "Iteration: 10600 loss: 0.0000000000 time: 0.06391334533691406\n",
      "Iteration: 10610 loss: 0.0000000000 time: 0.06157279014587402\n",
      "Iteration: 10620 loss: 0.0000000000 time: 0.06116056442260742\n",
      "Iteration: 10630 loss: 0.0000000000 time: 0.06046319007873535\n",
      "Iteration: 10640 loss: 0.0000000000 time: 0.06734013557434082\n",
      "Iteration: 10650 loss: 0.0000000000 time: 0.06684637069702148\n",
      "Iteration: 10660 loss: 0.0000000000 time: 0.06602859497070312\n",
      "Iteration: 10670 loss: 0.0000000000 time: 0.06719803810119629\n",
      "Iteration: 10680 loss: 0.0000000000 time: 0.06821513175964355\n",
      "Iteration: 10690 loss: 0.0000000000 time: 0.06567764282226562\n",
      "Iteration: 10700 loss: 0.0000000000 time: 0.0660257339477539\n",
      "Iteration: 10710 loss: 0.0000000000 time: 0.06562614440917969\n",
      "Iteration: 10720 loss: 0.0000000000 time: 0.06716728210449219\n",
      "Iteration: 10730 loss: 0.0000000000 time: 0.06684088706970215\n",
      "Iteration: 10740 loss: 0.0000000000 time: 0.06714725494384766\n",
      "Iteration: 10750 loss: 0.0000000000 time: 0.06772017478942871\n",
      "Iteration: 10760 loss: 0.0000000000 time: 0.06786656379699707\n",
      "Iteration: 10770 loss: 0.0000000000 time: 0.0651240348815918\n",
      "Iteration: 10780 loss: 0.0000000000 time: 0.065155029296875\n",
      "Iteration: 10790 loss: 0.0000000000 time: 0.06877756118774414\n",
      "Iteration: 10800 loss: 0.0000000000 time: 0.06602716445922852\n",
      "Iteration: 10810 loss: 0.0000000000 time: 0.061601877212524414\n",
      "Iteration: 10820 loss: 0.0000000000 time: 0.06275439262390137\n",
      "Iteration: 10830 loss: 0.0000000000 time: 0.05934453010559082\n",
      "Iteration: 10840 loss: 0.0000000000 time: 0.062254905700683594\n",
      "Iteration: 10850 loss: 0.0000000000 time: 0.06794548034667969\n",
      "Iteration: 10860 loss: 0.0000000000 time: 0.06715965270996094\n",
      "Iteration: 10870 loss: 0.0000000000 time: 0.06935620307922363\n",
      "Iteration: 10880 loss: 0.0000000000 time: 0.07144856452941895\n",
      "Iteration: 10890 loss: 0.0000000000 time: 0.06622600555419922\n",
      "Iteration: 10900 loss: 0.0000000000 time: 0.06785774230957031\n",
      "Iteration: 10910 loss: 0.0000000000 time: 0.07090377807617188\n",
      "Iteration: 10920 loss: 0.0000000000 time: 0.06834125518798828\n",
      "Iteration: 10930 loss: 0.0000000000 time: 0.07000446319580078\n",
      "Iteration: 10940 loss: 0.0000000000 time: 0.07159137725830078\n",
      "Iteration: 10950 loss: 0.0000000000 time: 0.0696263313293457\n",
      "Iteration: 10960 loss: 0.0000000000 time: 0.06906604766845703\n",
      "Iteration: 10970 loss: 0.0000000000 time: 0.07359433174133301\n",
      "Iteration: 10980 loss: 0.0000000000 time: 0.06818675994873047\n",
      "Iteration: 10990 loss: 0.0000000000 time: 0.0686330795288086\n",
      "Iteration: 11000 loss: 0.0000000000 time: 0.07152366638183594\n",
      "Iteration: 11010 loss: 0.0000000000 time: 0.06862115859985352\n",
      "Iteration: 11020 loss: 0.0000000000 time: 0.0725867748260498\n",
      "Iteration: 11030 loss: 0.0000000000 time: 0.06746125221252441\n",
      "Iteration: 11040 loss: 0.0000000000 time: 0.0671849250793457\n",
      "Iteration: 11050 loss: 0.0000000000 time: 0.06703448295593262\n",
      "Iteration: 11060 loss: 0.0000000000 time: 0.06846761703491211\n",
      "Iteration: 11070 loss: 0.0000000000 time: 0.06560754776000977\n",
      "Iteration: 11080 loss: 0.0000000000 time: 0.06741738319396973\n",
      "Iteration: 11090 loss: 0.0000000030 time: 0.0711054801940918\n",
      "Iteration: 11100 loss: 0.0000096946 time: 0.06657052040100098\n",
      "Iteration: 11110 loss: 0.0000001716 time: 0.06560277938842773\n",
      "Iteration: 11120 loss: 0.0000000563 time: 0.06986689567565918\n",
      "Iteration: 11130 loss: 0.0000012075 time: 0.06003069877624512\n",
      "Iteration: 11140 loss: 0.0000008854 time: 0.057631492614746094\n",
      "Iteration: 11150 loss: 0.0000000931 time: 0.06767606735229492\n",
      "Iteration: 11160 loss: 0.0000000123 time: 0.06716704368591309\n",
      "Iteration: 11170 loss: 0.0000000327 time: 0.06491780281066895\n",
      "Iteration: 11180 loss: 0.0000000122 time: 0.07215547561645508\n",
      "Iteration: 11190 loss: 0.0000000014 time: 0.0744178295135498\n",
      "Iteration: 11200 loss: 0.0000000000 time: 0.07305526733398438\n",
      "Iteration: 11210 loss: 0.0000000001 time: 0.0737314224243164\n",
      "Iteration: 11220 loss: 0.0000000001 time: 0.07194638252258301\n",
      "Iteration: 11230 loss: 0.0000000001 time: 0.0665137767791748\n",
      "Iteration: 11240 loss: 0.0000000000 time: 0.0706319808959961\n",
      "Iteration: 11250 loss: 0.0000000000 time: 0.06424212455749512\n",
      "Iteration: 11260 loss: 0.0000000000 time: 0.05896306037902832\n",
      "Iteration: 11270 loss: 0.0000000000 time: 0.06155681610107422\n",
      "Iteration: 11280 loss: 0.0000000000 time: 0.07457995414733887\n",
      "Iteration: 11290 loss: 0.0000000000 time: 0.07347464561462402\n",
      "Iteration: 11300 loss: 0.0000000000 time: 0.07242012023925781\n",
      "Iteration: 11310 loss: 0.0000000000 time: 0.07267117500305176\n",
      "Iteration: 11320 loss: 0.0000000000 time: 0.07280993461608887\n",
      "Iteration: 11330 loss: 0.0000000000 time: 0.06772613525390625\n",
      "Iteration: 11340 loss: 0.0000000000 time: 0.06152176856994629\n",
      "Iteration: 11350 loss: 0.0000000000 time: 0.06424808502197266\n",
      "Iteration: 11360 loss: 0.0000000000 time: 0.061873674392700195\n",
      "Iteration: 11370 loss: 0.0000000000 time: 0.06070685386657715\n",
      "Iteration: 11380 loss: 0.0000000000 time: 0.06278371810913086\n",
      "Iteration: 11390 loss: 0.0000000000 time: 0.06100964546203613\n",
      "Iteration: 11400 loss: 0.0000000000 time: 0.06754469871520996\n",
      "Iteration: 11410 loss: 0.0000000000 time: 0.05865812301635742\n",
      "Iteration: 11420 loss: 0.0000000000 time: 0.06123709678649902\n",
      "Iteration: 11430 loss: 0.0000000000 time: 0.05987238883972168\n",
      "Iteration: 11440 loss: 0.0000000000 time: 0.05901503562927246\n",
      "Iteration: 11450 loss: 0.0000000000 time: 0.058396339416503906\n",
      "Iteration: 11460 loss: 0.0000000000 time: 0.06646227836608887\n",
      "Iteration: 11470 loss: 0.0000000000 time: 0.06494522094726562\n",
      "Iteration: 11480 loss: 0.0000000000 time: 0.06490492820739746\n",
      "Iteration: 11490 loss: 0.0000000000 time: 0.06694436073303223\n",
      "Iteration: 11500 loss: 0.0000000000 time: 0.06722474098205566\n",
      "Iteration: 11510 loss: 0.0000000000 time: 0.06711220741271973\n",
      "Iteration: 11520 loss: 0.0000000000 time: 0.06902909278869629\n",
      "Iteration: 11530 loss: 0.0000000000 time: 0.06962776184082031\n",
      "Iteration: 11540 loss: 0.0000000000 time: 0.06662750244140625\n",
      "Iteration: 11550 loss: 0.0000000000 time: 0.06256580352783203\n",
      "Iteration: 11560 loss: 0.0000000000 time: 0.07398152351379395\n",
      "Iteration: 11570 loss: 0.0000000000 time: 0.07483625411987305\n",
      "Iteration: 11580 loss: 0.0000000000 time: 0.07199931144714355\n",
      "Iteration: 11590 loss: 0.0000000000 time: 0.07247400283813477\n",
      "Iteration: 11600 loss: 0.0000000000 time: 0.07287764549255371\n",
      "Iteration: 11610 loss: 0.0000000000 time: 0.07039856910705566\n",
      "Iteration: 11620 loss: 0.0000000000 time: 0.07381677627563477\n",
      "Iteration: 11630 loss: 0.0000000000 time: 0.07604193687438965\n",
      "Iteration: 11640 loss: 0.0000000000 time: 0.07067680358886719\n",
      "Iteration: 11650 loss: 0.0000000000 time: 0.07247281074523926\n",
      "Iteration: 11660 loss: 0.0000000000 time: 0.07410359382629395\n",
      "Iteration: 11670 loss: 0.0000000000 time: 0.07184815406799316\n",
      "Iteration: 11680 loss: 0.0000000000 time: 0.07300066947937012\n",
      "Iteration: 11690 loss: 0.0000000000 time: 0.07381844520568848\n",
      "Iteration: 11700 loss: 0.0000000000 time: 0.07001399993896484\n",
      "Iteration: 11710 loss: 0.0000000000 time: 0.07142066955566406\n",
      "Iteration: 11720 loss: 0.0000000000 time: 0.06894993782043457\n",
      "Iteration: 11730 loss: 0.0000000000 time: 0.06818842887878418\n",
      "Iteration: 11740 loss: 0.0000000000 time: 0.06805253028869629\n",
      "Iteration: 11750 loss: 0.0000000000 time: 0.07026529312133789\n",
      "Iteration: 11760 loss: 0.0000000000 time: 0.06844949722290039\n",
      "Iteration: 11770 loss: 0.0000000000 time: 0.06548523902893066\n",
      "Iteration: 11780 loss: 0.0000000000 time: 0.060892581939697266\n",
      "Iteration: 11790 loss: 0.0000000000 time: 0.05804324150085449\n",
      "Iteration: 11800 loss: 0.0000000000 time: 0.06829333305358887\n",
      "Iteration: 11810 loss: 0.0000000000 time: 0.07039189338684082\n",
      "Iteration: 11820 loss: 0.0000000081 time: 0.07108163833618164\n",
      "Iteration: 11830 loss: 0.0000198278 time: 0.06826591491699219\n",
      "Iteration: 11840 loss: 0.0000112536 time: 0.060068368911743164\n",
      "Iteration: 11850 loss: 0.0000029786 time: 0.06693148612976074\n",
      "Iteration: 11860 loss: 0.0000017340 time: 0.06619882583618164\n",
      "Iteration: 11870 loss: 0.0000005944 time: 0.06270265579223633\n",
      "Iteration: 11880 loss: 0.0000000717 time: 0.06394529342651367\n",
      "Iteration: 11890 loss: 0.0000000006 time: 0.06582045555114746\n",
      "Iteration: 11900 loss: 0.0000000074 time: 0.06406855583190918\n",
      "Iteration: 11910 loss: 0.0000000069 time: 0.06253886222839355\n",
      "Iteration: 11920 loss: 0.0000000033 time: 0.06447935104370117\n",
      "Iteration: 11930 loss: 0.0000000013 time: 0.06581759452819824\n",
      "Iteration: 11940 loss: 0.0000000004 time: 0.06365394592285156\n",
      "Iteration: 11950 loss: 0.0000000001 time: 0.06313896179199219\n",
      "Iteration: 11960 loss: 0.0000000001 time: 0.06357288360595703\n",
      "Iteration: 11970 loss: 0.0000000000 time: 0.06573796272277832\n",
      "Iteration: 11980 loss: 0.0000000000 time: 0.06513762474060059\n",
      "Iteration: 11990 loss: 0.0000000000 time: 0.06214499473571777\n",
      "Iteration: 12000 loss: 0.0000000000 time: 0.06404805183410645\n",
      "Iteration: 12010 loss: 0.0000000000 time: 0.06058454513549805\n",
      "Iteration: 12020 loss: 0.0000000000 time: 0.06428384780883789\n",
      "Iteration: 12030 loss: 0.0000000000 time: 0.06860136985778809\n",
      "Iteration: 12040 loss: 0.0000000000 time: 0.06815457344055176\n",
      "Iteration: 12050 loss: 0.0000000000 time: 0.07103085517883301\n",
      "Iteration: 12060 loss: 0.0000000000 time: 0.06359338760375977\n",
      "Iteration: 12070 loss: 0.0000000000 time: 0.06569337844848633\n",
      "Iteration: 12080 loss: 0.0000000000 time: 0.06816935539245605\n",
      "Iteration: 12090 loss: 0.0000000000 time: 0.06493949890136719\n",
      "Iteration: 12100 loss: 0.0000000000 time: 0.0627596378326416\n",
      "Iteration: 12110 loss: 0.0000000000 time: 0.06262898445129395\n",
      "Iteration: 12120 loss: 0.0000000000 time: 0.06412315368652344\n",
      "Iteration: 12130 loss: 0.0000000000 time: 0.06447839736938477\n",
      "Iteration: 12140 loss: 0.0000000000 time: 0.0652928352355957\n",
      "Iteration: 12150 loss: 0.0000000000 time: 0.06923031806945801\n",
      "Iteration: 12160 loss: 0.0000000000 time: 0.07307696342468262\n",
      "Iteration: 12170 loss: 0.0000000000 time: 0.07111430168151855\n",
      "Iteration: 12180 loss: 0.0000000000 time: 0.0611567497253418\n",
      "Iteration: 12190 loss: 0.0000000000 time: 0.06673049926757812\n",
      "Iteration: 12200 loss: 0.0000000000 time: 0.06975007057189941\n",
      "Iteration: 12210 loss: 0.0000000000 time: 0.06774592399597168\n",
      "Iteration: 12220 loss: 0.0000000000 time: 0.06761527061462402\n",
      "Iteration: 12230 loss: 0.0000000000 time: 0.0740664005279541\n",
      "Iteration: 12240 loss: 0.0000000000 time: 0.07335758209228516\n",
      "Iteration: 12250 loss: 0.0000000000 time: 0.06075549125671387\n",
      "Iteration: 12260 loss: 0.0000000000 time: 0.05743575096130371\n",
      "Iteration: 12270 loss: 0.0000000000 time: 0.05922675132751465\n",
      "Iteration: 12280 loss: 0.0000000000 time: 0.0647878646850586\n",
      "Iteration: 12290 loss: 0.0000000000 time: 0.07059216499328613\n",
      "Iteration: 12300 loss: 0.0000000000 time: 0.07482433319091797\n",
      "Iteration: 12310 loss: 0.0000000000 time: 0.07516908645629883\n",
      "Iteration: 12320 loss: 0.0000000000 time: 0.07420945167541504\n",
      "Iteration: 12330 loss: 0.0000000000 time: 0.07294511795043945\n",
      "Iteration: 12340 loss: 0.0000000000 time: 0.07349658012390137\n",
      "Iteration: 12350 loss: 0.0000000000 time: 0.0722191333770752\n",
      "Iteration: 12360 loss: 0.0000000000 time: 0.07207489013671875\n",
      "Iteration: 12370 loss: 0.0000000000 time: 0.07636141777038574\n",
      "Iteration: 12380 loss: 0.0000000000 time: 0.06271028518676758\n",
      "Iteration: 12390 loss: 0.0000000000 time: 0.06084275245666504\n",
      "Iteration: 12400 loss: 0.0000000000 time: 0.06492328643798828\n",
      "Iteration: 12410 loss: 0.0000000000 time: 0.07203149795532227\n",
      "Iteration: 12420 loss: 0.0000000000 time: 0.06920194625854492\n",
      "Iteration: 12430 loss: 0.0000000000 time: 0.07086300849914551\n",
      "Iteration: 12440 loss: 0.0000000000 time: 0.0707392692565918\n",
      "Iteration: 12450 loss: 0.0000000000 time: 0.06678462028503418\n",
      "Iteration: 12460 loss: 0.0000000000 time: 0.06454110145568848\n",
      "Iteration: 12470 loss: 0.0000000000 time: 0.0698857307434082\n",
      "Iteration: 12480 loss: 0.0000000000 time: 0.07046747207641602\n",
      "Iteration: 12490 loss: 0.0000000000 time: 0.07219648361206055\n",
      "Iteration: 12500 loss: 0.0000000000 time: 0.07477617263793945\n",
      "Iteration: 12510 loss: 0.0000000000 time: 0.07142329216003418\n",
      "Iteration: 12520 loss: 0.0000000047 time: 0.07415366172790527\n",
      "Iteration: 12530 loss: 0.0000143940 time: 0.07463693618774414\n",
      "Iteration: 12540 loss: 0.0000074593 time: 0.07145142555236816\n",
      "Iteration: 12550 loss: 0.0000024959 time: 0.07320475578308105\n",
      "Iteration: 12560 loss: 0.0000018181 time: 0.07493948936462402\n",
      "Iteration: 12570 loss: 0.0000004692 time: 0.07186341285705566\n",
      "Iteration: 12580 loss: 0.0000000066 time: 0.07312965393066406\n",
      "Iteration: 12590 loss: 0.0000000266 time: 0.07584547996520996\n",
      "Iteration: 12600 loss: 0.0000000262 time: 0.07190394401550293\n",
      "Iteration: 12610 loss: 0.0000000084 time: 0.0730743408203125\n",
      "Iteration: 12620 loss: 0.0000000014 time: 0.0648047924041748\n",
      "Iteration: 12630 loss: 0.0000000001 time: 0.06739449501037598\n",
      "Iteration: 12640 loss: 0.0000000000 time: 0.0685584545135498\n",
      "Iteration: 12650 loss: 0.0000000000 time: 0.07241582870483398\n",
      "Iteration: 12660 loss: 0.0000000000 time: 0.07409501075744629\n",
      "Iteration: 12670 loss: 0.0000000000 time: 0.07040047645568848\n",
      "Iteration: 12680 loss: 0.0000000000 time: 0.07282233238220215\n",
      "Iteration: 12690 loss: 0.0000000000 time: 0.07415652275085449\n",
      "Iteration: 12700 loss: 0.0000000000 time: 0.07164335250854492\n",
      "Iteration: 12710 loss: 0.0000000000 time: 0.07140088081359863\n",
      "Iteration: 12720 loss: 0.0000000000 time: 0.07346343994140625\n",
      "Iteration: 12730 loss: 0.0000000000 time: 0.07050323486328125\n",
      "Iteration: 12740 loss: 0.0000000000 time: 0.07293200492858887\n",
      "Iteration: 12750 loss: 0.0000000000 time: 0.07331681251525879\n",
      "Iteration: 12760 loss: 0.0000000000 time: 0.0721592903137207\n",
      "Iteration: 12770 loss: 0.0000000000 time: 0.07059597969055176\n",
      "Iteration: 12780 loss: 0.0000000000 time: 0.07168912887573242\n",
      "Iteration: 12790 loss: 0.0000000000 time: 0.06952762603759766\n",
      "Iteration: 12800 loss: 0.0000000000 time: 0.06849145889282227\n",
      "Iteration: 12810 loss: 0.0000000000 time: 0.06938910484313965\n",
      "Iteration: 12820 loss: 0.0000000000 time: 0.0593419075012207\n",
      "Iteration: 12830 loss: 0.0000000000 time: 0.0665578842163086\n",
      "Iteration: 12840 loss: 0.0000000000 time: 0.07009577751159668\n",
      "Iteration: 12850 loss: 0.0000000000 time: 0.06313753128051758\n",
      "Iteration: 12860 loss: 0.0000000000 time: 0.06086874008178711\n",
      "Iteration: 12870 loss: 0.0000000000 time: 0.06194806098937988\n",
      "Iteration: 12880 loss: 0.0000000000 time: 0.0701284408569336\n",
      "Iteration: 12890 loss: 0.0000000000 time: 0.06915521621704102\n",
      "Iteration: 12900 loss: 0.0000000000 time: 0.0684351921081543\n",
      "Iteration: 12910 loss: 0.0000000000 time: 0.07059693336486816\n",
      "Iteration: 12920 loss: 0.0000000000 time: 0.07108855247497559\n",
      "Iteration: 12930 loss: 0.0000000000 time: 0.06793498992919922\n",
      "Iteration: 12940 loss: 0.0000000000 time: 0.0693659782409668\n",
      "Iteration: 12950 loss: 0.0000000000 time: 0.07226967811584473\n",
      "Iteration: 12960 loss: 0.0000000000 time: 0.0645592212677002\n",
      "Iteration: 12970 loss: 0.0000000000 time: 0.06282401084899902\n",
      "Iteration: 12980 loss: 0.0000000000 time: 0.06456375122070312\n",
      "Iteration: 12990 loss: 0.0000000000 time: 0.06406688690185547\n",
      "Iteration: 13000 loss: 0.0000000000 time: 0.06364154815673828\n",
      "Iteration: 13010 loss: 0.0000000000 time: 0.06383872032165527\n",
      "Iteration: 13020 loss: 0.0000000000 time: 0.0645151138305664\n",
      "Iteration: 13030 loss: 0.0000000000 time: 0.06976962089538574\n",
      "Iteration: 13040 loss: 0.0000000000 time: 0.07281851768493652\n",
      "Iteration: 13050 loss: 0.0000000000 time: 0.07198548316955566\n",
      "Iteration: 13060 loss: 0.0000000000 time: 0.0739145278930664\n",
      "Iteration: 13070 loss: 0.0000000000 time: 0.07177090644836426\n",
      "Iteration: 13080 loss: 0.0000000000 time: 0.07160377502441406\n",
      "Iteration: 13090 loss: 0.0000000000 time: 0.0668952465057373\n",
      "Iteration: 13100 loss: 0.0000000000 time: 0.06917333602905273\n",
      "Iteration: 13110 loss: 0.0000000000 time: 0.0637664794921875\n",
      "Iteration: 13120 loss: 0.0000000000 time: 0.06459283828735352\n",
      "Iteration: 13130 loss: 0.0000000000 time: 0.06372237205505371\n",
      "Iteration: 13140 loss: 0.0000000000 time: 0.06429481506347656\n",
      "Iteration: 13150 loss: 0.0000000000 time: 0.06335997581481934\n",
      "Iteration: 13160 loss: 0.0000000000 time: 0.06507611274719238\n",
      "Iteration: 13170 loss: 0.0000000000 time: 0.06598210334777832\n",
      "Iteration: 13180 loss: 0.0000000000 time: 0.07254242897033691\n",
      "Iteration: 13190 loss: 0.0000000000 time: 0.07429075241088867\n",
      "Iteration: 13200 loss: 0.0000000000 time: 0.07319068908691406\n",
      "Iteration: 13210 loss: 0.0000000000 time: 0.07180333137512207\n",
      "Iteration: 13220 loss: 0.0000000000 time: 0.07187604904174805\n",
      "Iteration: 13230 loss: 0.0000000000 time: 0.0733487606048584\n",
      "Iteration: 13240 loss: 0.0000000364 time: 0.07288312911987305\n",
      "Iteration: 13250 loss: 0.0000442232 time: 0.07256221771240234\n",
      "Iteration: 13260 loss: 0.0000018379 time: 0.07313370704650879\n",
      "Iteration: 13270 loss: 0.0000003762 time: 0.07137227058410645\n",
      "Iteration: 13280 loss: 0.0000000500 time: 0.07402324676513672\n",
      "Iteration: 13290 loss: 0.0000002135 time: 0.06440305709838867\n",
      "Iteration: 13300 loss: 0.0000001374 time: 0.061461448669433594\n",
      "Iteration: 13310 loss: 0.0000000504 time: 0.061368465423583984\n",
      "Iteration: 13320 loss: 0.0000000185 time: 0.06940221786499023\n",
      "Iteration: 13330 loss: 0.0000000066 time: 0.07018709182739258\n",
      "Iteration: 13340 loss: 0.0000000019 time: 0.06911754608154297\n",
      "Iteration: 13350 loss: 0.0000000005 time: 0.06931018829345703\n",
      "Iteration: 13360 loss: 0.0000000002 time: 0.07042384147644043\n",
      "Iteration: 13370 loss: 0.0000000001 time: 0.06865072250366211\n",
      "Iteration: 13380 loss: 0.0000000000 time: 0.0701456069946289\n",
      "Iteration: 13390 loss: 0.0000000000 time: 0.06991958618164062\n",
      "Iteration: 13400 loss: 0.0000000000 time: 0.06913232803344727\n",
      "Iteration: 13410 loss: 0.0000000000 time: 0.06777095794677734\n",
      "Iteration: 13420 loss: 0.0000000000 time: 0.07033801078796387\n",
      "Iteration: 13430 loss: 0.0000000000 time: 0.06945681571960449\n",
      "Iteration: 13440 loss: 0.0000000000 time: 0.0694420337677002\n",
      "Iteration: 13450 loss: 0.0000000000 time: 0.0693821907043457\n",
      "Iteration: 13460 loss: 0.0000000000 time: 0.07028937339782715\n",
      "Iteration: 13470 loss: 0.0000000000 time: 0.05774116516113281\n",
      "Iteration: 13480 loss: 0.0000000000 time: 0.06281161308288574\n",
      "Iteration: 13490 loss: 0.0000000000 time: 0.06382870674133301\n",
      "Iteration: 13500 loss: 0.0000000000 time: 0.06067347526550293\n",
      "Iteration: 13510 loss: 0.0000000000 time: 0.0715787410736084\n",
      "Iteration: 13520 loss: 0.0000000000 time: 0.07387995719909668\n",
      "Iteration: 13530 loss: 0.0000000000 time: 0.07410216331481934\n",
      "Iteration: 13540 loss: 0.0000000000 time: 0.06833243370056152\n",
      "Iteration: 13550 loss: 0.0000000000 time: 0.06678009033203125\n",
      "Iteration: 13560 loss: 0.0000000000 time: 0.06899404525756836\n",
      "Iteration: 13570 loss: 0.0000000000 time: 0.06640744209289551\n",
      "Iteration: 13580 loss: 0.0000000000 time: 0.06622934341430664\n",
      "Iteration: 13590 loss: 0.0000000000 time: 0.06950712203979492\n",
      "Iteration: 13600 loss: 0.0000000000 time: 0.06168174743652344\n",
      "Iteration: 13610 loss: 0.0000000000 time: 0.061833858489990234\n",
      "Iteration: 13620 loss: 0.0000000000 time: 0.07423686981201172\n",
      "Iteration: 13630 loss: 0.0000000000 time: 0.07027292251586914\n",
      "Iteration: 13640 loss: 0.0000000000 time: 0.0686802864074707\n",
      "Iteration: 13650 loss: 0.0000000000 time: 0.07079434394836426\n",
      "Iteration: 13660 loss: 0.0000000000 time: 0.07194089889526367\n",
      "Iteration: 13670 loss: 0.0000000000 time: 0.06793928146362305\n",
      "Iteration: 13680 loss: 0.0000000000 time: 0.06973004341125488\n",
      "Iteration: 13690 loss: 0.0000000000 time: 0.07252168655395508\n",
      "Iteration: 13700 loss: 0.0000000000 time: 0.07028627395629883\n",
      "Iteration: 13710 loss: 0.0000000000 time: 0.06722640991210938\n",
      "Iteration: 13720 loss: 0.0000000000 time: 0.0625925064086914\n",
      "Iteration: 13730 loss: 0.0000000000 time: 0.06624913215637207\n",
      "Iteration: 13740 loss: 0.0000000000 time: 0.06723213195800781\n",
      "Iteration: 13750 loss: 0.0000000000 time: 0.06747841835021973\n",
      "Iteration: 13760 loss: 0.0000000000 time: 0.06762909889221191\n",
      "Iteration: 13770 loss: 0.0000000000 time: 0.06536293029785156\n",
      "Iteration: 13780 loss: 0.0000000000 time: 0.06572890281677246\n",
      "Iteration: 13790 loss: 0.0000000000 time: 0.06660866737365723\n",
      "Iteration: 13800 loss: 0.0000000000 time: 0.06642007827758789\n",
      "Iteration: 13810 loss: 0.0000000000 time: 0.06490278244018555\n",
      "Iteration: 13820 loss: 0.0000000000 time: 0.06716489791870117\n",
      "Iteration: 13830 loss: 0.0000000000 time: 0.06639599800109863\n",
      "Iteration: 13840 loss: 0.0000000000 time: 0.0704946517944336\n",
      "Iteration: 13850 loss: 0.0000000000 time: 0.06790709495544434\n",
      "Iteration: 13860 loss: 0.0000000000 time: 0.0680239200592041\n",
      "Iteration: 13870 loss: 0.0000000000 time: 0.06972455978393555\n",
      "Iteration: 13880 loss: 0.0000000000 time: 0.0663604736328125\n",
      "Iteration: 13890 loss: 0.0000000000 time: 0.06810164451599121\n",
      "Iteration: 13900 loss: 0.0000000000 time: 0.06846237182617188\n",
      "Iteration: 13910 loss: 0.0000000000 time: 0.06906485557556152\n",
      "Iteration: 13920 loss: 0.0000000007 time: 0.056571245193481445\n",
      "Iteration: 13930 loss: 0.0000024298 time: 0.0605161190032959\n",
      "Iteration: 13940 loss: 0.0000161855 time: 0.05973362922668457\n",
      "Iteration: 13950 loss: 0.0000062980 time: 0.06154203414916992\n",
      "Iteration: 13960 loss: 0.0000010463 time: 0.06183457374572754\n",
      "Iteration: 13970 loss: 0.0000000296 time: 0.06358861923217773\n",
      "Iteration: 13980 loss: 0.0000001739 time: 0.06039834022521973\n",
      "Iteration: 13990 loss: 0.0000000966 time: 0.06339073181152344\n",
      "Iteration: 14000 loss: 0.0000000126 time: 0.07282090187072754\n",
      "Iteration: 14010 loss: 0.0000000004 time: 0.07336878776550293\n",
      "Iteration: 14020 loss: 0.0000000014 time: 0.0733027458190918\n",
      "Iteration: 14030 loss: 0.0000000011 time: 0.07078957557678223\n",
      "Iteration: 14040 loss: 0.0000000005 time: 0.07494449615478516\n",
      "Iteration: 14050 loss: 0.0000000002 time: 0.07405829429626465\n",
      "Iteration: 14060 loss: 0.0000000001 time: 0.0724484920501709\n",
      "Iteration: 14070 loss: 0.0000000000 time: 0.0730898380279541\n",
      "Iteration: 14080 loss: 0.0000000000 time: 0.07314324378967285\n",
      "Iteration: 14090 loss: 0.0000000000 time: 0.0719001293182373\n",
      "Iteration: 14100 loss: 0.0000000000 time: 0.07358407974243164\n",
      "Iteration: 14110 loss: 0.0000000000 time: 0.07366681098937988\n",
      "Iteration: 14120 loss: 0.0000000000 time: 0.0722346305847168\n",
      "Iteration: 14130 loss: 0.0000000000 time: 0.07255744934082031\n",
      "Iteration: 14140 loss: 0.0000000000 time: 0.07297348976135254\n",
      "Iteration: 14150 loss: 0.0000000000 time: 0.07243037223815918\n",
      "Iteration: 14160 loss: 0.0000000000 time: 0.072265625\n",
      "Iteration: 14170 loss: 0.0000000000 time: 0.07208681106567383\n",
      "Iteration: 14180 loss: 0.0000000000 time: 0.07091474533081055\n",
      "Iteration: 14190 loss: 0.0000000000 time: 0.07427096366882324\n",
      "Iteration: 14200 loss: 0.0000000000 time: 0.07006669044494629\n",
      "Iteration: 14210 loss: 0.0000000000 time: 0.06295967102050781\n",
      "Iteration: 14220 loss: 0.0000000000 time: 0.06501054763793945\n",
      "Iteration: 14230 loss: 0.0000000000 time: 0.06302094459533691\n",
      "Iteration: 14240 loss: 0.0000000000 time: 0.062009572982788086\n",
      "Iteration: 14250 loss: 0.0000000000 time: 0.06055164337158203\n",
      "Iteration: 14260 loss: 0.0000000000 time: 0.06255412101745605\n",
      "Iteration: 14270 loss: 0.0000000000 time: 0.06403660774230957\n",
      "Iteration: 14280 loss: 0.0000000000 time: 0.06923341751098633\n",
      "Iteration: 14290 loss: 0.0000000000 time: 0.07126688957214355\n",
      "Iteration: 14300 loss: 0.0000000000 time: 0.07171797752380371\n",
      "Iteration: 14310 loss: 0.0000000000 time: 0.07406353950500488\n",
      "Iteration: 14320 loss: 0.0000000000 time: 0.06582164764404297\n",
      "Iteration: 14330 loss: 0.0000000000 time: 0.06601119041442871\n",
      "Iteration: 14340 loss: 0.0000000000 time: 0.06402349472045898\n",
      "Iteration: 14350 loss: 0.0000000000 time: 0.05857539176940918\n",
      "Iteration: 14360 loss: 0.0000000000 time: 0.07331347465515137\n",
      "Iteration: 14370 loss: 0.0000000000 time: 0.07313418388366699\n",
      "Iteration: 14380 loss: 0.0000000000 time: 0.07196831703186035\n",
      "Iteration: 14390 loss: 0.0000000000 time: 0.07116556167602539\n",
      "Iteration: 14400 loss: 0.0000000000 time: 0.07118535041809082\n",
      "Iteration: 14410 loss: 0.0000000000 time: 0.07445836067199707\n",
      "Iteration: 14420 loss: 0.0000000000 time: 0.0716552734375\n",
      "Iteration: 14430 loss: 0.0000000000 time: 0.07093381881713867\n",
      "Iteration: 14440 loss: 0.0000000000 time: 0.07304215431213379\n",
      "Iteration: 14450 loss: 0.0000000000 time: 0.07117056846618652\n",
      "Iteration: 14460 loss: 0.0000000000 time: 0.07238507270812988\n",
      "Iteration: 14470 loss: 0.0000000000 time: 0.07317447662353516\n",
      "Iteration: 14480 loss: 0.0000000000 time: 0.07066488265991211\n",
      "Iteration: 14490 loss: 0.0000000000 time: 0.07260537147521973\n",
      "Iteration: 14500 loss: 0.0000000000 time: 0.0723884105682373\n",
      "Iteration: 14510 loss: 0.0000000000 time: 0.05903935432434082\n",
      "Iteration: 14520 loss: 0.0000000000 time: 0.05714702606201172\n",
      "Iteration: 14530 loss: 0.0000000000 time: 0.05850934982299805\n",
      "Iteration: 14540 loss: 0.0000000000 time: 0.05697011947631836\n",
      "Iteration: 14550 loss: 0.0000000000 time: 0.058074235916137695\n",
      "Iteration: 14560 loss: 0.0000000000 time: 0.05747413635253906\n",
      "Iteration: 14570 loss: 0.0000000000 time: 0.06834006309509277\n",
      "Iteration: 14580 loss: 0.0000000000 time: 0.06870222091674805\n",
      "Iteration: 14590 loss: 0.0000000000 time: 0.06592679023742676\n",
      "Iteration: 14600 loss: 0.0000000000 time: 0.0667884349822998\n",
      "Iteration: 14610 loss: 0.0000000000 time: 0.06850886344909668\n",
      "Iteration: 14620 loss: 0.0000000000 time: 0.06700468063354492\n",
      "Iteration: 14630 loss: 0.0000000000 time: 0.06442427635192871\n",
      "Iteration: 14640 loss: 0.0000000002 time: 0.06907153129577637\n",
      "Iteration: 14650 loss: 0.0000005343 time: 0.07552385330200195\n",
      "Iteration: 14660 loss: 0.0000070497 time: 0.0685577392578125\n",
      "Iteration: 14670 loss: 0.0000001735 time: 0.06632065773010254\n",
      "Iteration: 14680 loss: 0.0000002880 time: 0.06810116767883301\n",
      "Iteration: 14690 loss: 0.0000007316 time: 0.06629037857055664\n",
      "Iteration: 14700 loss: 0.0000000006 time: 0.06630349159240723\n",
      "Iteration: 14710 loss: 0.0000000845 time: 0.06961536407470703\n",
      "Iteration: 14720 loss: 0.0000000129 time: 0.06634402275085449\n",
      "Iteration: 14730 loss: 0.0000000036 time: 0.07164120674133301\n",
      "Iteration: 14740 loss: 0.0000000037 time: 0.0710446834564209\n",
      "Iteration: 14750 loss: 0.0000000000 time: 0.07000946998596191\n",
      "Iteration: 14760 loss: 0.0000000005 time: 0.06829452514648438\n",
      "Iteration: 14770 loss: 0.0000000000 time: 0.06982183456420898\n",
      "Iteration: 14780 loss: 0.0000000001 time: 0.07023119926452637\n",
      "Iteration: 14790 loss: 0.0000000000 time: 0.06925320625305176\n",
      "Iteration: 14800 loss: 0.0000000000 time: 0.07059359550476074\n",
      "Iteration: 14810 loss: 0.0000000000 time: 0.07257366180419922\n",
      "Iteration: 14820 loss: 0.0000000000 time: 0.06905126571655273\n",
      "Iteration: 14830 loss: 0.0000000000 time: 0.06890439987182617\n",
      "Iteration: 14840 loss: 0.0000000000 time: 0.07210779190063477\n",
      "Iteration: 14850 loss: 0.0000000000 time: 0.07101082801818848\n",
      "Iteration: 14860 loss: 0.0000000000 time: 0.06569528579711914\n",
      "Iteration: 14870 loss: 0.0000000000 time: 0.06570696830749512\n",
      "Iteration: 14880 loss: 0.0000000000 time: 0.07177495956420898\n",
      "Iteration: 14890 loss: 0.0000000000 time: 0.07554936408996582\n",
      "Iteration: 14900 loss: 0.0000000000 time: 0.07571530342102051\n",
      "Iteration: 14910 loss: 0.0000000000 time: 0.06527876853942871\n",
      "Iteration: 14920 loss: 0.0000000000 time: 0.0636286735534668\n",
      "Iteration: 14930 loss: 0.0000000000 time: 0.06483578681945801\n",
      "Iteration: 14940 loss: 0.0000000000 time: 0.059476613998413086\n",
      "Iteration: 14950 loss: 0.0000000000 time: 0.06911277770996094\n",
      "Iteration: 14960 loss: 0.0000000000 time: 0.0728600025177002\n",
      "Iteration: 14970 loss: 0.0000000000 time: 0.07090115547180176\n",
      "Iteration: 14980 loss: 0.0000000000 time: 0.06820511817932129\n",
      "Iteration: 14990 loss: 0.0000000000 time: 0.06947708129882812\n",
      "Iteration: 15000 loss: 0.0000000000 time: 0.0712130069732666\n",
      "-->mesh : \n",
      "     n_triangles :  64\n",
      "     n_vertices  :  41\n",
      "     n_edges     :  104\n",
      "     h_max           :  0.2500000000006653\n",
      "     h_min           :  0.1767766952961665\n",
      "-->test_fun      : \n",
      "     order       :  1\n",
      "     dof         :  25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 12:14:08.366393: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:14:08.371246: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:14:08.373588: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:14:08.375805: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:14:08.377706: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:14:08.379721: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:14:08.381361: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:14:08.383353: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:14:08.385151: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:14:08.387327: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:14:09.685659: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_26' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_26}}]]\n",
      "2023-12-26 12:14:09.685822: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_41' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_41}}]]\n",
      "2023-12-26 12:14:09.685887: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_58' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_58}}]]\n",
      "2023-12-26 12:14:09.686025: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_74' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_74}}]]\n",
      "2023-12-26 12:14:09.686229: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_94' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_94}}]]\n",
      "2023-12-26 12:14:09.686383: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_123' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_123}}]]\n",
      "2023-12-26 12:14:09.686517: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_141' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_141}}]]\n",
      "2023-12-26 12:14:09.686599: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_159' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_159}}]]\n",
      "2023-12-26 12:14:09.686738: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_164' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_164}}]]\n",
      "2023-12-26 12:14:09.686914: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_180' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_180}}]]\n",
      "2023-12-26 12:14:09.687130: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_198' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_198}}]]\n",
      "2023-12-26 12:14:09.687287: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_226' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_226}}]]\n",
      "2023-12-26 12:14:09.687505: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_234' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_234}}]]\n",
      "2023-12-26 12:14:09.687739: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_263' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_263}}]]\n",
      "2023-12-26 12:14:09.687880: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_281' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_281}}]]\n",
      "2023-12-26 12:14:09.688058: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_299' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_299}}]]\n",
      "2023-12-26 12:14:09.688218: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_304' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_304}}]]\n",
      "2023-12-26 12:14:09.688372: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_320' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_320}}]]\n",
      "2023-12-26 12:14:09.688506: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_336' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_336}}]]\n",
      "2023-12-26 12:14:09.688737: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_352' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_352}}]]\n",
      "2023-12-26 12:14:09.688868: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_372' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_372}}]]\n",
      "2023-12-26 12:14:09.688970: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_401' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_401}}]]\n",
      "2023-12-26 12:14:09.689103: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_419' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_419}}]]\n",
      "2023-12-26 12:14:09.689261: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_437' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_437}}]]\n",
      "2023-12-26 12:14:09.689395: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_442' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_442}}]]\n",
      "2023-12-26 12:14:09.689569: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_458' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_458}}]]\n",
      "2023-12-26 12:14:09.689706: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_476' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_476}}]]\n",
      "2023-12-26 12:14:09.689948: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_504' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_504}}]]\n",
      "2023-12-26 12:14:09.690124: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_512' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_512}}]]\n",
      "2023-12-26 12:14:09.690277: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_541' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_541}}]]\n",
      "2023-12-26 12:14:09.690434: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_559' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_559}}]]\n",
      "2023-12-26 12:14:09.690571: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_577' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_577}}]]\n",
      "2023-12-26 12:14:09.690748: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_582' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_582}}]]\n",
      "2023-12-26 12:14:09.690896: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_598' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_598}}]]\n",
      "2023-12-26 12:14:09.691047: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_614' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_614}}]]\n",
      "2023-12-26 12:14:09.691202: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_630' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_630}}]]\n",
      "2023-12-26 12:14:09.691430: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_650' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_650}}]]\n",
      "2023-12-26 12:14:09.691490: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_679' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_679}}]]\n",
      "2023-12-26 12:14:09.691589: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_697' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_697}}]]\n",
      "2023-12-26 12:14:09.691666: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_715' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_715}}]]\n",
      "2023-12-26 12:14:09.691763: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_720' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_720}}]]\n",
      "2023-12-26 12:14:09.691826: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_736' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_736}}]]\n",
      "2023-12-26 12:14:09.691947: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_754' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_754}}]]\n",
      "2023-12-26 12:14:09.692050: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_782' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_782}}]]\n",
      "2023-12-26 12:14:09.692112: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_790' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_790}}]]\n",
      "2023-12-26 12:14:09.692206: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_819' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_819}}]]\n",
      "2023-12-26 12:14:09.692268: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_837' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_837}}]]\n",
      "2023-12-26 12:14:09.692364: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_855' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_855}}]]\n",
      "2023-12-26 12:14:09.692427: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_860' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_860}}]]\n",
      "2023-12-26 12:14:09.692523: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_876' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_876}}]]\n",
      "2023-12-26 12:14:09.692586: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_892' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_892}}]]\n",
      "2023-12-26 12:14:09.692682: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_908' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_908}}]]\n",
      "2023-12-26 12:14:09.692743: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_928' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_928}}]]\n",
      "2023-12-26 12:14:09.692839: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_957' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_957}}]]\n",
      "2023-12-26 12:14:09.692900: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_975' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_975}}]]\n",
      "2023-12-26 12:14:09.693014: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_993' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_993}}]]\n",
      "2023-12-26 12:14:09.693111: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_998' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_998}}]]\n",
      "2023-12-26 12:14:09.693207: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1014' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1014}}]]\n",
      "2023-12-26 12:14:09.693270: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1032' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1032}}]]\n",
      "2023-12-26 12:14:09.693366: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1060' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1060}}]]\n",
      "2023-12-26 12:14:09.693428: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1068' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1068}}]]\n",
      "2023-12-26 12:14:09.693524: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1096' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1096}}]]\n",
      "2023-12-26 12:14:09.693584: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1107' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1107}}]]\n",
      "2023-12-26 12:14:09.693679: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1114' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1114}}]]\n",
      "2023-12-26 12:14:09.693741: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1119' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1119}}]]\n",
      "2023-12-26 12:14:09.693837: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1122' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1122}}]]\n",
      "2023-12-26 12:14:09.693898: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1125' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1125}}]]\n",
      "2023-12-26 12:14:09.693994: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1128' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1128}}]]\n",
      "2023-12-26 12:14:09.694090: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1131' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1131}}]]\n",
      "2023-12-26 12:14:09.694186: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1134' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1134}}]]\n",
      "2023-12-26 12:14:09.694282: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1137' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1137}}]]\n",
      "2023-12-26 12:14:09.694377: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1140' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1140}}]]\n",
      "2023-12-26 12:14:09.694474: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1143' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1143}}]]\n",
      "2023-12-26 12:14:09.694570: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1146' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1146}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 loss: 0.0085826756 time: 9.013261318206787\n",
      "Iteration: 10 loss: 0.0021162419 time: 0.18950939178466797\n",
      "Iteration: 20 loss: 0.0012719658 time: 0.19253158569335938\n",
      "Iteration: 30 loss: 0.0008170565 time: 0.192427396774292\n",
      "Iteration: 40 loss: 0.0005867273 time: 0.1760876178741455\n",
      "Iteration: 50 loss: 0.0005543954 time: 0.1840050220489502\n",
      "Iteration: 60 loss: 0.0005322618 time: 0.18494534492492676\n",
      "Iteration: 70 loss: 0.0005120517 time: 0.18655872344970703\n",
      "Iteration: 80 loss: 0.0005017849 time: 0.18309593200683594\n",
      "Iteration: 90 loss: 0.0004912078 time: 0.17968010902404785\n",
      "Iteration: 100 loss: 0.0004804885 time: 0.17446446418762207\n",
      "Iteration: 110 loss: 0.0004686886 time: 0.18153667449951172\n",
      "Iteration: 120 loss: 0.0004554401 time: 0.17797255516052246\n",
      "Iteration: 130 loss: 0.0004405032 time: 0.17516756057739258\n",
      "Iteration: 140 loss: 0.0004236225 time: 0.17312932014465332\n",
      "Iteration: 150 loss: 0.0004047164 time: 0.1858367919921875\n",
      "Iteration: 160 loss: 0.0003839269 time: 0.17750954627990723\n",
      "Iteration: 170 loss: 0.0003617999 time: 0.17570233345031738\n",
      "Iteration: 180 loss: 0.0003394605 time: 0.16898441314697266\n",
      "Iteration: 190 loss: 0.0003185751 time: 0.1721971035003662\n",
      "Iteration: 200 loss: 0.0003009208 time: 0.17955970764160156\n",
      "Iteration: 210 loss: 0.0002875240 time: 0.18050003051757812\n",
      "Iteration: 220 loss: 0.0002779751 time: 0.17484617233276367\n",
      "Iteration: 230 loss: 0.0002707761 time: 0.1718919277191162\n",
      "Iteration: 240 loss: 0.0002644841 time: 0.1721491813659668\n",
      "Iteration: 250 loss: 0.0002584047 time: 0.17380404472351074\n",
      "Iteration: 260 loss: 0.0002524036 time: 0.17367100715637207\n",
      "Iteration: 270 loss: 0.0002464980 time: 0.18077492713928223\n",
      "Iteration: 280 loss: 0.0002406883 time: 0.1780092716217041\n",
      "Iteration: 290 loss: 0.0002349662 time: 0.17641520500183105\n",
      "Iteration: 300 loss: 0.0002293429 time: 0.1706545352935791\n",
      "Iteration: 310 loss: 0.0002238547 time: 0.17534160614013672\n",
      "Iteration: 320 loss: 0.0002185534 time: 0.1714937686920166\n",
      "Iteration: 330 loss: 0.0002134957 time: 0.1655256748199463\n",
      "Iteration: 340 loss: 0.0002087294 time: 0.17422819137573242\n",
      "Iteration: 350 loss: 0.0002042804 time: 0.17419815063476562\n",
      "Iteration: 360 loss: 0.0002001416 time: 0.17016887664794922\n",
      "Iteration: 370 loss: 0.0001962683 time: 0.17715668678283691\n",
      "Iteration: 380 loss: 0.0001925816 time: 0.1740119457244873\n",
      "Iteration: 390 loss: 0.0001889794 time: 0.18102264404296875\n",
      "Iteration: 400 loss: 0.0001853484 time: 0.16624069213867188\n",
      "Iteration: 410 loss: 0.0001815734 time: 0.17861175537109375\n",
      "Iteration: 420 loss: 0.0001775416 time: 0.17290067672729492\n",
      "Iteration: 430 loss: 0.0001731413 time: 0.1716904640197754\n",
      "Iteration: 440 loss: 0.0001682565 time: 0.17424321174621582\n",
      "Iteration: 450 loss: 0.0001627606 time: 0.17218661308288574\n",
      "Iteration: 460 loss: 0.0001565097 time: 0.17800068855285645\n",
      "Iteration: 470 loss: 0.0001493383 time: 0.16974806785583496\n",
      "Iteration: 480 loss: 0.0001410594 time: 0.18162822723388672\n",
      "Iteration: 490 loss: 0.0001314734 time: 0.16758465766906738\n",
      "Iteration: 500 loss: 0.0001203955 time: 0.16974377632141113\n",
      "Iteration: 510 loss: 0.0001077125 time: 0.17965412139892578\n",
      "Iteration: 520 loss: 0.0000934861 time: 0.1773970127105713\n",
      "Iteration: 530 loss: 0.0000780962 time: 0.17243456840515137\n",
      "Iteration: 540 loss: 0.0000623650 time: 0.16818523406982422\n",
      "Iteration: 550 loss: 0.0000475279 time: 0.17920947074890137\n",
      "Iteration: 560 loss: 0.0000349244 time: 0.16645359992980957\n",
      "Iteration: 570 loss: 0.0000254774 time: 0.16764211654663086\n",
      "Iteration: 580 loss: 0.0000192823 time: 0.1704561710357666\n",
      "Iteration: 590 loss: 0.0000156553 time: 0.16436100006103516\n",
      "Iteration: 600 loss: 0.0000136253 time: 0.17386269569396973\n",
      "Iteration: 610 loss: 0.0000124133 time: 0.1751265525817871\n",
      "Iteration: 620 loss: 0.0000115740 time: 0.1775345802307129\n",
      "Iteration: 630 loss: 0.0000109053 time: 0.1723926067352295\n",
      "Iteration: 640 loss: 0.0000103320 time: 0.17724370956420898\n",
      "Iteration: 650 loss: 0.0000098303 time: 0.17378807067871094\n",
      "Iteration: 660 loss: 0.0000093894 time: 0.1778886318206787\n",
      "Iteration: 670 loss: 0.0000089991 time: 0.17965340614318848\n",
      "Iteration: 680 loss: 0.0000086497 time: 0.16454601287841797\n",
      "Iteration: 690 loss: 0.0000083334 time: 0.17568135261535645\n",
      "Iteration: 700 loss: 0.0000080441 time: 0.17507600784301758\n",
      "Iteration: 710 loss: 0.0000077769 time: 0.17617058753967285\n",
      "Iteration: 720 loss: 0.0000075281 time: 0.17385387420654297\n",
      "Iteration: 730 loss: 0.0000072945 time: 0.17557907104492188\n",
      "Iteration: 740 loss: 0.0000070738 time: 0.17794108390808105\n",
      "Iteration: 750 loss: 0.0000068638 time: 0.1614236831665039\n",
      "Iteration: 760 loss: 0.0000066629 time: 0.17698454856872559\n",
      "Iteration: 770 loss: 0.0000064700 time: 0.1795654296875\n",
      "Iteration: 780 loss: 0.0000062839 time: 0.17076826095581055\n",
      "Iteration: 790 loss: 0.0000061038 time: 0.17897534370422363\n",
      "Iteration: 800 loss: 0.0000059293 time: 0.16619181632995605\n",
      "Iteration: 810 loss: 0.0000057597 time: 0.17027854919433594\n",
      "Iteration: 820 loss: 0.0000055949 time: 0.17797136306762695\n",
      "Iteration: 830 loss: 0.0000054346 time: 0.17790651321411133\n",
      "Iteration: 840 loss: 0.0000052785 time: 0.1687469482421875\n",
      "Iteration: 850 loss: 0.0000051268 time: 0.17296218872070312\n",
      "Iteration: 860 loss: 0.0000049792 time: 0.17384839057922363\n",
      "Iteration: 870 loss: 0.0000048358 time: 0.17457294464111328\n",
      "Iteration: 880 loss: 0.0000046967 time: 0.17694664001464844\n",
      "Iteration: 890 loss: 0.0000045618 time: 0.1727147102355957\n",
      "Iteration: 900 loss: 0.0000044312 time: 0.1770036220550537\n",
      "Iteration: 910 loss: 0.0000043050 time: 0.17647266387939453\n",
      "Iteration: 920 loss: 0.0000041833 time: 0.1705307960510254\n",
      "Iteration: 930 loss: 0.0000040659 time: 0.1695537567138672\n",
      "Iteration: 940 loss: 0.0000039530 time: 0.18064379692077637\n",
      "Iteration: 950 loss: 0.0000038446 time: 0.17076659202575684\n",
      "Iteration: 960 loss: 0.0000037406 time: 0.17244911193847656\n",
      "Iteration: 970 loss: 0.0000036411 time: 0.16834068298339844\n",
      "Iteration: 980 loss: 0.0000035458 time: 0.16945219039916992\n",
      "Iteration: 990 loss: 0.0000034548 time: 0.16919875144958496\n",
      "Iteration: 1000 loss: 0.0000033680 time: 0.1738896369934082\n",
      "Iteration: 1010 loss: 0.0000032851 time: 0.17661309242248535\n",
      "Iteration: 1020 loss: 0.0000032061 time: 0.17724370956420898\n",
      "Iteration: 1030 loss: 0.0000031308 time: 0.16889071464538574\n",
      "Iteration: 1040 loss: 0.0000030591 time: 0.17557001113891602\n",
      "Iteration: 1050 loss: 0.0000029906 time: 0.17214202880859375\n",
      "Iteration: 1060 loss: 0.0000029253 time: 0.17128586769104004\n",
      "Iteration: 1070 loss: 0.0000028629 time: 0.17541217803955078\n",
      "Iteration: 1080 loss: 0.0000028033 time: 0.17637419700622559\n",
      "Iteration: 1090 loss: 0.0000027462 time: 0.17348575592041016\n",
      "Iteration: 1100 loss: 0.0000026914 time: 0.17774605751037598\n",
      "Iteration: 1110 loss: 0.0000026389 time: 0.18030548095703125\n",
      "Iteration: 1120 loss: 0.0000025883 time: 0.1740553379058838\n",
      "Iteration: 1130 loss: 0.0000025396 time: 0.17554593086242676\n",
      "Iteration: 1140 loss: 0.0000024926 time: 0.17789340019226074\n",
      "Iteration: 1150 loss: 0.0000024471 time: 0.17606544494628906\n",
      "Iteration: 1160 loss: 0.0000024030 time: 0.16959047317504883\n",
      "Iteration: 1170 loss: 0.0000023603 time: 0.17067813873291016\n",
      "Iteration: 1180 loss: 0.0000023188 time: 0.17084336280822754\n",
      "Iteration: 1190 loss: 0.0000022785 time: 0.1722273826599121\n",
      "Iteration: 1200 loss: 0.0000022392 time: 0.1729111671447754\n",
      "Iteration: 1210 loss: 0.0000022009 time: 0.17107224464416504\n",
      "Iteration: 1220 loss: 0.0000021635 time: 0.1656792163848877\n",
      "Iteration: 1230 loss: 0.0000021271 time: 0.1627793312072754\n",
      "Iteration: 1240 loss: 0.0000020915 time: 0.17322444915771484\n",
      "Iteration: 1250 loss: 0.0000020567 time: 0.17691779136657715\n",
      "Iteration: 1260 loss: 0.0000020227 time: 0.16907811164855957\n",
      "Iteration: 1270 loss: 0.0000019894 time: 0.16992664337158203\n",
      "Iteration: 1280 loss: 0.0000019570 time: 0.1804947853088379\n",
      "Iteration: 1290 loss: 0.0000019252 time: 0.18186450004577637\n",
      "Iteration: 1300 loss: 0.0000018942 time: 0.17574143409729004\n",
      "Iteration: 1310 loss: 0.0000018639 time: 0.17580080032348633\n",
      "Iteration: 1320 loss: 0.0000018342 time: 0.1657414436340332\n",
      "Iteration: 1330 loss: 0.0000018053 time: 0.17489838600158691\n",
      "Iteration: 1340 loss: 0.0000017770 time: 0.1719987392425537\n",
      "Iteration: 1350 loss: 0.0000017494 time: 0.17337870597839355\n",
      "Iteration: 1360 loss: 0.0000017224 time: 0.17831683158874512\n",
      "Iteration: 1370 loss: 0.0000016961 time: 0.18471765518188477\n",
      "Iteration: 1380 loss: 0.0000016703 time: 0.1708686351776123\n",
      "Iteration: 1390 loss: 0.0000016452 time: 0.17952752113342285\n",
      "Iteration: 1400 loss: 0.0000016207 time: 0.1825408935546875\n",
      "Iteration: 1410 loss: 0.0000015968 time: 0.16697120666503906\n",
      "Iteration: 1420 loss: 0.0000015734 time: 0.17416095733642578\n",
      "Iteration: 1430 loss: 0.0000015506 time: 0.16913104057312012\n",
      "Iteration: 1440 loss: 0.0000015283 time: 0.17149877548217773\n",
      "Iteration: 1450 loss: 0.0000015066 time: 0.16812419891357422\n",
      "Iteration: 1460 loss: 0.0000014854 time: 0.17621397972106934\n",
      "Iteration: 1470 loss: 0.0000014646 time: 0.18308353424072266\n",
      "Iteration: 1480 loss: 0.0000014444 time: 0.17311382293701172\n",
      "Iteration: 1490 loss: 0.0000014246 time: 0.17339658737182617\n",
      "Iteration: 1500 loss: 0.0000014052 time: 0.18583226203918457\n",
      "Iteration: 1510 loss: 0.0000013863 time: 0.17634224891662598\n",
      "Iteration: 1520 loss: 0.0000013678 time: 0.1661243438720703\n",
      "Iteration: 1530 loss: 0.0000013497 time: 0.17399382591247559\n",
      "Iteration: 1540 loss: 0.0000013320 time: 0.17172980308532715\n",
      "Iteration: 1550 loss: 0.0000013147 time: 0.1654505729675293\n",
      "Iteration: 1560 loss: 0.0000012977 time: 0.16764521598815918\n",
      "Iteration: 1570 loss: 0.0000012811 time: 0.1705629825592041\n",
      "Iteration: 1580 loss: 0.0000012647 time: 0.1685338020324707\n",
      "Iteration: 1590 loss: 0.0000012487 time: 0.17242908477783203\n",
      "Iteration: 1600 loss: 0.0000012330 time: 0.17981791496276855\n",
      "Iteration: 1610 loss: 0.0000012176 time: 0.1747128963470459\n",
      "Iteration: 1620 loss: 0.0000012024 time: 0.180708646774292\n",
      "Iteration: 1630 loss: 0.0000011875 time: 0.17795872688293457\n",
      "Iteration: 1640 loss: 0.0000011728 time: 0.16872525215148926\n",
      "Iteration: 1650 loss: 0.0000011584 time: 0.17156338691711426\n",
      "Iteration: 1660 loss: 0.0000011442 time: 0.168853759765625\n",
      "Iteration: 1670 loss: 0.0000011302 time: 0.17974638938903809\n",
      "Iteration: 1680 loss: 0.0000011164 time: 0.1816856861114502\n",
      "Iteration: 1690 loss: 0.0000011028 time: 0.17528152465820312\n",
      "Iteration: 1700 loss: 0.0000010894 time: 0.1684727668762207\n",
      "Iteration: 1710 loss: 0.0000010762 time: 0.17665672302246094\n",
      "Iteration: 1720 loss: 0.0000010632 time: 0.17208027839660645\n",
      "Iteration: 1730 loss: 0.0000010503 time: 0.17953705787658691\n",
      "Iteration: 1740 loss: 0.0000010375 time: 0.1800687313079834\n",
      "Iteration: 1750 loss: 0.0000010249 time: 0.18068289756774902\n",
      "Iteration: 1760 loss: 0.0000010125 time: 0.16916847229003906\n",
      "Iteration: 1770 loss: 0.0000010002 time: 0.16962957382202148\n",
      "Iteration: 1780 loss: 0.0000009880 time: 0.17078447341918945\n",
      "Iteration: 1790 loss: 0.0000009760 time: 0.17493820190429688\n",
      "Iteration: 1800 loss: 0.0000009641 time: 0.1744835376739502\n",
      "Iteration: 1810 loss: 0.0000009523 time: 0.18033957481384277\n",
      "Iteration: 1820 loss: 0.0000009406 time: 0.17489051818847656\n",
      "Iteration: 1830 loss: 0.0000009290 time: 0.1744534969329834\n",
      "Iteration: 1840 loss: 0.0000009176 time: 0.1781318187713623\n",
      "Iteration: 1850 loss: 0.0000009062 time: 0.1706554889678955\n",
      "Iteration: 1860 loss: 0.0000008950 time: 0.1680889129638672\n",
      "Iteration: 1870 loss: 0.0000008838 time: 0.16778922080993652\n",
      "Iteration: 1880 loss: 0.0000008727 time: 0.17856764793395996\n",
      "Iteration: 1890 loss: 0.0000008618 time: 0.16981935501098633\n",
      "Iteration: 1900 loss: 0.0000008509 time: 0.1621565818786621\n",
      "Iteration: 1910 loss: 0.0000008401 time: 0.17867827415466309\n",
      "Iteration: 1920 loss: 0.0000008294 time: 0.17540454864501953\n",
      "Iteration: 1930 loss: 0.0000008187 time: 0.17664790153503418\n",
      "Iteration: 1940 loss: 0.0000008082 time: 0.17103362083435059\n",
      "Iteration: 1950 loss: 0.0000007977 time: 0.17736530303955078\n",
      "Iteration: 1960 loss: 0.0000007873 time: 0.1727292537689209\n",
      "Iteration: 1970 loss: 0.0000007770 time: 0.17018628120422363\n",
      "Iteration: 1980 loss: 0.0000007668 time: 0.17479538917541504\n",
      "Iteration: 1990 loss: 0.0000007567 time: 0.17351675033569336\n",
      "Iteration: 2000 loss: 0.0000007466 time: 0.17095303535461426\n",
      "Iteration: 2010 loss: 0.0000007366 time: 0.17404460906982422\n",
      "Iteration: 2020 loss: 0.0000007266 time: 0.17083382606506348\n",
      "Iteration: 2030 loss: 0.0000007168 time: 0.1781759262084961\n",
      "Iteration: 2040 loss: 0.0000007070 time: 0.17715167999267578\n",
      "Iteration: 2050 loss: 0.0000006973 time: 0.1711406707763672\n",
      "Iteration: 2060 loss: 0.0000006876 time: 0.1735222339630127\n",
      "Iteration: 2070 loss: 0.0000006781 time: 0.17536020278930664\n",
      "Iteration: 2080 loss: 0.0000006686 time: 0.17345929145812988\n",
      "Iteration: 2090 loss: 0.0000006591 time: 0.17283844947814941\n",
      "Iteration: 2100 loss: 0.0000006498 time: 0.17227602005004883\n",
      "Iteration: 2110 loss: 0.0000006405 time: 0.1757807731628418\n",
      "Iteration: 2120 loss: 0.0000006312 time: 0.17403411865234375\n",
      "Iteration: 2130 loss: 0.0000006221 time: 0.165910005569458\n",
      "Iteration: 2140 loss: 0.0000006130 time: 0.16470813751220703\n",
      "Iteration: 2150 loss: 0.0000006040 time: 0.16944241523742676\n",
      "Iteration: 2160 loss: 0.0000005950 time: 0.17521405220031738\n",
      "Iteration: 2170 loss: 0.0000005862 time: 0.16824936866760254\n",
      "Iteration: 2180 loss: 0.0000005774 time: 0.16469693183898926\n",
      "Iteration: 2190 loss: 0.0000005686 time: 0.16544008255004883\n",
      "Iteration: 2200 loss: 0.0000005600 time: 0.17069339752197266\n",
      "Iteration: 2210 loss: 0.0000005514 time: 0.16672492027282715\n",
      "Iteration: 2220 loss: 0.0000005429 time: 0.17019414901733398\n",
      "Iteration: 2230 loss: 0.0000005345 time: 0.17223763465881348\n",
      "Iteration: 2240 loss: 0.0000005261 time: 0.17353415489196777\n",
      "Iteration: 2250 loss: 0.0000005178 time: 0.17398834228515625\n",
      "Iteration: 2260 loss: 0.0000005096 time: 0.1688690185546875\n",
      "Iteration: 2270 loss: 0.0000005015 time: 0.1623549461364746\n",
      "Iteration: 2280 loss: 0.0000004934 time: 0.1728982925415039\n",
      "Iteration: 2290 loss: 0.0000004855 time: 0.1706833839416504\n",
      "Iteration: 2300 loss: 0.0000004776 time: 0.1630997657775879\n",
      "Iteration: 2310 loss: 0.0000004697 time: 0.16901874542236328\n",
      "Iteration: 2320 loss: 0.0000004620 time: 0.16831016540527344\n",
      "Iteration: 2330 loss: 0.0000004543 time: 0.1681199073791504\n",
      "Iteration: 2340 loss: 0.0000004468 time: 0.1749124526977539\n",
      "Iteration: 2350 loss: 0.0000004393 time: 0.17284870147705078\n",
      "Iteration: 2360 loss: 0.0000004319 time: 0.1768946647644043\n",
      "Iteration: 2370 loss: 0.0000004245 time: 0.17172503471374512\n",
      "Iteration: 2380 loss: 0.0000004173 time: 0.17645740509033203\n",
      "Iteration: 2390 loss: 0.0000004101 time: 0.16639089584350586\n",
      "Iteration: 2400 loss: 0.0000004030 time: 0.1758430004119873\n",
      "Iteration: 2410 loss: 0.0000003960 time: 0.17692208290100098\n",
      "Iteration: 2420 loss: 0.0000003891 time: 0.17261004447937012\n",
      "Iteration: 2430 loss: 0.0000003823 time: 0.17239761352539062\n",
      "Iteration: 2440 loss: 0.0000003755 time: 0.1743457317352295\n",
      "Iteration: 2450 loss: 0.0000003689 time: 0.17989301681518555\n",
      "Iteration: 2460 loss: 0.0000003623 time: 0.17412137985229492\n",
      "Iteration: 2470 loss: 0.0000003558 time: 0.17691564559936523\n",
      "Iteration: 2480 loss: 0.0000003494 time: 0.17976737022399902\n",
      "Iteration: 2490 loss: 0.0000003431 time: 0.18239927291870117\n",
      "Iteration: 2500 loss: 0.0000003369 time: 0.17654848098754883\n",
      "Iteration: 2510 loss: 0.0000003307 time: 0.17799782752990723\n",
      "Iteration: 2520 loss: 0.0000003246 time: 0.17321085929870605\n",
      "Iteration: 2530 loss: 0.0000003187 time: 0.17315077781677246\n",
      "Iteration: 2540 loss: 0.0000003128 time: 0.17637991905212402\n",
      "Iteration: 2550 loss: 0.0000003070 time: 0.18097519874572754\n",
      "Iteration: 2560 loss: 0.0000003012 time: 0.1686413288116455\n",
      "Iteration: 2570 loss: 0.0000002956 time: 0.17973995208740234\n",
      "Iteration: 2580 loss: 0.0000002900 time: 0.17232060432434082\n",
      "Iteration: 2590 loss: 0.0000002846 time: 0.17153286933898926\n",
      "Iteration: 2600 loss: 0.0000002792 time: 0.1752183437347412\n",
      "Iteration: 2610 loss: 0.0000002739 time: 0.17849469184875488\n",
      "Iteration: 2620 loss: 0.0000002687 time: 0.17582464218139648\n",
      "Iteration: 2630 loss: 0.0000002635 time: 0.17003726959228516\n",
      "Iteration: 2640 loss: 0.0000002585 time: 0.17510056495666504\n",
      "Iteration: 2650 loss: 0.0000002535 time: 0.1747264862060547\n",
      "Iteration: 2660 loss: 0.0000002486 time: 0.17977595329284668\n",
      "Iteration: 2670 loss: 0.0000002438 time: 0.177351713180542\n",
      "Iteration: 2680 loss: 0.0000002390 time: 0.1758134365081787\n",
      "Iteration: 2690 loss: 0.0000002343 time: 0.1733555793762207\n",
      "Iteration: 2700 loss: 0.0000002298 time: 0.17009258270263672\n",
      "Iteration: 2710 loss: 0.0000002253 time: 0.1743321418762207\n",
      "Iteration: 2720 loss: 0.0000002208 time: 0.16709017753601074\n",
      "Iteration: 2730 loss: 0.0000002165 time: 0.17556190490722656\n",
      "Iteration: 2740 loss: 0.0000002122 time: 0.17162203788757324\n",
      "Iteration: 2750 loss: 0.0000002080 time: 0.17915892601013184\n",
      "Iteration: 2760 loss: 0.0000002039 time: 0.17138075828552246\n",
      "Iteration: 2770 loss: 0.0000001998 time: 0.1701509952545166\n",
      "Iteration: 2780 loss: 0.0000001958 time: 0.1741039752960205\n",
      "Iteration: 2790 loss: 0.0000001919 time: 0.1780846118927002\n",
      "Iteration: 2800 loss: 0.0000001881 time: 0.17489886283874512\n",
      "Iteration: 2810 loss: 0.0000001843 time: 0.16935157775878906\n",
      "Iteration: 2820 loss: 0.0000001806 time: 0.16866493225097656\n",
      "Iteration: 2830 loss: 0.0000001770 time: 0.17615270614624023\n",
      "Iteration: 2840 loss: 0.0000001735 time: 0.16701245307922363\n",
      "Iteration: 2850 loss: 0.0000001700 time: 0.17496371269226074\n",
      "Iteration: 2860 loss: 0.0000001666 time: 0.17356324195861816\n",
      "Iteration: 2870 loss: 0.0000001632 time: 0.17208433151245117\n",
      "Iteration: 2880 loss: 0.0000001600 time: 0.17064857482910156\n",
      "Iteration: 2890 loss: 0.0000001567 time: 0.1734328269958496\n",
      "Iteration: 2900 loss: 0.0000001536 time: 0.17502331733703613\n",
      "Iteration: 2910 loss: 0.0000001505 time: 0.17351198196411133\n",
      "Iteration: 2920 loss: 0.0000001475 time: 0.1738431453704834\n",
      "Iteration: 2930 loss: 0.0000001445 time: 0.17528223991394043\n",
      "Iteration: 2940 loss: 0.0000001416 time: 0.17296648025512695\n",
      "Iteration: 2950 loss: 0.0000001388 time: 0.17352724075317383\n",
      "Iteration: 2960 loss: 0.0000001360 time: 0.17798566818237305\n",
      "Iteration: 2970 loss: 0.0000001333 time: 0.17174649238586426\n",
      "Iteration: 2980 loss: 0.0000001307 time: 0.17161035537719727\n",
      "Iteration: 2990 loss: 0.0000001281 time: 0.1701812744140625\n",
      "Iteration: 3000 loss: 0.0000001256 time: 0.1712648868560791\n",
      "Iteration: 3010 loss: 0.0000001231 time: 0.17468714714050293\n",
      "Iteration: 3020 loss: 0.0000001207 time: 0.17359113693237305\n",
      "Iteration: 3030 loss: 0.0000001183 time: 0.17111444473266602\n",
      "Iteration: 3040 loss: 0.0000001160 time: 0.16488218307495117\n",
      "Iteration: 3050 loss: 0.0000001137 time: 0.17433762550354004\n",
      "Iteration: 3060 loss: 0.0000001115 time: 0.1730186939239502\n",
      "Iteration: 3070 loss: 0.0000001094 time: 0.1674041748046875\n",
      "Iteration: 3080 loss: 0.0000001073 time: 0.17229962348937988\n",
      "Iteration: 3090 loss: 0.0000001052 time: 0.17793536186218262\n",
      "Iteration: 3100 loss: 0.0000001032 time: 0.17325353622436523\n",
      "Iteration: 3110 loss: 0.0000001012 time: 0.17453503608703613\n",
      "Iteration: 3120 loss: 0.0000000993 time: 0.1814737319946289\n",
      "Iteration: 3130 loss: 0.0000000975 time: 0.1729428768157959\n",
      "Iteration: 3140 loss: 0.0000000956 time: 0.16886043548583984\n",
      "Iteration: 3150 loss: 0.0000000939 time: 0.18029093742370605\n",
      "Iteration: 3160 loss: 0.0000000921 time: 0.16980957984924316\n",
      "Iteration: 3170 loss: 0.0000000905 time: 0.16620087623596191\n",
      "Iteration: 3180 loss: 0.0000000888 time: 0.17615342140197754\n",
      "Iteration: 3190 loss: 0.0000000872 time: 0.18185210227966309\n",
      "Iteration: 3200 loss: 0.0000000856 time: 0.17747259140014648\n",
      "Iteration: 3210 loss: 0.0000000841 time: 0.1751868724822998\n",
      "Iteration: 3220 loss: 0.0000000826 time: 0.1665661334991455\n",
      "Iteration: 3230 loss: 0.0000000811 time: 0.1810901165008545\n",
      "Iteration: 3240 loss: 0.0000000797 time: 0.18081450462341309\n",
      "Iteration: 3250 loss: 0.0000000783 time: 0.18131566047668457\n",
      "Iteration: 3260 loss: 0.0000000770 time: 0.1741657257080078\n",
      "Iteration: 3270 loss: 0.0000000757 time: 0.17963624000549316\n",
      "Iteration: 3280 loss: 0.0000000744 time: 0.17171597480773926\n",
      "Iteration: 3290 loss: 0.0000000731 time: 0.16982412338256836\n",
      "Iteration: 3300 loss: 0.0000000719 time: 0.18007493019104004\n",
      "Iteration: 3310 loss: 0.0000000707 time: 0.1938486099243164\n",
      "Iteration: 3320 loss: 0.0000000695 time: 0.16416192054748535\n",
      "Iteration: 3330 loss: 0.0000000684 time: 0.16867733001708984\n",
      "Iteration: 3340 loss: 0.0000000673 time: 0.16574311256408691\n",
      "Iteration: 3350 loss: 0.0000000662 time: 0.17101097106933594\n",
      "Iteration: 3360 loss: 0.0000000651 time: 0.16626453399658203\n",
      "Iteration: 3370 loss: 0.0000000641 time: 0.18332576751708984\n",
      "Iteration: 3380 loss: 0.0000000630 time: 0.18007540702819824\n",
      "Iteration: 3390 loss: 0.0000000621 time: 0.18139362335205078\n",
      "Iteration: 3400 loss: 0.0000000611 time: 0.16236519813537598\n",
      "Iteration: 3410 loss: 0.0000000601 time: 0.17661142349243164\n",
      "Iteration: 3420 loss: 0.0000000592 time: 0.16849374771118164\n",
      "Iteration: 3430 loss: 0.0000000583 time: 0.17003703117370605\n",
      "Iteration: 3440 loss: 0.0000000574 time: 0.16923022270202637\n",
      "Iteration: 3450 loss: 0.0000000565 time: 0.1743175983428955\n",
      "Iteration: 3460 loss: 0.0000000557 time: 0.172560453414917\n",
      "Iteration: 3470 loss: 0.0000000548 time: 0.17354345321655273\n",
      "Iteration: 3480 loss: 0.0000000540 time: 0.1698927879333496\n",
      "Iteration: 3490 loss: 0.0000000532 time: 0.17316460609436035\n",
      "Iteration: 3500 loss: 0.0000000524 time: 0.17220139503479004\n",
      "Iteration: 3510 loss: 0.0000000516 time: 0.17515015602111816\n",
      "Iteration: 3520 loss: 0.0000000509 time: 0.1714935302734375\n",
      "Iteration: 3530 loss: 0.0000000502 time: 0.1735532283782959\n",
      "Iteration: 3540 loss: 0.0000004233 time: 0.17119121551513672\n",
      "Iteration: 3550 loss: 0.0000001444 time: 0.1748335361480713\n",
      "Iteration: 3560 loss: 0.0000001392 time: 0.1754765510559082\n",
      "Iteration: 3570 loss: 0.0000000961 time: 0.16626620292663574\n",
      "Iteration: 3580 loss: 0.0000000526 time: 0.16568279266357422\n",
      "Iteration: 3590 loss: 0.0000000548 time: 0.1710643768310547\n",
      "Iteration: 3600 loss: 0.0000000460 time: 0.17069172859191895\n",
      "Iteration: 3610 loss: 0.0000000463 time: 0.17756891250610352\n",
      "Iteration: 3620 loss: 0.0000000451 time: 0.1782827377319336\n",
      "Iteration: 3630 loss: 0.0000000442 time: 0.1726698875427246\n",
      "Iteration: 3640 loss: 0.0000000437 time: 0.17273283004760742\n",
      "Iteration: 3650 loss: 0.0000000431 time: 0.17763090133666992\n",
      "Iteration: 3660 loss: 0.0000000425 time: 0.1794750690460205\n",
      "Iteration: 3670 loss: 0.0000000420 time: 0.17458510398864746\n",
      "Iteration: 3680 loss: 0.0000000414 time: 0.17657947540283203\n",
      "Iteration: 3690 loss: 0.0000000409 time: 0.17641997337341309\n",
      "Iteration: 3700 loss: 0.0000000404 time: 0.17027807235717773\n",
      "Iteration: 3710 loss: 0.0000000398 time: 0.1702890396118164\n",
      "Iteration: 3720 loss: 0.0000000393 time: 0.16724300384521484\n",
      "Iteration: 3730 loss: 0.0000000388 time: 0.17032361030578613\n",
      "Iteration: 3740 loss: 0.0000000383 time: 0.17459368705749512\n",
      "Iteration: 3750 loss: 0.0000000378 time: 0.18096709251403809\n",
      "Iteration: 3760 loss: 0.0000000373 time: 0.1709122657775879\n",
      "Iteration: 3770 loss: 0.0000000368 time: 0.17514610290527344\n",
      "Iteration: 3780 loss: 0.0000000364 time: 0.16850638389587402\n",
      "Iteration: 3790 loss: 0.0000000359 time: 0.1741197109222412\n",
      "Iteration: 3800 loss: 0.0000000354 time: 0.16960453987121582\n",
      "Iteration: 3810 loss: 0.0000000350 time: 0.17882680892944336\n",
      "Iteration: 3820 loss: 0.0000000345 time: 0.18245887756347656\n",
      "Iteration: 3830 loss: 0.0000000341 time: 0.17504143714904785\n",
      "Iteration: 3840 loss: 0.0000000336 time: 0.16666913032531738\n",
      "Iteration: 3850 loss: 0.0000000332 time: 0.17711329460144043\n",
      "Iteration: 3860 loss: 0.0000000327 time: 0.16931700706481934\n",
      "Iteration: 3870 loss: 0.0000000323 time: 0.17334604263305664\n",
      "Iteration: 3880 loss: 0.0000000319 time: 0.17273569107055664\n",
      "Iteration: 3890 loss: 0.0000000315 time: 0.17971491813659668\n",
      "Iteration: 3900 loss: 0.0000000310 time: 0.1757216453552246\n",
      "Iteration: 3910 loss: 0.0000000306 time: 0.1730806827545166\n",
      "Iteration: 3920 loss: 0.0000000302 time: 0.17213726043701172\n",
      "Iteration: 3930 loss: 0.0000000298 time: 0.17481756210327148\n",
      "Iteration: 3940 loss: 0.0000000298 time: 0.17235922813415527\n",
      "Iteration: 3950 loss: 0.0000002713 time: 0.1721339225769043\n",
      "Iteration: 3960 loss: 0.0000009220 time: 0.17173123359680176\n",
      "Iteration: 3970 loss: 0.0000003566 time: 0.17092537879943848\n",
      "Iteration: 3980 loss: 0.0000000685 time: 0.17337536811828613\n",
      "Iteration: 3990 loss: 0.0000000308 time: 0.17516064643859863\n",
      "Iteration: 4000 loss: 0.0000000389 time: 0.1752796173095703\n",
      "Iteration: 4010 loss: 0.0000000323 time: 0.1808757781982422\n",
      "Iteration: 4020 loss: 0.0000000280 time: 0.177290678024292\n",
      "Iteration: 4030 loss: 0.0000000268 time: 0.1722109317779541\n",
      "Iteration: 4040 loss: 0.0000000264 time: 0.1721205711364746\n",
      "Iteration: 4050 loss: 0.0000000260 time: 0.17155098915100098\n",
      "Iteration: 4060 loss: 0.0000000257 time: 0.17091870307922363\n",
      "Iteration: 4070 loss: 0.0000000254 time: 0.17517447471618652\n",
      "Iteration: 4080 loss: 0.0000000251 time: 0.18176627159118652\n",
      "Iteration: 4090 loss: 0.0000000248 time: 0.18166708946228027\n",
      "Iteration: 4100 loss: 0.0000000245 time: 0.17984247207641602\n",
      "Iteration: 4110 loss: 0.0000000242 time: 0.17966961860656738\n",
      "Iteration: 4120 loss: 0.0000000239 time: 0.16712665557861328\n",
      "Iteration: 4130 loss: 0.0000000236 time: 0.17730951309204102\n",
      "Iteration: 4140 loss: 0.0000000233 time: 0.17826366424560547\n",
      "Iteration: 4150 loss: 0.0000000230 time: 0.16494059562683105\n",
      "Iteration: 4160 loss: 0.0000000227 time: 0.19331145286560059\n",
      "Iteration: 4170 loss: 0.0000000224 time: 0.17098259925842285\n",
      "Iteration: 4180 loss: 0.0000000221 time: 0.18549704551696777\n",
      "Iteration: 4190 loss: 0.0000000218 time: 0.16777300834655762\n",
      "Iteration: 4200 loss: 0.0000000215 time: 0.1726546287536621\n",
      "Iteration: 4210 loss: 0.0000000213 time: 0.1723012924194336\n",
      "Iteration: 4220 loss: 0.0000000210 time: 0.17504334449768066\n",
      "Iteration: 4230 loss: 0.0000000207 time: 0.18960785865783691\n",
      "Iteration: 4240 loss: 0.0000000205 time: 0.18222784996032715\n",
      "Iteration: 4250 loss: 0.0000000202 time: 0.17097830772399902\n",
      "Iteration: 4260 loss: 0.0000000199 time: 0.17465782165527344\n",
      "Iteration: 4270 loss: 0.0000000197 time: 0.172560453414917\n",
      "Iteration: 4280 loss: 0.0000000194 time: 0.1747744083404541\n",
      "Iteration: 4290 loss: 0.0000000192 time: 0.18683075904846191\n",
      "Iteration: 4300 loss: 0.0000000189 time: 0.17743468284606934\n",
      "Iteration: 4310 loss: 0.0000000189 time: 0.1689291000366211\n",
      "Iteration: 4320 loss: 0.0000001176 time: 0.1746673583984375\n",
      "Iteration: 4330 loss: 0.0000000225 time: 0.1784815788269043\n",
      "Iteration: 4340 loss: 0.0000000496 time: 0.17399239540100098\n",
      "Iteration: 4350 loss: 0.0000001543 time: 0.173842191696167\n",
      "Iteration: 4360 loss: 0.0000000739 time: 0.1838526725769043\n",
      "Iteration: 4370 loss: 0.0000000206 time: 0.17540645599365234\n",
      "Iteration: 4380 loss: 0.0000000187 time: 0.1718289852142334\n",
      "Iteration: 4390 loss: 0.0000000194 time: 0.17048311233520508\n",
      "Iteration: 4400 loss: 0.0000000182 time: 0.17608404159545898\n",
      "Iteration: 4410 loss: 0.0000000173 time: 0.17648887634277344\n",
      "Iteration: 4420 loss: 0.0000000169 time: 0.16813945770263672\n",
      "Iteration: 4430 loss: 0.0000000166 time: 0.17195868492126465\n",
      "Iteration: 4440 loss: 0.0000000164 time: 0.17043066024780273\n",
      "Iteration: 4450 loss: 0.0000000161 time: 0.16951680183410645\n",
      "Iteration: 4460 loss: 0.0000000159 time: 0.1712648868560791\n",
      "Iteration: 4470 loss: 0.0000000158 time: 0.17308878898620605\n",
      "Iteration: 4480 loss: 0.0000000156 time: 0.1772010326385498\n",
      "Iteration: 4490 loss: 0.0000000154 time: 0.1760399341583252\n",
      "Iteration: 4500 loss: 0.0000000152 time: 0.1747722625732422\n",
      "Iteration: 4510 loss: 0.0000000150 time: 0.1779928207397461\n",
      "Iteration: 4520 loss: 0.0000000148 time: 0.18184232711791992\n",
      "Iteration: 4530 loss: 0.0000000146 time: 0.17313456535339355\n",
      "Iteration: 4540 loss: 0.0000000145 time: 0.17224860191345215\n",
      "Iteration: 4550 loss: 0.0000000143 time: 0.18305516242980957\n",
      "Iteration: 4560 loss: 0.0000000141 time: 0.19072365760803223\n",
      "Iteration: 4570 loss: 0.0000000139 time: 0.18130230903625488\n",
      "Iteration: 4580 loss: 0.0000000138 time: 0.1905057430267334\n",
      "Iteration: 4590 loss: 0.0000000136 time: 0.18299174308776855\n",
      "Iteration: 4600 loss: 0.0000000134 time: 0.1793961524963379\n",
      "Iteration: 4610 loss: 0.0000000133 time: 0.18088889122009277\n",
      "Iteration: 4620 loss: 0.0000000131 time: 0.16969537734985352\n",
      "Iteration: 4630 loss: 0.0000000130 time: 0.17504405975341797\n",
      "Iteration: 4640 loss: 0.0000000128 time: 0.16410470008850098\n",
      "Iteration: 4650 loss: 0.0000000127 time: 0.1751399040222168\n",
      "Iteration: 4660 loss: 0.0000000125 time: 0.17588305473327637\n",
      "Iteration: 4670 loss: 0.0000000123 time: 0.16978120803833008\n",
      "Iteration: 4680 loss: 0.0000000122 time: 0.18050026893615723\n",
      "Iteration: 4690 loss: 0.0000000234 time: 0.17614030838012695\n",
      "Iteration: 4700 loss: 0.0000029184 time: 0.17032718658447266\n",
      "Iteration: 4710 loss: 0.0000004209 time: 0.1692824363708496\n",
      "Iteration: 4720 loss: 0.0000003176 time: 0.17208313941955566\n",
      "Iteration: 4730 loss: 0.0000000818 time: 0.18082094192504883\n",
      "Iteration: 4740 loss: 0.0000000120 time: 0.17178702354431152\n",
      "Iteration: 4750 loss: 0.0000000180 time: 0.17493367195129395\n",
      "Iteration: 4760 loss: 0.0000000164 time: 0.17285418510437012\n",
      "Iteration: 4770 loss: 0.0000000131 time: 0.17475652694702148\n",
      "Iteration: 4780 loss: 0.0000000117 time: 0.1717519760131836\n",
      "Iteration: 4790 loss: 0.0000000113 time: 0.16990017890930176\n",
      "Iteration: 4800 loss: 0.0000000111 time: 0.1751408576965332\n",
      "Iteration: 4810 loss: 0.0000000110 time: 0.1744849681854248\n",
      "Iteration: 4820 loss: 0.0000000108 time: 0.17018866539001465\n",
      "Iteration: 4830 loss: 0.0000000107 time: 0.17381882667541504\n",
      "Iteration: 4840 loss: 0.0000000106 time: 0.176499605178833\n",
      "Iteration: 4850 loss: 0.0000000105 time: 0.17256736755371094\n",
      "Iteration: 4860 loss: 0.0000000104 time: 0.16681861877441406\n",
      "Iteration: 4870 loss: 0.0000000103 time: 0.17195868492126465\n",
      "Iteration: 4880 loss: 0.0000000101 time: 0.17091846466064453\n",
      "Iteration: 4890 loss: 0.0000000100 time: 0.17267370223999023\n",
      "Iteration: 4900 loss: 0.0000000102 time: 0.17397499084472656\n",
      "Iteration: 4910 loss: 0.0000000584 time: 0.17581510543823242\n",
      "Iteration: 4920 loss: 0.0000001279 time: 0.18064665794372559\n",
      "Iteration: 4930 loss: 0.0000001693 time: 0.18080735206604004\n",
      "Iteration: 4940 loss: 0.0000000643 time: 0.1724071502685547\n",
      "Iteration: 4950 loss: 0.0000000159 time: 0.16830873489379883\n",
      "Iteration: 4960 loss: 0.0000000101 time: 0.17616939544677734\n",
      "Iteration: 4970 loss: 0.0000000111 time: 0.17205548286437988\n",
      "Iteration: 4980 loss: 0.0000000102 time: 0.18445229530334473\n",
      "Iteration: 4990 loss: 0.0000000092 time: 0.17493891716003418\n",
      "Iteration: 5000 loss: 0.0000000091 time: 0.17246580123901367\n",
      "Iteration: 5010 loss: 0.0000000090 time: 0.1753377914428711\n",
      "Iteration: 5020 loss: 0.0000000089 time: 0.16631531715393066\n",
      "Iteration: 5030 loss: 0.0000000088 time: 0.17383933067321777\n",
      "Iteration: 5040 loss: 0.0000000087 time: 0.17133092880249023\n",
      "Iteration: 5050 loss: 0.0000000086 time: 0.16675448417663574\n",
      "Iteration: 5060 loss: 0.0000000085 time: 0.17179584503173828\n",
      "Iteration: 5070 loss: 0.0000000085 time: 0.17480731010437012\n",
      "Iteration: 5080 loss: 0.0000000139 time: 0.17248249053955078\n",
      "Iteration: 5090 loss: 0.0000004907 time: 0.1786506175994873\n",
      "Iteration: 5100 loss: 0.0000002502 time: 0.1813206672668457\n",
      "Iteration: 5110 loss: 0.0000001260 time: 0.17426037788391113\n",
      "Iteration: 5120 loss: 0.0000001006 time: 0.1764843463897705\n",
      "Iteration: 5130 loss: 0.0000000121 time: 0.17845821380615234\n",
      "Iteration: 5140 loss: 0.0000000149 time: 0.17105317115783691\n",
      "Iteration: 5150 loss: 0.0000000109 time: 0.16937470436096191\n",
      "Iteration: 5160 loss: 0.0000000085 time: 0.17591047286987305\n",
      "Iteration: 5170 loss: 0.0000000081 time: 0.18013930320739746\n",
      "Iteration: 5180 loss: 0.0000000080 time: 0.17224407196044922\n",
      "Iteration: 5190 loss: 0.0000000077 time: 0.1722242832183838\n",
      "Iteration: 5200 loss: 0.0000000076 time: 0.17895054817199707\n",
      "Iteration: 5210 loss: 0.0000000076 time: 0.17612791061401367\n",
      "Iteration: 5220 loss: 0.0000000075 time: 0.1729278564453125\n",
      "Iteration: 5230 loss: 0.0000000074 time: 0.16523075103759766\n",
      "Iteration: 5240 loss: 0.0000000075 time: 0.17191362380981445\n",
      "Iteration: 5250 loss: 0.0000000166 time: 0.17585492134094238\n",
      "Iteration: 5260 loss: 0.0000007761 time: 0.17204642295837402\n",
      "Iteration: 5270 loss: 0.0000002243 time: 0.17087125778198242\n",
      "Iteration: 5280 loss: 0.0000001055 time: 0.17241501808166504\n",
      "Iteration: 5290 loss: 0.0000000171 time: 0.17249155044555664\n",
      "Iteration: 5300 loss: 0.0000000095 time: 0.1654493808746338\n",
      "Iteration: 5310 loss: 0.0000000121 time: 0.17442536354064941\n",
      "Iteration: 5320 loss: 0.0000000077 time: 0.1704702377319336\n",
      "Iteration: 5330 loss: 0.0000000072 time: 0.17809343338012695\n",
      "Iteration: 5340 loss: 0.0000000078 time: 0.1634514331817627\n",
      "Iteration: 5350 loss: 0.0000000307 time: 0.17632579803466797\n",
      "Iteration: 5360 loss: 0.0000009642 time: 0.17940211296081543\n",
      "Iteration: 5370 loss: 0.0000004646 time: 0.17446470260620117\n",
      "Iteration: 5380 loss: 0.0000000435 time: 0.1705031394958496\n",
      "Iteration: 5390 loss: 0.0000000468 time: 0.1757349967956543\n",
      "Iteration: 5400 loss: 0.0000000187 time: 0.16445565223693848\n",
      "Iteration: 5410 loss: 0.0000000074 time: 0.17546606063842773\n",
      "Iteration: 5420 loss: 0.0000000092 time: 0.17735671997070312\n",
      "Iteration: 5430 loss: 0.0000000074 time: 0.1815202236175537\n",
      "Iteration: 5440 loss: 0.0000000066 time: 0.17200636863708496\n",
      "Iteration: 5450 loss: 0.0000000064 time: 0.17306804656982422\n",
      "Iteration: 5460 loss: 0.0000000064 time: 0.17747211456298828\n",
      "Iteration: 5470 loss: 0.0000000063 time: 0.1813068389892578\n",
      "Iteration: 5480 loss: 0.0000000063 time: 0.17780280113220215\n",
      "Iteration: 5490 loss: 0.0000000062 time: 0.17427992820739746\n",
      "Iteration: 5500 loss: 0.0000000061 time: 0.16491150856018066\n",
      "Iteration: 5510 loss: 0.0000000061 time: 0.1669161319732666\n",
      "Iteration: 5520 loss: 0.0000000061 time: 0.170332670211792\n",
      "Iteration: 5530 loss: 0.0000000066 time: 0.16148734092712402\n",
      "Iteration: 5540 loss: 0.0000000393 time: 0.17228126525878906\n",
      "Iteration: 5550 loss: 0.0000015690 time: 0.1740283966064453\n",
      "Iteration: 5560 loss: 0.0000003652 time: 0.17572689056396484\n",
      "Iteration: 5570 loss: 0.0000001280 time: 0.1794109344482422\n",
      "Iteration: 5580 loss: 0.0000000096 time: 0.18282055854797363\n",
      "Iteration: 5590 loss: 0.0000000301 time: 0.17666983604431152\n",
      "Iteration: 5600 loss: 0.0000000065 time: 0.17164278030395508\n",
      "Iteration: 5610 loss: 0.0000000089 time: 0.1687171459197998\n",
      "Iteration: 5620 loss: 0.0000000058 time: 0.17467975616455078\n",
      "Iteration: 5630 loss: 0.0000000062 time: 0.17662739753723145\n",
      "Iteration: 5640 loss: 0.0000000117 time: 0.17031025886535645\n",
      "Iteration: 5650 loss: 0.0000005225 time: 0.17857146263122559\n",
      "Iteration: 5660 loss: 0.0000003782 time: 0.16268515586853027\n",
      "Iteration: 5670 loss: 0.0000000164 time: 0.1735093593597412\n",
      "Iteration: 5680 loss: 0.0000000287 time: 0.1798717975616455\n",
      "Iteration: 5690 loss: 0.0000000214 time: 0.1670529842376709\n",
      "Iteration: 5700 loss: 0.0000000061 time: 0.17246532440185547\n",
      "Iteration: 5710 loss: 0.0000000070 time: 0.1813664436340332\n",
      "Iteration: 5720 loss: 0.0000000058 time: 0.16754961013793945\n",
      "Iteration: 5730 loss: 0.0000000056 time: 0.1792888641357422\n",
      "Iteration: 5740 loss: 0.0000000054 time: 0.17013072967529297\n",
      "Iteration: 5750 loss: 0.0000000054 time: 0.17582011222839355\n",
      "Iteration: 5760 loss: 0.0000000053 time: 0.1703479290008545\n",
      "Iteration: 5770 loss: 0.0000000053 time: 0.16759610176086426\n",
      "Iteration: 5780 loss: 0.0000000052 time: 0.17187809944152832\n",
      "Iteration: 5790 loss: 0.0000000052 time: 0.17731046676635742\n",
      "Iteration: 5800 loss: 0.0000000062 time: 0.16980552673339844\n",
      "Iteration: 5810 loss: 0.0000001141 time: 0.17022013664245605\n",
      "Iteration: 5820 loss: 0.0000010256 time: 0.16920042037963867\n",
      "Iteration: 5830 loss: 0.0000003543 time: 0.1764228343963623\n",
      "Iteration: 5840 loss: 0.0000000190 time: 0.1795940399169922\n",
      "Iteration: 5850 loss: 0.0000000124 time: 0.17701411247253418\n",
      "Iteration: 5860 loss: 0.0000000221 time: 0.1721663475036621\n",
      "Iteration: 5870 loss: 0.0000000123 time: 0.1630711555480957\n",
      "Iteration: 5880 loss: 0.0000000052 time: 0.17401385307312012\n",
      "Iteration: 5890 loss: 0.0000000058 time: 0.1752936840057373\n",
      "Iteration: 5900 loss: 0.0000000052 time: 0.1765902042388916\n",
      "Iteration: 5910 loss: 0.0000000051 time: 0.17814850807189941\n",
      "Iteration: 5920 loss: 0.0000000050 time: 0.17708182334899902\n",
      "Iteration: 5930 loss: 0.0000000049 time: 0.1712353229522705\n",
      "Iteration: 5940 loss: 0.0000000049 time: 0.16869044303894043\n",
      "Iteration: 5950 loss: 0.0000000049 time: 0.1786179542541504\n",
      "Iteration: 5960 loss: 0.0000000048 time: 0.17542624473571777\n",
      "Iteration: 5970 loss: 0.0000000050 time: 0.1779310703277588\n",
      "Iteration: 5980 loss: 0.0000000368 time: 0.16811537742614746\n",
      "Iteration: 5990 loss: 0.0000011814 time: 0.16855812072753906\n",
      "Iteration: 6000 loss: 0.0000001935 time: 0.1630251407623291\n",
      "Iteration: 6010 loss: 0.0000000335 time: 0.1758108139038086\n",
      "Iteration: 6020 loss: 0.0000000098 time: 0.17329192161560059\n",
      "Iteration: 6030 loss: 0.0000000051 time: 0.16973280906677246\n",
      "Iteration: 6040 loss: 0.0000000049 time: 0.1701347827911377\n",
      "Iteration: 6050 loss: 0.0000000054 time: 0.17559242248535156\n",
      "Iteration: 6060 loss: 0.0000000053 time: 0.18020844459533691\n",
      "Iteration: 6070 loss: 0.0000000048 time: 0.1755068302154541\n",
      "Iteration: 6080 loss: 0.0000000046 time: 0.16963887214660645\n",
      "Iteration: 6090 loss: 0.0000000046 time: 0.1805129051208496\n",
      "Iteration: 6100 loss: 0.0000000047 time: 0.1625833511352539\n",
      "Iteration: 6110 loss: 0.0000000102 time: 0.17271900177001953\n",
      "Iteration: 6120 loss: 0.0000004327 time: 0.17213225364685059\n",
      "Iteration: 6130 loss: 0.0000000617 time: 0.1727609634399414\n",
      "Iteration: 6140 loss: 0.0000002768 time: 0.16657376289367676\n",
      "Iteration: 6150 loss: 0.0000000595 time: 0.1828157901763916\n",
      "Iteration: 6160 loss: 0.0000000099 time: 0.1771860122680664\n",
      "Iteration: 6170 loss: 0.0000000185 time: 0.17460942268371582\n",
      "Iteration: 6180 loss: 0.0000000045 time: 0.16681265830993652\n",
      "Iteration: 6190 loss: 0.0000000063 time: 0.17887616157531738\n",
      "Iteration: 6200 loss: 0.0000000045 time: 0.17195582389831543\n",
      "Iteration: 6210 loss: 0.0000000045 time: 0.1727924346923828\n",
      "Iteration: 6220 loss: 0.0000000044 time: 0.1749269962310791\n",
      "Iteration: 6230 loss: 0.0000000044 time: 0.16633105278015137\n",
      "Iteration: 6240 loss: 0.0000000043 time: 0.17119956016540527\n",
      "Iteration: 6250 loss: 0.0000000043 time: 0.17833924293518066\n",
      "Iteration: 6260 loss: 0.0000000043 time: 0.16743707656860352\n",
      "Iteration: 6270 loss: 0.0000000042 time: 0.17171454429626465\n",
      "Iteration: 6280 loss: 0.0000000042 time: 0.17375588417053223\n",
      "Iteration: 6290 loss: 0.0000000042 time: 0.17167234420776367\n",
      "Iteration: 6300 loss: 0.0000000042 time: 0.1692206859588623\n",
      "Iteration: 6310 loss: 0.0000000041 time: 0.17972326278686523\n",
      "Iteration: 6320 loss: 0.0000000041 time: 0.18073749542236328\n",
      "Iteration: 6330 loss: 0.0000000042 time: 0.1800537109375\n",
      "Iteration: 6340 loss: 0.0000000137 time: 0.17551779747009277\n",
      "Iteration: 6350 loss: 0.0000011622 time: 0.17778563499450684\n",
      "Iteration: 6360 loss: 0.0000009797 time: 0.17305755615234375\n",
      "Iteration: 6370 loss: 0.0000001626 time: 0.17794322967529297\n",
      "Iteration: 6380 loss: 0.0000000137 time: 0.17835068702697754\n",
      "Iteration: 6390 loss: 0.0000000059 time: 0.1734771728515625\n",
      "Iteration: 6400 loss: 0.0000000119 time: 0.1752769947052002\n",
      "Iteration: 6410 loss: 0.0000000094 time: 0.17200255393981934\n",
      "Iteration: 6420 loss: 0.0000000048 time: 0.1694798469543457\n",
      "Iteration: 6430 loss: 0.0000000042 time: 0.17464137077331543\n",
      "Iteration: 6440 loss: 0.0000000044 time: 0.17699670791625977\n",
      "Iteration: 6450 loss: 0.0000000099 time: 0.17462944984436035\n",
      "Iteration: 6460 loss: 0.0000006210 time: 0.18178915977478027\n",
      "Iteration: 6470 loss: 0.0000004905 time: 0.17587876319885254\n",
      "Iteration: 6480 loss: 0.0000000555 time: 0.1629478931427002\n",
      "Iteration: 6490 loss: 0.0000000307 time: 0.1805710792541504\n",
      "Iteration: 6500 loss: 0.0000000091 time: 0.1735072135925293\n",
      "Iteration: 6510 loss: 0.0000000122 time: 0.16344928741455078\n",
      "Iteration: 6520 loss: 0.0000000052 time: 0.1687002182006836\n",
      "Iteration: 6530 loss: 0.0000000041 time: 0.17218232154846191\n",
      "Iteration: 6540 loss: 0.0000000043 time: 0.1677720546722412\n",
      "Iteration: 6550 loss: 0.0000000039 time: 0.17322158813476562\n",
      "Iteration: 6560 loss: 0.0000000039 time: 0.17499613761901855\n",
      "Iteration: 6570 loss: 0.0000000038 time: 0.1727592945098877\n",
      "Iteration: 6580 loss: 0.0000000038 time: 0.1761772632598877\n",
      "Iteration: 6590 loss: 0.0000000038 time: 0.17592167854309082\n",
      "Iteration: 6600 loss: 0.0000000038 time: 0.16611361503601074\n",
      "Iteration: 6610 loss: 0.0000000037 time: 0.1731274127960205\n",
      "Iteration: 6620 loss: 0.0000000037 time: 0.1767573356628418\n",
      "Iteration: 6630 loss: 0.0000000037 time: 0.17343473434448242\n",
      "Iteration: 6640 loss: 0.0000000037 time: 0.17781281471252441\n",
      "Iteration: 6650 loss: 0.0000000037 time: 0.1797618865966797\n",
      "Iteration: 6660 loss: 0.0000000038 time: 0.17448949813842773\n",
      "Iteration: 6670 loss: 0.0000000246 time: 0.16947174072265625\n",
      "Iteration: 6680 loss: 0.0000024991 time: 0.17595529556274414\n",
      "Iteration: 6690 loss: 0.0000000750 time: 0.16570377349853516\n",
      "Iteration: 6700 loss: 0.0000000872 time: 0.17432689666748047\n",
      "Iteration: 6710 loss: 0.0000000383 time: 0.1793210506439209\n",
      "Iteration: 6720 loss: 0.0000000189 time: 0.17398810386657715\n",
      "Iteration: 6730 loss: 0.0000000117 time: 0.17345809936523438\n",
      "Iteration: 6740 loss: 0.0000000080 time: 0.17419171333312988\n",
      "Iteration: 6750 loss: 0.0000000054 time: 0.17335820198059082\n",
      "Iteration: 6760 loss: 0.0000000039 time: 0.17662572860717773\n",
      "Iteration: 6770 loss: 0.0000000036 time: 0.17121291160583496\n",
      "Iteration: 6780 loss: 0.0000000036 time: 0.17241382598876953\n",
      "Iteration: 6790 loss: 0.0000000036 time: 0.17005014419555664\n",
      "Iteration: 6800 loss: 0.0000000035 time: 0.17194795608520508\n",
      "Iteration: 6810 loss: 0.0000000035 time: 0.17380714416503906\n",
      "Iteration: 6820 loss: 0.0000000035 time: 0.17406034469604492\n",
      "Iteration: 6830 loss: 0.0000000040 time: 0.17739319801330566\n",
      "Iteration: 6840 loss: 0.0000000850 time: 0.1710197925567627\n",
      "Iteration: 6850 loss: 0.0000004506 time: 0.17695188522338867\n",
      "Iteration: 6860 loss: 0.0000003059 time: 0.1678617000579834\n",
      "Iteration: 6870 loss: 0.0000001279 time: 0.17397427558898926\n",
      "Iteration: 6880 loss: 0.0000000407 time: 0.17326021194458008\n",
      "Iteration: 6890 loss: 0.0000000186 time: 0.16774868965148926\n",
      "Iteration: 6900 loss: 0.0000000073 time: 0.16521573066711426\n",
      "Iteration: 6910 loss: 0.0000000040 time: 0.17345285415649414\n",
      "Iteration: 6920 loss: 0.0000000035 time: 0.17194819450378418\n",
      "Iteration: 6930 loss: 0.0000000035 time: 0.1677858829498291\n",
      "Iteration: 6940 loss: 0.0000000035 time: 0.16810274124145508\n",
      "Iteration: 6950 loss: 0.0000000034 time: 0.18013763427734375\n",
      "Iteration: 6960 loss: 0.0000000034 time: 0.16877174377441406\n",
      "Iteration: 6970 loss: 0.0000000033 time: 0.17470741271972656\n",
      "Iteration: 6980 loss: 0.0000000033 time: 0.17459845542907715\n",
      "Iteration: 6990 loss: 0.0000000033 time: 0.17735910415649414\n",
      "Iteration: 7000 loss: 0.0000000033 time: 0.17218899726867676\n",
      "Iteration: 7010 loss: 0.0000000033 time: 0.172105073928833\n",
      "Iteration: 7020 loss: 0.0000000033 time: 0.17053961753845215\n",
      "Iteration: 7030 loss: 0.0000000078 time: 0.18007946014404297\n",
      "Iteration: 7040 loss: 0.0000005121 time: 0.17556524276733398\n",
      "Iteration: 7050 loss: 0.0000002950 time: 0.17404866218566895\n",
      "Iteration: 7060 loss: 0.0000000808 time: 0.17473340034484863\n",
      "Iteration: 7070 loss: 0.0000001090 time: 0.17513608932495117\n",
      "Iteration: 7080 loss: 0.0000000490 time: 0.18365120887756348\n",
      "Iteration: 7090 loss: 0.0000000098 time: 0.18169093132019043\n",
      "Iteration: 7100 loss: 0.0000000036 time: 0.17220425605773926\n",
      "Iteration: 7110 loss: 0.0000000053 time: 0.17710566520690918\n",
      "Iteration: 7120 loss: 0.0000000035 time: 0.18229365348815918\n",
      "Iteration: 7130 loss: 0.0000000034 time: 0.17438769340515137\n",
      "Iteration: 7140 loss: 0.0000000032 time: 0.17051172256469727\n",
      "Iteration: 7150 loss: 0.0000000032 time: 0.17087364196777344\n",
      "Iteration: 7160 loss: 0.0000000032 time: 0.17501354217529297\n",
      "Iteration: 7170 loss: 0.0000000031 time: 0.17606830596923828\n",
      "Iteration: 7180 loss: 0.0000000031 time: 0.17339396476745605\n",
      "Iteration: 7190 loss: 0.0000000031 time: 0.17300796508789062\n",
      "Iteration: 7200 loss: 0.0000000031 time: 0.17934346199035645\n",
      "Iteration: 7210 loss: 0.0000000031 time: 0.17447304725646973\n",
      "Iteration: 7220 loss: 0.0000000031 time: 0.1780247688293457\n",
      "Iteration: 7230 loss: 0.0000000134 time: 0.17236089706420898\n",
      "Iteration: 7240 loss: 0.0000014009 time: 0.17357850074768066\n",
      "Iteration: 7250 loss: 0.0000001054 time: 0.1770014762878418\n",
      "Iteration: 7260 loss: 0.0000000711 time: 0.1668262481689453\n",
      "Iteration: 7270 loss: 0.0000000237 time: 0.17223882675170898\n",
      "Iteration: 7280 loss: 0.0000000093 time: 0.17003893852233887\n",
      "Iteration: 7290 loss: 0.0000000057 time: 0.17536497116088867\n",
      "Iteration: 7300 loss: 0.0000000044 time: 0.17132353782653809\n",
      "Iteration: 7310 loss: 0.0000000039 time: 0.17545509338378906\n",
      "Iteration: 7320 loss: 0.0000000034 time: 0.1836841106414795\n",
      "Iteration: 7330 loss: 0.0000000031 time: 0.1715383529663086\n",
      "Iteration: 7340 loss: 0.0000000039 time: 0.17629170417785645\n",
      "Iteration: 7350 loss: 0.0000000414 time: 0.17161083221435547\n",
      "Iteration: 7360 loss: 0.0000014806 time: 0.17908692359924316\n",
      "Iteration: 7370 loss: 0.0000004480 time: 0.17613959312438965\n",
      "Iteration: 7380 loss: 0.0000000449 time: 0.17119073867797852\n",
      "Iteration: 7390 loss: 0.0000000426 time: 0.17320704460144043\n",
      "Iteration: 7400 loss: 0.0000000144 time: 0.16848278045654297\n",
      "Iteration: 7410 loss: 0.0000000080 time: 0.17736148834228516\n",
      "Iteration: 7420 loss: 0.0000000035 time: 0.16832852363586426\n",
      "Iteration: 7430 loss: 0.0000000041 time: 0.17157292366027832\n",
      "Iteration: 7440 loss: 0.0000000031 time: 0.17775821685791016\n",
      "Iteration: 7450 loss: 0.0000000029 time: 0.17212247848510742\n",
      "Iteration: 7460 loss: 0.0000000029 time: 0.16105890274047852\n",
      "Iteration: 7470 loss: 0.0000000029 time: 0.1728043556213379\n",
      "Iteration: 7480 loss: 0.0000000029 time: 0.17191600799560547\n",
      "Iteration: 7490 loss: 0.0000000029 time: 0.17489123344421387\n",
      "Iteration: 7500 loss: 0.0000000028 time: 0.16485071182250977\n",
      "Iteration: 7510 loss: 0.0000000028 time: 0.1766829490661621\n",
      "Iteration: 7520 loss: 0.0000000028 time: 0.1697843074798584\n",
      "Iteration: 7530 loss: 0.0000000028 time: 0.1680455207824707\n",
      "Iteration: 7540 loss: 0.0000000028 time: 0.1704862117767334\n",
      "Iteration: 7550 loss: 0.0000000030 time: 0.17735075950622559\n",
      "Iteration: 7560 loss: 0.0000000187 time: 0.17862939834594727\n",
      "Iteration: 7570 loss: 0.0000013477 time: 0.1738753318786621\n",
      "Iteration: 7580 loss: 0.0000008053 time: 0.17495274543762207\n",
      "Iteration: 7590 loss: 0.0000001050 time: 0.17484021186828613\n",
      "Iteration: 7600 loss: 0.0000000046 time: 0.1669003963470459\n",
      "Iteration: 7610 loss: 0.0000000259 time: 0.1739506721496582\n",
      "Iteration: 7620 loss: 0.0000000137 time: 0.17418956756591797\n",
      "Iteration: 7630 loss: 0.0000000028 time: 0.1768326759338379\n",
      "Iteration: 7640 loss: 0.0000000042 time: 0.17408490180969238\n",
      "Iteration: 7650 loss: 0.0000000029 time: 0.16803646087646484\n",
      "Iteration: 7660 loss: 0.0000000029 time: 0.1714627742767334\n",
      "Iteration: 7670 loss: 0.0000000027 time: 0.17926526069641113\n",
      "Iteration: 7680 loss: 0.0000000028 time: 0.1787264347076416\n",
      "Iteration: 7690 loss: 0.0000000069 time: 0.17071866989135742\n",
      "Iteration: 7700 loss: 0.0000003923 time: 0.1721353530883789\n",
      "Iteration: 7710 loss: 0.0000002664 time: 0.1815190315246582\n",
      "Iteration: 7720 loss: 0.0000000275 time: 0.17419981956481934\n",
      "Iteration: 7730 loss: 0.0000000581 time: 0.1725001335144043\n",
      "Iteration: 7740 loss: 0.0000000227 time: 0.18248939514160156\n",
      "Iteration: 7750 loss: 0.0000000034 time: 0.18021392822265625\n",
      "Iteration: 7760 loss: 0.0000000040 time: 0.17272281646728516\n",
      "Iteration: 7770 loss: 0.0000000036 time: 0.16753268241882324\n",
      "Iteration: 7780 loss: 0.0000000027 time: 0.16764307022094727\n",
      "Iteration: 7790 loss: 0.0000000028 time: 0.1720590591430664\n",
      "Iteration: 7800 loss: 0.0000000026 time: 0.17429637908935547\n",
      "Iteration: 7810 loss: 0.0000000026 time: 0.17393732070922852\n",
      "Iteration: 7820 loss: 0.0000000026 time: 0.17070245742797852\n",
      "Iteration: 7830 loss: 0.0000000026 time: 0.17117810249328613\n",
      "Iteration: 7840 loss: 0.0000000026 time: 0.1768035888671875\n",
      "Iteration: 7850 loss: 0.0000000026 time: 0.17432165145874023\n",
      "Iteration: 7860 loss: 0.0000000031 time: 0.17099475860595703\n",
      "Iteration: 7870 loss: 0.0000000933 time: 0.1798710823059082\n",
      "Iteration: 7880 loss: 0.0000014385 time: 0.17051100730895996\n",
      "Iteration: 7890 loss: 0.0000006554 time: 0.17476963996887207\n",
      "Iteration: 7900 loss: 0.0000002163 time: 0.1700437068939209\n",
      "Iteration: 7910 loss: 0.0000000787 time: 0.166945219039917\n",
      "Iteration: 7920 loss: 0.0000000291 time: 0.17148089408874512\n",
      "Iteration: 7930 loss: 0.0000000109 time: 0.17066740989685059\n",
      "Iteration: 7940 loss: 0.0000000045 time: 0.1724388599395752\n",
      "Iteration: 7950 loss: 0.0000000027 time: 0.16896843910217285\n",
      "Iteration: 7960 loss: 0.0000000026 time: 0.17881035804748535\n",
      "Iteration: 7970 loss: 0.0000000027 time: 0.16884088516235352\n",
      "Iteration: 7980 loss: 0.0000000026 time: 0.18251514434814453\n",
      "Iteration: 7990 loss: 0.0000000025 time: 0.1717829704284668\n",
      "Iteration: 8000 loss: 0.0000000025 time: 0.17485451698303223\n",
      "Iteration: 8010 loss: 0.0000000025 time: 0.1721646785736084\n",
      "Iteration: 8020 loss: 0.0000000025 time: 0.1745595932006836\n",
      "Iteration: 8030 loss: 0.0000000025 time: 0.1680924892425537\n",
      "Iteration: 8040 loss: 0.0000000024 time: 0.16995620727539062\n",
      "Iteration: 8050 loss: 0.0000000025 time: 0.1687924861907959\n",
      "Iteration: 8060 loss: 0.0000000051 time: 0.1754016876220703\n",
      "Iteration: 8070 loss: 0.0000004719 time: 0.17576241493225098\n",
      "Iteration: 8080 loss: 0.0000006188 time: 0.17706680297851562\n",
      "Iteration: 8090 loss: 0.0000001062 time: 0.176116943359375\n",
      "Iteration: 8100 loss: 0.0000000332 time: 0.17482304573059082\n",
      "Iteration: 8110 loss: 0.0000000127 time: 0.16673636436462402\n",
      "Iteration: 8120 loss: 0.0000000051 time: 0.17791366577148438\n",
      "Iteration: 8130 loss: 0.0000000028 time: 0.1754317283630371\n",
      "Iteration: 8140 loss: 0.0000000024 time: 0.17080044746398926\n",
      "Iteration: 8150 loss: 0.0000000025 time: 0.17333674430847168\n",
      "Iteration: 8160 loss: 0.0000000025 time: 0.17235040664672852\n",
      "Iteration: 8170 loss: 0.0000000024 time: 0.17493295669555664\n",
      "Iteration: 8180 loss: 0.0000000023 time: 0.1841738224029541\n",
      "Iteration: 8190 loss: 0.0000000023 time: 0.17317557334899902\n",
      "Iteration: 8200 loss: 0.0000000023 time: 0.17769289016723633\n",
      "Iteration: 8210 loss: 0.0000000023 time: 0.1667461395263672\n",
      "Iteration: 8220 loss: 0.0000000023 time: 0.18085122108459473\n",
      "Iteration: 8230 loss: 0.0000000034 time: 0.17531371116638184\n",
      "Iteration: 8240 loss: 0.0000001767 time: 0.16665935516357422\n",
      "Iteration: 8250 loss: 0.0000002465 time: 0.1670215129852295\n",
      "Iteration: 8260 loss: 0.0000004672 time: 0.17424583435058594\n",
      "Iteration: 8270 loss: 0.0000001854 time: 0.1754474639892578\n",
      "Iteration: 8280 loss: 0.0000000694 time: 0.1786785125732422\n",
      "Iteration: 8290 loss: 0.0000000258 time: 0.1864013671875\n",
      "Iteration: 8300 loss: 0.0000000090 time: 0.17082738876342773\n",
      "Iteration: 8310 loss: 0.0000000033 time: 0.18187999725341797\n",
      "Iteration: 8320 loss: 0.0000000023 time: 0.16916370391845703\n",
      "Iteration: 8330 loss: 0.0000000026 time: 0.1793076992034912\n",
      "Iteration: 8340 loss: 0.0000000024 time: 0.1610703468322754\n",
      "Iteration: 8350 loss: 0.0000000023 time: 0.16952276229858398\n",
      "Iteration: 8360 loss: 0.0000000023 time: 0.17066526412963867\n",
      "Iteration: 8370 loss: 0.0000000023 time: 0.18035578727722168\n",
      "Iteration: 8380 loss: 0.0000000022 time: 0.1715407371520996\n",
      "Iteration: 8390 loss: 0.0000000022 time: 0.1792607307434082\n",
      "Iteration: 8400 loss: 0.0000000022 time: 0.17846465110778809\n",
      "Iteration: 8410 loss: 0.0000000022 time: 0.16945648193359375\n",
      "Iteration: 8420 loss: 0.0000000022 time: 0.17661523818969727\n",
      "Iteration: 8430 loss: 0.0000000022 time: 0.1755998134613037\n",
      "Iteration: 8440 loss: 0.0000000022 time: 0.1795029640197754\n",
      "Iteration: 8450 loss: 0.0000000022 time: 0.18499755859375\n",
      "Iteration: 8460 loss: 0.0000000021 time: 0.18233180046081543\n",
      "Iteration: 8470 loss: 0.0000000021 time: 0.17336416244506836\n",
      "Iteration: 8480 loss: 0.0000000023 time: 0.17618417739868164\n",
      "Iteration: 8490 loss: 0.0000000599 time: 0.17755532264709473\n",
      "Iteration: 8500 loss: 0.0000005702 time: 0.17230010032653809\n",
      "Iteration: 8510 loss: 0.0000002770 time: 0.17138028144836426\n",
      "Iteration: 8520 loss: 0.0000000207 time: 0.17345786094665527\n",
      "Iteration: 8530 loss: 0.0000000086 time: 0.1784353256225586\n",
      "Iteration: 8540 loss: 0.0000000164 time: 0.17428207397460938\n",
      "Iteration: 8550 loss: 0.0000000089 time: 0.17221665382385254\n",
      "Iteration: 8560 loss: 0.0000000039 time: 0.17981576919555664\n",
      "Iteration: 8570 loss: 0.0000000025 time: 0.18026113510131836\n",
      "Iteration: 8580 loss: 0.0000000023 time: 0.1753385066986084\n",
      "Iteration: 8590 loss: 0.0000000042 time: 0.1744680404663086\n",
      "Iteration: 8600 loss: 0.0000001183 time: 0.17218494415283203\n",
      "Iteration: 8610 loss: 0.0000010038 time: 0.17290163040161133\n",
      "Iteration: 8620 loss: 0.0000000219 time: 0.17394161224365234\n",
      "Iteration: 8630 loss: 0.0000001143 time: 0.17281746864318848\n",
      "Iteration: 8640 loss: 0.0000000231 time: 0.18097567558288574\n",
      "Iteration: 8650 loss: 0.0000000097 time: 0.16749167442321777\n",
      "Iteration: 8660 loss: 0.0000000057 time: 0.17433667182922363\n",
      "Iteration: 8670 loss: 0.0000000037 time: 0.18132925033569336\n",
      "Iteration: 8680 loss: 0.0000000021 time: 0.1761486530303955\n",
      "Iteration: 8690 loss: 0.0000000023 time: 0.17848587036132812\n",
      "Iteration: 8700 loss: 0.0000000022 time: 0.17300891876220703\n",
      "Iteration: 8710 loss: 0.0000000021 time: 0.17540788650512695\n",
      "Iteration: 8720 loss: 0.0000000020 time: 0.1680290699005127\n",
      "Iteration: 8730 loss: 0.0000000020 time: 0.171356201171875\n",
      "Iteration: 8740 loss: 0.0000000020 time: 0.16549372673034668\n",
      "Iteration: 8750 loss: 0.0000000020 time: 0.17919540405273438\n",
      "Iteration: 8760 loss: 0.0000000020 time: 0.1758589744567871\n",
      "Iteration: 8770 loss: 0.0000000020 time: 0.16811561584472656\n",
      "Iteration: 8780 loss: 0.0000000020 time: 0.18220925331115723\n",
      "Iteration: 8790 loss: 0.0000000020 time: 0.17828798294067383\n",
      "Iteration: 8800 loss: 0.0000000020 time: 0.1799931526184082\n",
      "Iteration: 8810 loss: 0.0000000041 time: 0.17348170280456543\n",
      "Iteration: 8820 loss: 0.0000002172 time: 0.16203904151916504\n",
      "Iteration: 8830 loss: 0.0000001536 time: 0.16991424560546875\n",
      "Iteration: 8840 loss: 0.0000004355 time: 0.17289042472839355\n",
      "Iteration: 8850 loss: 0.0000000966 time: 0.1764671802520752\n",
      "Iteration: 8860 loss: 0.0000000054 time: 0.172257661819458\n",
      "Iteration: 8870 loss: 0.0000000067 time: 0.1788797378540039\n",
      "Iteration: 8880 loss: 0.0000000088 time: 0.17617082595825195\n",
      "Iteration: 8890 loss: 0.0000000029 time: 0.17802786827087402\n",
      "Iteration: 8900 loss: 0.0000000023 time: 0.1731429100036621\n",
      "Iteration: 8910 loss: 0.0000000022 time: 0.17127680778503418\n",
      "Iteration: 8920 loss: 0.0000000020 time: 0.1784677505493164\n",
      "Iteration: 8930 loss: 0.0000000019 time: 0.1846013069152832\n",
      "Iteration: 8940 loss: 0.0000000019 time: 0.17364835739135742\n",
      "Iteration: 8950 loss: 0.0000000019 time: 0.17966341972351074\n",
      "Iteration: 8960 loss: 0.0000000019 time: 0.17872118949890137\n",
      "Iteration: 8970 loss: 0.0000000019 time: 0.17931842803955078\n",
      "Iteration: 8980 loss: 0.0000000019 time: 0.17730379104614258\n",
      "Iteration: 8990 loss: 0.0000000019 time: 0.17792201042175293\n",
      "Iteration: 9000 loss: 0.0000000019 time: 0.18174099922180176\n",
      "Iteration: 9010 loss: 0.0000000051 time: 0.17986154556274414\n",
      "Iteration: 9020 loss: 0.0000004984 time: 0.17759037017822266\n",
      "Iteration: 9030 loss: 0.0000005886 time: 0.18610262870788574\n",
      "Iteration: 9040 loss: 0.0000000912 time: 0.17458081245422363\n",
      "Iteration: 9050 loss: 0.0000000207 time: 0.1795039176940918\n",
      "Iteration: 9060 loss: 0.0000000056 time: 0.1758289337158203\n",
      "Iteration: 9070 loss: 0.0000000021 time: 0.17533230781555176\n",
      "Iteration: 9080 loss: 0.0000000020 time: 0.1671123504638672\n",
      "Iteration: 9090 loss: 0.0000000023 time: 0.1719067096710205\n",
      "Iteration: 9100 loss: 0.0000000022 time: 0.17888188362121582\n",
      "Iteration: 9110 loss: 0.0000000019 time: 0.182905912399292\n",
      "Iteration: 9120 loss: 0.0000000018 time: 0.17492294311523438\n",
      "Iteration: 9130 loss: 0.0000000020 time: 0.17833352088928223\n",
      "Iteration: 9140 loss: 0.0000000213 time: 0.17393255233764648\n",
      "Iteration: 9150 loss: 0.0000022384 time: 0.18043160438537598\n",
      "Iteration: 9160 loss: 0.0000001406 time: 0.17615532875061035\n",
      "Iteration: 9170 loss: 0.0000001348 time: 0.1772000789642334\n",
      "Iteration: 9180 loss: 0.0000000600 time: 0.17411375045776367\n",
      "Iteration: 9190 loss: 0.0000000272 time: 0.17765164375305176\n",
      "Iteration: 9200 loss: 0.0000000133 time: 0.17350506782531738\n",
      "Iteration: 9210 loss: 0.0000000064 time: 0.16586804389953613\n",
      "Iteration: 9220 loss: 0.0000000029 time: 0.17493820190429688\n",
      "Iteration: 9230 loss: 0.0000000019 time: 0.17013263702392578\n",
      "Iteration: 9240 loss: 0.0000000019 time: 0.17228317260742188\n",
      "Iteration: 9250 loss: 0.0000000019 time: 0.17092442512512207\n",
      "Iteration: 9260 loss: 0.0000000018 time: 0.17616009712219238\n",
      "Iteration: 9270 loss: 0.0000000018 time: 0.17245221138000488\n",
      "Iteration: 9280 loss: 0.0000000018 time: 0.17799639701843262\n",
      "Iteration: 9290 loss: 0.0000000017 time: 0.17986249923706055\n",
      "Iteration: 9300 loss: 0.0000000017 time: 0.16785860061645508\n",
      "Iteration: 9310 loss: 0.0000000017 time: 0.16997742652893066\n",
      "Iteration: 9320 loss: 0.0000000017 time: 0.17694783210754395\n",
      "Iteration: 9330 loss: 0.0000000017 time: 0.17567873001098633\n",
      "Iteration: 9340 loss: 0.0000000017 time: 0.1735060214996338\n",
      "Iteration: 9350 loss: 0.0000000017 time: 0.17528676986694336\n",
      "Iteration: 9360 loss: 0.0000000017 time: 0.17455410957336426\n",
      "Iteration: 9370 loss: 0.0000000017 time: 0.17673134803771973\n",
      "Iteration: 9380 loss: 0.0000000017 time: 0.17459559440612793\n",
      "Iteration: 9390 loss: 0.0000000016 time: 0.17369651794433594\n",
      "Iteration: 9400 loss: 0.0000000016 time: 0.17667508125305176\n",
      "Iteration: 9410 loss: 0.0000000016 time: 0.17373991012573242\n",
      "Iteration: 9420 loss: 0.0000000017 time: 0.16480660438537598\n",
      "Iteration: 9430 loss: 0.0000000126 time: 0.16432714462280273\n",
      "Iteration: 9440 loss: 0.0000018205 time: 0.17415165901184082\n",
      "Iteration: 9450 loss: 0.0000001030 time: 0.17514324188232422\n",
      "Iteration: 9460 loss: 0.0000001191 time: 0.1792283058166504\n",
      "Iteration: 9470 loss: 0.0000000714 time: 0.17099761962890625\n",
      "Iteration: 9480 loss: 0.0000000218 time: 0.1674814224243164\n",
      "Iteration: 9490 loss: 0.0000000091 time: 0.1797335147857666\n",
      "Iteration: 9500 loss: 0.0000000645 time: 0.16924023628234863\n",
      "Iteration: 9510 loss: 0.0000006486 time: 0.1760256290435791\n",
      "Iteration: 9520 loss: 0.0000001585 time: 0.1778724193572998\n",
      "Iteration: 9530 loss: 0.0000000705 time: 0.1767284870147705\n",
      "Iteration: 9540 loss: 0.0000000300 time: 0.1791083812713623\n",
      "Iteration: 9550 loss: 0.0000000117 time: 0.17941737174987793\n",
      "Iteration: 9560 loss: 0.0000000039 time: 0.17917656898498535\n",
      "Iteration: 9570 loss: 0.0000000016 time: 0.16012787818908691\n",
      "Iteration: 9580 loss: 0.0000000023 time: 0.17414021492004395\n",
      "Iteration: 9590 loss: 0.0000000016 time: 0.1737504005432129\n",
      "Iteration: 9600 loss: 0.0000000019 time: 0.17779135704040527\n",
      "Iteration: 9610 loss: 0.0000000027 time: 0.17240047454833984\n",
      "Iteration: 9620 loss: 0.0000000162 time: 0.17111515998840332\n",
      "Iteration: 9630 loss: 0.0000003844 time: 0.17235255241394043\n",
      "Iteration: 9640 loss: 0.0000000058 time: 0.16646480560302734\n",
      "Iteration: 9650 loss: 0.0000000521 time: 0.16817021369934082\n",
      "Iteration: 9660 loss: 0.0000000626 time: 0.18421602249145508\n",
      "Iteration: 9670 loss: 0.0000000191 time: 0.17618465423583984\n",
      "Iteration: 9680 loss: 0.0000000039 time: 0.1702263355255127\n",
      "Iteration: 9690 loss: 0.0000000018 time: 0.1752161979675293\n",
      "Iteration: 9700 loss: 0.0000000017 time: 0.17341971397399902\n",
      "Iteration: 9710 loss: 0.0000000017 time: 0.17408537864685059\n",
      "Iteration: 9720 loss: 0.0000000017 time: 0.1728827953338623\n",
      "Iteration: 9730 loss: 0.0000000016 time: 0.17742276191711426\n",
      "Iteration: 9740 loss: 0.0000000015 time: 0.17633938789367676\n",
      "Iteration: 9750 loss: 0.0000000015 time: 0.1765761375427246\n",
      "Iteration: 9760 loss: 0.0000000015 time: 0.1752479076385498\n",
      "Iteration: 9770 loss: 0.0000000015 time: 0.17948675155639648\n",
      "Iteration: 9780 loss: 0.0000000016 time: 0.17010283470153809\n",
      "Iteration: 9790 loss: 0.0000000114 time: 0.1772773265838623\n",
      "Iteration: 9800 loss: 0.0000010875 time: 0.17227602005004883\n",
      "Iteration: 9810 loss: 0.0000008850 time: 0.1780257225036621\n",
      "Iteration: 9820 loss: 0.0000001141 time: 0.1744859218597412\n",
      "Iteration: 9830 loss: 0.0000000032 time: 0.17072510719299316\n",
      "Iteration: 9840 loss: 0.0000000088 time: 0.1728806495666504\n",
      "Iteration: 9850 loss: 0.0000000125 time: 0.17735004425048828\n",
      "Iteration: 9860 loss: 0.0000000056 time: 0.17426753044128418\n",
      "Iteration: 9870 loss: 0.0000000016 time: 0.17464041709899902\n",
      "Iteration: 9880 loss: 0.0000000020 time: 0.17281675338745117\n",
      "Iteration: 9890 loss: 0.0000000016 time: 0.16816353797912598\n",
      "Iteration: 9900 loss: 0.0000000015 time: 0.16902565956115723\n",
      "Iteration: 9910 loss: 0.0000000015 time: 0.17909622192382812\n",
      "Iteration: 9920 loss: 0.0000000015 time: 0.16995787620544434\n",
      "Iteration: 9930 loss: 0.0000000015 time: 0.17200398445129395\n",
      "Iteration: 9940 loss: 0.0000000014 time: 0.1723346710205078\n",
      "Iteration: 9950 loss: 0.0000000014 time: 0.17216110229492188\n",
      "Iteration: 9960 loss: 0.0000000014 time: 0.17602872848510742\n",
      "Iteration: 9970 loss: 0.0000000016 time: 0.18103742599487305\n",
      "Iteration: 9980 loss: 0.0000000218 time: 0.17642831802368164\n",
      "Iteration: 9990 loss: 0.0000012720 time: 0.17238879203796387\n",
      "Iteration: 10000 loss: 0.0000000027 time: 0.16989731788635254\n",
      "Iteration: 10010 loss: 0.0000000372 time: 0.1691441535949707\n",
      "Iteration: 10020 loss: 0.0000000289 time: 0.17031359672546387\n",
      "Iteration: 10030 loss: 0.0000000160 time: 0.1784074306488037\n",
      "Iteration: 10040 loss: 0.0000000070 time: 0.17382526397705078\n",
      "Iteration: 10050 loss: 0.0000000024 time: 0.17656874656677246\n",
      "Iteration: 10060 loss: 0.0000000014 time: 0.17286944389343262\n",
      "Iteration: 10070 loss: 0.0000000016 time: 0.17423152923583984\n",
      "Iteration: 10080 loss: 0.0000000014 time: 0.1647815704345703\n",
      "Iteration: 10090 loss: 0.0000000014 time: 0.17729759216308594\n",
      "Iteration: 10100 loss: 0.0000000022 time: 0.17458438873291016\n",
      "Iteration: 10110 loss: 0.0000000531 time: 0.1787102222442627\n",
      "Iteration: 10120 loss: 0.0000016487 time: 0.17326092720031738\n",
      "Iteration: 10130 loss: 0.0000000830 time: 0.17193102836608887\n",
      "Iteration: 10140 loss: 0.0000001738 time: 0.17470598220825195\n",
      "Iteration: 10150 loss: 0.0000000147 time: 0.17674040794372559\n",
      "Iteration: 10160 loss: 0.0000000101 time: 0.1809377670288086\n",
      "Iteration: 10170 loss: 0.0000000081 time: 0.17546558380126953\n",
      "Iteration: 10180 loss: 0.0000000017 time: 0.1654829978942871\n",
      "Iteration: 10190 loss: 0.0000000022 time: 0.16813278198242188\n",
      "Iteration: 10200 loss: 0.0000000016 time: 0.1725153923034668\n",
      "Iteration: 10210 loss: 0.0000000014 time: 0.17953252792358398\n",
      "Iteration: 10220 loss: 0.0000000014 time: 0.17176318168640137\n",
      "Iteration: 10230 loss: 0.0000000014 time: 0.16943693161010742\n",
      "Iteration: 10240 loss: 0.0000000013 time: 0.17286920547485352\n",
      "Iteration: 10250 loss: 0.0000000013 time: 0.17331242561340332\n",
      "Iteration: 10260 loss: 0.0000000013 time: 0.1804513931274414\n",
      "Iteration: 10270 loss: 0.0000000013 time: 0.17650246620178223\n",
      "Iteration: 10280 loss: 0.0000000013 time: 0.18315553665161133\n",
      "Iteration: 10290 loss: 0.0000000013 time: 0.17644500732421875\n",
      "Iteration: 10300 loss: 0.0000000013 time: 0.17112278938293457\n",
      "Iteration: 10310 loss: 0.0000000013 time: 0.17296195030212402\n",
      "Iteration: 10320 loss: 0.0000000013 time: 0.17628026008605957\n",
      "Iteration: 10330 loss: 0.0000000013 time: 0.18001556396484375\n",
      "Iteration: 10340 loss: 0.0000000089 time: 0.17925095558166504\n",
      "Iteration: 10350 loss: 0.0000009043 time: 0.17705154418945312\n",
      "Iteration: 10360 loss: 0.0000008754 time: 0.17461276054382324\n",
      "Iteration: 10370 loss: 0.0000000706 time: 0.17098307609558105\n",
      "Iteration: 10380 loss: 0.0000000016 time: 0.16872859001159668\n",
      "Iteration: 10390 loss: 0.0000000098 time: 0.18155121803283691\n",
      "Iteration: 10400 loss: 0.0000000123 time: 0.17915654182434082\n",
      "Iteration: 10410 loss: 0.0000000062 time: 0.174605131149292\n",
      "Iteration: 10420 loss: 0.0000000016 time: 0.1766645908355713\n",
      "Iteration: 10430 loss: 0.0000000015 time: 0.17096710205078125\n",
      "Iteration: 10440 loss: 0.0000000015 time: 0.16408634185791016\n",
      "Iteration: 10450 loss: 0.0000000023 time: 0.16373729705810547\n",
      "Iteration: 10460 loss: 0.0000000907 time: 0.18349409103393555\n",
      "Iteration: 10470 loss: 0.0000003138 time: 0.16768550872802734\n",
      "Iteration: 10480 loss: 0.0000001484 time: 0.17163801193237305\n",
      "Iteration: 10490 loss: 0.0000000310 time: 0.17717289924621582\n",
      "Iteration: 10500 loss: 0.0000000095 time: 0.17294836044311523\n",
      "Iteration: 10510 loss: 0.0000000103 time: 0.1765918731689453\n",
      "Iteration: 10520 loss: 0.0000000024 time: 0.17554402351379395\n",
      "Iteration: 10530 loss: 0.0000000020 time: 0.17168354988098145\n",
      "Iteration: 10540 loss: 0.0000000016 time: 0.1722414493560791\n",
      "Iteration: 10550 loss: 0.0000000012 time: 0.17610859870910645\n",
      "Iteration: 10560 loss: 0.0000000012 time: 0.17260265350341797\n",
      "Iteration: 10570 loss: 0.0000000012 time: 0.16794657707214355\n",
      "Iteration: 10580 loss: 0.0000000012 time: 0.1658475399017334\n",
      "Iteration: 10590 loss: 0.0000000012 time: 0.17808866500854492\n",
      "Iteration: 10600 loss: 0.0000000012 time: 0.17101335525512695\n",
      "Iteration: 10610 loss: 0.0000000012 time: 0.17822480201721191\n",
      "Iteration: 10620 loss: 0.0000000012 time: 0.18276238441467285\n",
      "Iteration: 10630 loss: 0.0000000011 time: 0.17127466201782227\n",
      "Iteration: 10640 loss: 0.0000000011 time: 0.18019771575927734\n",
      "Iteration: 10650 loss: 0.0000000012 time: 0.18363094329833984\n",
      "Iteration: 10660 loss: 0.0000000078 time: 0.1667633056640625\n",
      "Iteration: 10670 loss: 0.0000006851 time: 0.1728520393371582\n",
      "Iteration: 10680 loss: 0.0000005815 time: 0.17338776588439941\n",
      "Iteration: 10690 loss: 0.0000000028 time: 0.1679997444152832\n",
      "Iteration: 10700 loss: 0.0000000499 time: 0.1689586639404297\n",
      "Iteration: 10710 loss: 0.0000000380 time: 0.17443585395812988\n",
      "Iteration: 10720 loss: 0.0000000098 time: 0.17470169067382812\n",
      "Iteration: 10730 loss: 0.0000000012 time: 0.1745469570159912\n",
      "Iteration: 10740 loss: 0.0000000027 time: 0.1751265525817871\n",
      "Iteration: 10750 loss: 0.0000000014 time: 0.17649435997009277\n",
      "Iteration: 10760 loss: 0.0000000013 time: 0.1755082607269287\n",
      "Iteration: 10770 loss: 0.0000000012 time: 0.1726229190826416\n",
      "Iteration: 10780 loss: 0.0000000012 time: 0.17651581764221191\n",
      "Iteration: 10790 loss: 0.0000000012 time: 0.1758124828338623\n",
      "Iteration: 10800 loss: 0.0000000016 time: 0.16913080215454102\n",
      "Iteration: 10810 loss: 0.0000000301 time: 0.17838811874389648\n",
      "Iteration: 10820 loss: 0.0000009997 time: 0.169142484664917\n",
      "Iteration: 10830 loss: 0.0000001235 time: 0.1768662929534912\n",
      "Iteration: 10840 loss: 0.0000000837 time: 0.17406392097473145\n",
      "Iteration: 10850 loss: 0.0000000117 time: 0.17117810249328613\n",
      "Iteration: 10860 loss: 0.0000000061 time: 0.17906546592712402\n",
      "Iteration: 10870 loss: 0.0000000061 time: 0.18289923667907715\n",
      "Iteration: 10880 loss: 0.0000000017 time: 0.18395113945007324\n",
      "Iteration: 10890 loss: 0.0000000018 time: 0.18044471740722656\n",
      "Iteration: 10900 loss: 0.0000000011 time: 0.17931079864501953\n",
      "Iteration: 10910 loss: 0.0000000011 time: 0.1775214672088623\n",
      "Iteration: 10920 loss: 0.0000000011 time: 0.1701810359954834\n",
      "Iteration: 10930 loss: 0.0000000011 time: 0.17034006118774414\n",
      "Iteration: 10940 loss: 0.0000000011 time: 0.1790022850036621\n",
      "Iteration: 10950 loss: 0.0000000010 time: 0.1806957721710205\n",
      "Iteration: 10960 loss: 0.0000000011 time: 0.17382001876831055\n",
      "Iteration: 10970 loss: 0.0000000038 time: 0.17972660064697266\n",
      "Iteration: 10980 loss: 0.0000002711 time: 0.16764187812805176\n",
      "Iteration: 10990 loss: 0.0000000097 time: 0.1657857894897461\n",
      "Iteration: 11000 loss: 0.0000003431 time: 0.1788628101348877\n",
      "Iteration: 11010 loss: 0.0000001037 time: 0.18019533157348633\n",
      "Iteration: 11020 loss: 0.0000000070 time: 0.1734025478363037\n",
      "Iteration: 11030 loss: 0.0000000047 time: 0.18171477317810059\n",
      "Iteration: 11040 loss: 0.0000000070 time: 0.17098593711853027\n",
      "Iteration: 11050 loss: 0.0000000016 time: 0.17939114570617676\n",
      "Iteration: 11060 loss: 0.0000000015 time: 0.16911864280700684\n",
      "Iteration: 11070 loss: 0.0000000012 time: 0.1663494110107422\n",
      "Iteration: 11080 loss: 0.0000000011 time: 0.17028474807739258\n",
      "Iteration: 11090 loss: 0.0000000010 time: 0.1815345287322998\n",
      "Iteration: 11100 loss: 0.0000000010 time: 0.1724684238433838\n",
      "Iteration: 11110 loss: 0.0000000010 time: 0.17171478271484375\n",
      "Iteration: 11120 loss: 0.0000000010 time: 0.17033839225769043\n",
      "Iteration: 11130 loss: 0.0000000010 time: 0.17532086372375488\n",
      "Iteration: 11140 loss: 0.0000000010 time: 0.16235899925231934\n",
      "Iteration: 11150 loss: 0.0000000010 time: 0.1703336238861084\n",
      "Iteration: 11160 loss: 0.0000000091 time: 0.16884541511535645\n",
      "Iteration: 11170 loss: 0.0000015822 time: 0.17064857482910156\n",
      "Iteration: 11180 loss: 0.0000000265 time: 0.17419958114624023\n",
      "Iteration: 11190 loss: 0.0000000548 time: 0.1766963005065918\n",
      "Iteration: 11200 loss: 0.0000000574 time: 0.1678149700164795\n",
      "Iteration: 11210 loss: 0.0000000225 time: 0.1668694019317627\n",
      "Iteration: 11220 loss: 0.0000000047 time: 0.16942477226257324\n",
      "Iteration: 11230 loss: 0.0000000012 time: 0.17841720581054688\n",
      "Iteration: 11240 loss: 0.0000000010 time: 0.17207789421081543\n",
      "Iteration: 11250 loss: 0.0000000010 time: 0.17699575424194336\n",
      "Iteration: 11260 loss: 0.0000000010 time: 0.17921781539916992\n",
      "Iteration: 11270 loss: 0.0000000010 time: 0.17760777473449707\n",
      "Iteration: 11280 loss: 0.0000000010 time: 0.17790579795837402\n",
      "Iteration: 11290 loss: 0.0000000015 time: 0.18393301963806152\n",
      "Iteration: 11300 loss: 0.0000000334 time: 0.17818951606750488\n",
      "Iteration: 11310 loss: 0.0000015673 time: 0.17318177223205566\n",
      "Iteration: 11320 loss: 0.0000002989 time: 0.16788840293884277\n",
      "Iteration: 11330 loss: 0.0000001548 time: 0.17165541648864746\n",
      "Iteration: 11340 loss: 0.0000000022 time: 0.17449426651000977\n",
      "Iteration: 11350 loss: 0.0000000170 time: 0.1763749122619629\n",
      "Iteration: 11360 loss: 0.0000000065 time: 0.17436885833740234\n",
      "Iteration: 11370 loss: 0.0000000017 time: 0.16875457763671875\n",
      "Iteration: 11380 loss: 0.0000000018 time: 0.1785588264465332\n",
      "Iteration: 11390 loss: 0.0000000012 time: 0.17021393775939941\n",
      "Iteration: 11400 loss: 0.0000000010 time: 0.1741163730621338\n",
      "Iteration: 11410 loss: 0.0000000010 time: 0.17597341537475586\n",
      "Iteration: 11420 loss: 0.0000000010 time: 0.17326855659484863\n",
      "Iteration: 11430 loss: 0.0000000009 time: 0.1743943691253662\n",
      "Iteration: 11440 loss: 0.0000000009 time: 0.1700427532196045\n",
      "Iteration: 11450 loss: 0.0000000009 time: 0.17383837699890137\n",
      "Iteration: 11460 loss: 0.0000000009 time: 0.17283153533935547\n",
      "Iteration: 11470 loss: 0.0000000009 time: 0.1757051944732666\n",
      "Iteration: 11480 loss: 0.0000000009 time: 0.17488718032836914\n",
      "Iteration: 11490 loss: 0.0000000009 time: 0.1787118911743164\n",
      "Iteration: 11500 loss: 0.0000000009 time: 0.17432904243469238\n",
      "Iteration: 11510 loss: 0.0000000009 time: 0.17609763145446777\n",
      "Iteration: 11520 loss: 0.0000000009 time: 0.17416048049926758\n",
      "Iteration: 11530 loss: 0.0000000011 time: 0.17123770713806152\n",
      "Iteration: 11540 loss: 0.0000000268 time: 0.1672508716583252\n",
      "Iteration: 11550 loss: 0.0000022229 time: 0.17547202110290527\n",
      "Iteration: 11560 loss: 0.0000000210 time: 0.18559765815734863\n",
      "Iteration: 11570 loss: 0.0000000701 time: 0.1750779151916504\n",
      "Iteration: 11580 loss: 0.0000000425 time: 0.17272114753723145\n",
      "Iteration: 11590 loss: 0.0000000230 time: 0.1661994457244873\n",
      "Iteration: 11600 loss: 0.0000000115 time: 0.18109583854675293\n",
      "Iteration: 11610 loss: 0.0000000046 time: 0.16814899444580078\n",
      "Iteration: 11620 loss: 0.0000000014 time: 0.17744946479797363\n",
      "Iteration: 11630 loss: 0.0000000010 time: 0.1698768138885498\n",
      "Iteration: 11640 loss: 0.0000000011 time: 0.16625022888183594\n",
      "Iteration: 11650 loss: 0.0000000009 time: 0.18314814567565918\n",
      "Iteration: 11660 loss: 0.0000000009 time: 0.17840051651000977\n",
      "Iteration: 11670 loss: 0.0000000009 time: 0.17287969589233398\n",
      "Iteration: 11680 loss: 0.0000000009 time: 0.17129755020141602\n",
      "Iteration: 11690 loss: 0.0000000009 time: 0.17474102973937988\n",
      "Iteration: 11700 loss: 0.0000000009 time: 0.17381715774536133\n",
      "Iteration: 11710 loss: 0.0000000019 time: 0.16411995887756348\n",
      "Iteration: 11720 loss: 0.0000001659 time: 0.17831134796142578\n",
      "Iteration: 11730 loss: 0.0000000351 time: 0.18090534210205078\n",
      "Iteration: 11740 loss: 0.0000000553 time: 0.17216062545776367\n",
      "Iteration: 11750 loss: 0.0000000395 time: 0.17709803581237793\n",
      "Iteration: 11760 loss: 0.0000000199 time: 0.17910122871398926\n",
      "Iteration: 11770 loss: 0.0000000097 time: 0.1701052188873291\n",
      "Iteration: 11780 loss: 0.0000000043 time: 0.17901897430419922\n",
      "Iteration: 11790 loss: 0.0000000017 time: 0.1735689640045166\n",
      "Iteration: 11800 loss: 0.0000000009 time: 0.16837763786315918\n",
      "Iteration: 11810 loss: 0.0000000009 time: 0.17676305770874023\n",
      "Iteration: 11820 loss: 0.0000000009 time: 0.16051936149597168\n",
      "Iteration: 11830 loss: 0.0000000008 time: 0.17254185676574707\n",
      "Iteration: 11840 loss: 0.0000000008 time: 0.17637968063354492\n",
      "Iteration: 11850 loss: 0.0000000008 time: 0.1715548038482666\n",
      "Iteration: 11860 loss: 0.0000000008 time: 0.17116117477416992\n",
      "Iteration: 11870 loss: 0.0000000008 time: 0.18153882026672363\n",
      "Iteration: 11880 loss: 0.0000000012 time: 0.17158746719360352\n",
      "Iteration: 11890 loss: 0.0000000538 time: 0.1798110008239746\n",
      "Iteration: 11900 loss: 0.0000019349 time: 0.17744922637939453\n",
      "Iteration: 11910 loss: 0.0000001829 time: 0.17564773559570312\n",
      "Iteration: 11920 loss: 0.0000000082 time: 0.1755049228668213\n",
      "Iteration: 11930 loss: 0.0000000019 time: 0.17874479293823242\n",
      "Iteration: 11940 loss: 0.0000000053 time: 0.1746814250946045\n",
      "Iteration: 11950 loss: 0.0000000063 time: 0.17239618301391602\n",
      "Iteration: 11960 loss: 0.0000000041 time: 0.1731281280517578\n",
      "Iteration: 11970 loss: 0.0000000016 time: 0.17295622825622559\n",
      "Iteration: 11980 loss: 0.0000000009 time: 0.17356371879577637\n",
      "Iteration: 11990 loss: 0.0000000010 time: 0.16800141334533691\n",
      "Iteration: 12000 loss: 0.0000000008 time: 0.17595553398132324\n",
      "Iteration: 12010 loss: 0.0000000008 time: 0.16947627067565918\n",
      "Iteration: 12020 loss: 0.0000000008 time: 0.17649197578430176\n",
      "Iteration: 12030 loss: 0.0000000008 time: 0.17731118202209473\n",
      "Iteration: 12040 loss: 0.0000000008 time: 0.17647743225097656\n",
      "Iteration: 12050 loss: 0.0000000008 time: 0.1746838092803955\n",
      "Iteration: 12060 loss: 0.0000000008 time: 0.17766857147216797\n",
      "Iteration: 12070 loss: 0.0000000008 time: 0.1723775863647461\n",
      "Iteration: 12080 loss: 0.0000000008 time: 0.17887663841247559\n",
      "Iteration: 12090 loss: 0.0000000008 time: 0.17671465873718262\n",
      "Iteration: 12100 loss: 0.0000000007 time: 0.1759648323059082\n",
      "Iteration: 12110 loss: 0.0000000007 time: 0.1761622428894043\n",
      "Iteration: 12120 loss: 0.0000000019 time: 0.1763145923614502\n",
      "Iteration: 12130 loss: 0.0000002258 time: 0.1781923770904541\n",
      "Iteration: 12140 loss: 0.0000002438 time: 0.1692349910736084\n",
      "Iteration: 12150 loss: 0.0000000075 time: 0.17395544052124023\n",
      "Iteration: 12160 loss: 0.0000000028 time: 0.17221999168395996\n",
      "Iteration: 12170 loss: 0.0000000022 time: 0.17215394973754883\n",
      "Iteration: 12180 loss: 0.0000000013 time: 0.16655850410461426\n",
      "Iteration: 12190 loss: 0.0000000008 time: 0.17188549041748047\n",
      "Iteration: 12200 loss: 0.0000000008 time: 0.17662453651428223\n",
      "Iteration: 12210 loss: 0.0000000011 time: 0.17470908164978027\n",
      "Iteration: 12220 loss: 0.0000000071 time: 0.17679810523986816\n",
      "Iteration: 12230 loss: 0.0000003063 time: 0.17885708808898926\n",
      "Iteration: 12240 loss: 0.0000000106 time: 0.1695082187652588\n",
      "Iteration: 12250 loss: 0.0000001977 time: 0.18236041069030762\n",
      "Iteration: 12260 loss: 0.0000000177 time: 0.17326998710632324\n",
      "Iteration: 12270 loss: 0.0000000236 time: 0.16835784912109375\n",
      "Iteration: 12280 loss: 0.0000000057 time: 0.17042803764343262\n",
      "Iteration: 12290 loss: 0.0000000015 time: 0.1751995086669922\n",
      "Iteration: 12300 loss: 0.0000000022 time: 0.17818999290466309\n",
      "Iteration: 12310 loss: 0.0000000012 time: 0.1780247688293457\n",
      "Iteration: 12320 loss: 0.0000000008 time: 0.17484164237976074\n",
      "Iteration: 12330 loss: 0.0000000007 time: 0.17446208000183105\n",
      "Iteration: 12340 loss: 0.0000000007 time: 0.1805577278137207\n",
      "Iteration: 12350 loss: 0.0000000007 time: 0.1749420166015625\n",
      "Iteration: 12360 loss: 0.0000000007 time: 0.1798098087310791\n",
      "Iteration: 12370 loss: 0.0000000007 time: 0.17396783828735352\n",
      "Iteration: 12380 loss: 0.0000000007 time: 0.17745161056518555\n",
      "Iteration: 12390 loss: 0.0000000007 time: 0.17933344841003418\n",
      "Iteration: 12400 loss: 0.0000000007 time: 0.17735695838928223\n",
      "Iteration: 12410 loss: 0.0000000007 time: 0.17353057861328125\n",
      "Iteration: 12420 loss: 0.0000000040 time: 0.17812228202819824\n",
      "Iteration: 12430 loss: 0.0000002718 time: 0.1692957878112793\n",
      "Iteration: 12440 loss: 0.0000000086 time: 0.18101811408996582\n",
      "Iteration: 12450 loss: 0.0000003201 time: 0.179793119430542\n",
      "Iteration: 12460 loss: 0.0000000539 time: 0.17572879791259766\n",
      "Iteration: 12470 loss: 0.0000000023 time: 0.16831302642822266\n",
      "Iteration: 12480 loss: 0.0000000137 time: 0.18036556243896484\n",
      "Iteration: 12490 loss: 0.0000000029 time: 0.17540693283081055\n",
      "Iteration: 12500 loss: 0.0000000015 time: 0.1703505516052246\n",
      "Iteration: 12510 loss: 0.0000000011 time: 0.17478370666503906\n",
      "Iteration: 12520 loss: 0.0000000009 time: 0.17120599746704102\n",
      "Iteration: 12530 loss: 0.0000000007 time: 0.17243218421936035\n",
      "Iteration: 12540 loss: 0.0000000007 time: 0.175429105758667\n",
      "Iteration: 12550 loss: 0.0000000007 time: 0.1784653663635254\n",
      "Iteration: 12560 loss: 0.0000000007 time: 0.17992734909057617\n",
      "Iteration: 12570 loss: 0.0000000007 time: 0.17331910133361816\n",
      "Iteration: 12580 loss: 0.0000000007 time: 0.17654109001159668\n",
      "Iteration: 12590 loss: 0.0000000014 time: 0.18084931373596191\n",
      "Iteration: 12600 loss: 0.0000000726 time: 0.17643237113952637\n",
      "Iteration: 12610 loss: 0.0000002898 time: 0.18095993995666504\n",
      "Iteration: 12620 loss: 0.0000001978 time: 0.17360329627990723\n",
      "Iteration: 12630 loss: 0.0000000179 time: 0.17768359184265137\n",
      "Iteration: 12640 loss: 0.0000000013 time: 0.170166015625\n",
      "Iteration: 12650 loss: 0.0000000063 time: 0.16817975044250488\n",
      "Iteration: 12660 loss: 0.0000000041 time: 0.1711287498474121\n",
      "Iteration: 12670 loss: 0.0000000009 time: 0.17699480056762695\n",
      "Iteration: 12680 loss: 0.0000000008 time: 0.1751093864440918\n",
      "Iteration: 12690 loss: 0.0000000008 time: 0.16899657249450684\n",
      "Iteration: 12700 loss: 0.0000000007 time: 0.1722722053527832\n",
      "Iteration: 12710 loss: 0.0000000033 time: 0.17940640449523926\n",
      "Iteration: 12720 loss: 0.0000002305 time: 0.1817793846130371\n",
      "Iteration: 12730 loss: 0.0000000480 time: 0.1771068572998047\n",
      "Iteration: 12740 loss: 0.0000003403 time: 0.17578887939453125\n",
      "Iteration: 12750 loss: 0.0000000500 time: 0.1799333095550537\n",
      "Iteration: 12760 loss: 0.0000000023 time: 0.17702627182006836\n",
      "Iteration: 12770 loss: 0.0000000132 time: 0.17328333854675293\n",
      "Iteration: 12780 loss: 0.0000000038 time: 0.17343640327453613\n",
      "Iteration: 12790 loss: 0.0000000011 time: 0.1683804988861084\n",
      "Iteration: 12800 loss: 0.0000000013 time: 0.16829586029052734\n",
      "Iteration: 12810 loss: 0.0000000007 time: 0.17817115783691406\n",
      "Iteration: 12820 loss: 0.0000000007 time: 0.1723315715789795\n",
      "Iteration: 12830 loss: 0.0000000007 time: 0.17609119415283203\n",
      "Iteration: 12840 loss: 0.0000000006 time: 0.1695852279663086\n",
      "Iteration: 12850 loss: 0.0000000006 time: 0.1812610626220703\n",
      "Iteration: 12860 loss: 0.0000000006 time: 0.17246246337890625\n",
      "Iteration: 12870 loss: 0.0000000006 time: 0.17868423461914062\n",
      "Iteration: 12880 loss: 0.0000000006 time: 0.17133021354675293\n",
      "Iteration: 12890 loss: 0.0000000006 time: 0.1764850616455078\n",
      "Iteration: 12900 loss: 0.0000000006 time: 0.1746370792388916\n",
      "Iteration: 12910 loss: 0.0000000006 time: 0.17608428001403809\n",
      "Iteration: 12920 loss: 0.0000000006 time: 0.16708660125732422\n",
      "Iteration: 12930 loss: 0.0000000006 time: 0.1716325283050537\n",
      "Iteration: 12940 loss: 0.0000000006 time: 0.17823505401611328\n",
      "Iteration: 12950 loss: 0.0000000006 time: 0.1726515293121338\n",
      "Iteration: 12960 loss: 0.0000000012 time: 0.17811799049377441\n",
      "Iteration: 12970 loss: 0.0000001623 time: 0.18413090705871582\n",
      "Iteration: 12980 loss: 0.0000003442 time: 0.18070292472839355\n",
      "Iteration: 12990 loss: 0.0000000595 time: 0.1724226474761963\n",
      "Iteration: 13000 loss: 0.0000000950 time: 0.1811513900756836\n",
      "Iteration: 13010 loss: 0.0000000437 time: 0.1789851188659668\n",
      "Iteration: 13020 loss: 0.0000000091 time: 0.17679762840270996\n",
      "Iteration: 13030 loss: 0.0000000037 time: 0.1720120906829834\n",
      "Iteration: 13040 loss: 0.0000000016 time: 0.17867136001586914\n",
      "Iteration: 13050 loss: 0.0000000017 time: 0.16994810104370117\n",
      "Iteration: 13060 loss: 0.0000000009 time: 0.17314410209655762\n",
      "Iteration: 13070 loss: 0.0000000006 time: 0.17813706398010254\n",
      "Iteration: 13080 loss: 0.0000000006 time: 0.18033647537231445\n",
      "Iteration: 13090 loss: 0.0000000008 time: 0.18117475509643555\n",
      "Iteration: 13100 loss: 0.0000000185 time: 0.18210506439208984\n",
      "Iteration: 13110 loss: 0.0000013193 time: 0.16857624053955078\n",
      "Iteration: 13120 loss: 0.0000005282 time: 0.17008757591247559\n",
      "Iteration: 13130 loss: 0.0000001409 time: 0.1765289306640625\n",
      "Iteration: 13140 loss: 0.0000000024 time: 0.1770467758178711\n",
      "Iteration: 13150 loss: 0.0000000120 time: 0.1644885540008545\n",
      "Iteration: 13160 loss: 0.0000000099 time: 0.1734757423400879\n",
      "Iteration: 13170 loss: 0.0000000009 time: 0.1727001667022705\n",
      "Iteration: 13180 loss: 0.0000000016 time: 0.1682910919189453\n",
      "Iteration: 13190 loss: 0.0000000007 time: 0.17363643646240234\n",
      "Iteration: 13200 loss: 0.0000000007 time: 0.17585420608520508\n",
      "Iteration: 13210 loss: 0.0000000006 time: 0.16578125953674316\n",
      "Iteration: 13220 loss: 0.0000000006 time: 0.17160773277282715\n",
      "Iteration: 13230 loss: 0.0000000006 time: 0.1761937141418457\n",
      "Iteration: 13240 loss: 0.0000000006 time: 0.16710710525512695\n",
      "Iteration: 13250 loss: 0.0000000006 time: 0.1699075698852539\n",
      "Iteration: 13260 loss: 0.0000000005 time: 0.17674899101257324\n",
      "Iteration: 13270 loss: 0.0000000005 time: 0.16522741317749023\n",
      "Iteration: 13280 loss: 0.0000000005 time: 0.17605829238891602\n",
      "Iteration: 13290 loss: 0.0000000005 time: 0.17843389511108398\n",
      "Iteration: 13300 loss: 0.0000000005 time: 0.16803407669067383\n",
      "Iteration: 13310 loss: 0.0000000005 time: 0.17279410362243652\n",
      "Iteration: 13320 loss: 0.0000000005 time: 0.17180442810058594\n",
      "Iteration: 13330 loss: 0.0000000005 time: 0.16880369186401367\n",
      "Iteration: 13340 loss: 0.0000000006 time: 0.18326783180236816\n",
      "Iteration: 13350 loss: 0.0000000066 time: 0.17669439315795898\n",
      "Iteration: 13360 loss: 0.0000008455 time: 0.1748046875\n",
      "Iteration: 13370 loss: 0.0000008887 time: 0.17324113845825195\n",
      "Iteration: 13380 loss: 0.0000001470 time: 0.17693614959716797\n",
      "Iteration: 13390 loss: 0.0000000229 time: 0.1739814281463623\n",
      "Iteration: 13400 loss: 0.0000000020 time: 0.16860413551330566\n",
      "Iteration: 13410 loss: 0.0000000013 time: 0.1760554313659668\n",
      "Iteration: 13420 loss: 0.0000000029 time: 0.1745617389678955\n",
      "Iteration: 13430 loss: 0.0000000023 time: 0.17943048477172852\n",
      "Iteration: 13440 loss: 0.0000000009 time: 0.1748652458190918\n",
      "Iteration: 13450 loss: 0.0000000006 time: 0.17466497421264648\n",
      "Iteration: 13460 loss: 0.0000000006 time: 0.17464900016784668\n",
      "Iteration: 13470 loss: 0.0000000006 time: 0.17566156387329102\n",
      "Iteration: 13480 loss: 0.0000000006 time: 0.1687755584716797\n",
      "Iteration: 13490 loss: 0.0000000005 time: 0.1845102310180664\n",
      "Iteration: 13500 loss: 0.0000000005 time: 0.17976737022399902\n",
      "Iteration: 13510 loss: 0.0000000005 time: 0.17737579345703125\n",
      "Iteration: 13520 loss: 0.0000000005 time: 0.17586922645568848\n",
      "Iteration: 13530 loss: 0.0000000007 time: 0.18008923530578613\n",
      "Iteration: 13540 loss: 0.0000000160 time: 0.18100953102111816\n",
      "Iteration: 13550 loss: 0.0000010322 time: 0.1765131950378418\n",
      "Iteration: 13560 loss: 0.0000000225 time: 0.1739025115966797\n",
      "Iteration: 13570 loss: 0.0000000601 time: 0.17897748947143555\n",
      "Iteration: 13580 loss: 0.0000000352 time: 0.1706697940826416\n",
      "Iteration: 13590 loss: 0.0000000138 time: 0.1719818115234375\n",
      "Iteration: 13600 loss: 0.0000000032 time: 0.1634676456451416\n",
      "Iteration: 13610 loss: 0.0000000006 time: 0.17250943183898926\n",
      "Iteration: 13620 loss: 0.0000000008 time: 0.17541241645812988\n",
      "Iteration: 13630 loss: 0.0000000007 time: 0.17728161811828613\n",
      "Iteration: 13640 loss: 0.0000000005 time: 0.17876458168029785\n",
      "Iteration: 13650 loss: 0.0000000005 time: 0.17506933212280273\n",
      "Iteration: 13660 loss: 0.0000000005 time: 0.18108868598937988\n",
      "Iteration: 13670 loss: 0.0000000006 time: 0.18061542510986328\n",
      "Iteration: 13680 loss: 0.0000000049 time: 0.17507243156433105\n",
      "Iteration: 13690 loss: 0.0000003104 time: 0.18116116523742676\n",
      "Iteration: 13700 loss: 0.0000000024 time: 0.17101240158081055\n",
      "Iteration: 13710 loss: 0.0000002724 time: 0.18056273460388184\n",
      "Iteration: 13720 loss: 0.0000000227 time: 0.17892122268676758\n",
      "Iteration: 13730 loss: 0.0000000139 time: 0.17308807373046875\n",
      "Iteration: 13740 loss: 0.0000000109 time: 0.16669273376464844\n",
      "Iteration: 13750 loss: 0.0000000010 time: 0.18182039260864258\n",
      "Iteration: 13760 loss: 0.0000000019 time: 0.16476655006408691\n",
      "Iteration: 13770 loss: 0.0000000007 time: 0.1783127784729004\n",
      "Iteration: 13780 loss: 0.0000000005 time: 0.17677712440490723\n",
      "Iteration: 13790 loss: 0.0000000006 time: 0.17563343048095703\n",
      "Iteration: 13800 loss: 0.0000000005 time: 0.17893767356872559\n",
      "Iteration: 13810 loss: 0.0000000005 time: 0.1725177764892578\n",
      "Iteration: 13820 loss: 0.0000000005 time: 0.1766805648803711\n",
      "Iteration: 13830 loss: 0.0000000005 time: 0.17475318908691406\n",
      "Iteration: 13840 loss: 0.0000000005 time: 0.17041945457458496\n",
      "Iteration: 13850 loss: 0.0000000004 time: 0.17893362045288086\n",
      "Iteration: 13860 loss: 0.0000000004 time: 0.18405866622924805\n",
      "Iteration: 13870 loss: 0.0000000004 time: 0.16411542892456055\n",
      "Iteration: 13880 loss: 0.0000000004 time: 0.1668550968170166\n",
      "Iteration: 13890 loss: 0.0000000004 time: 0.17403078079223633\n",
      "Iteration: 13900 loss: 0.0000000008 time: 0.16890192031860352\n",
      "Iteration: 13910 loss: 0.0000000521 time: 0.1726527214050293\n",
      "Iteration: 13920 loss: 0.0000004988 time: 0.17031407356262207\n",
      "Iteration: 13930 loss: 0.0000004335 time: 0.17701292037963867\n",
      "Iteration: 13940 loss: 0.0000000945 time: 0.1765286922454834\n",
      "Iteration: 13950 loss: 0.0000000606 time: 0.18007206916809082\n",
      "Iteration: 13960 loss: 0.0000000126 time: 0.17434120178222656\n",
      "Iteration: 13970 loss: 0.0000000106 time: 0.16739654541015625\n",
      "Iteration: 13980 loss: 0.0000000032 time: 0.1775376796722412\n",
      "Iteration: 13990 loss: 0.0000000030 time: 0.17449617385864258\n",
      "Iteration: 14000 loss: 0.0000000248 time: 0.17444443702697754\n",
      "Iteration: 14010 loss: 0.0000004515 time: 0.17028355598449707\n",
      "Iteration: 14020 loss: 0.0000000265 time: 0.17981815338134766\n",
      "Iteration: 14030 loss: 0.0000000441 time: 0.1735692024230957\n",
      "Iteration: 14040 loss: 0.0000000334 time: 0.16866517066955566\n",
      "Iteration: 14050 loss: 0.0000000152 time: 0.1702132225036621\n",
      "Iteration: 14060 loss: 0.0000000062 time: 0.16984987258911133\n",
      "Iteration: 14070 loss: 0.0000000025 time: 0.18004846572875977\n",
      "Iteration: 14080 loss: 0.0000000008 time: 0.1719989776611328\n",
      "Iteration: 14090 loss: 0.0000000005 time: 0.17829561233520508\n",
      "Iteration: 14100 loss: 0.0000000006 time: 0.18295931816101074\n",
      "Iteration: 14110 loss: 0.0000000005 time: 0.17157673835754395\n",
      "Iteration: 14120 loss: 0.0000000004 time: 0.17693376541137695\n",
      "Iteration: 14130 loss: 0.0000000005 time: 0.17582130432128906\n",
      "Iteration: 14140 loss: 0.0000000022 time: 0.18055486679077148\n",
      "Iteration: 14150 loss: 0.0000000958 time: 0.17596673965454102\n",
      "Iteration: 14160 loss: 0.0000008973 time: 0.1833639144897461\n",
      "Iteration: 14170 loss: 0.0000000058 time: 0.177656888961792\n",
      "Iteration: 14180 loss: 0.0000001075 time: 0.17592763900756836\n",
      "Iteration: 14190 loss: 0.0000000109 time: 0.18265795707702637\n",
      "Iteration: 14200 loss: 0.0000000103 time: 0.186614990234375\n",
      "Iteration: 14210 loss: 0.0000000021 time: 0.17657947540283203\n",
      "Iteration: 14220 loss: 0.0000000023 time: 0.17945241928100586\n",
      "Iteration: 14230 loss: 0.0000000005 time: 0.16958856582641602\n",
      "Iteration: 14240 loss: 0.0000000006 time: 0.17701435089111328\n",
      "Iteration: 14250 loss: 0.0000000005 time: 0.16856026649475098\n",
      "Iteration: 14260 loss: 0.0000000005 time: 0.17507624626159668\n",
      "Iteration: 14270 loss: 0.0000000004 time: 0.17606306076049805\n",
      "Iteration: 14280 loss: 0.0000000004 time: 0.1744232177734375\n",
      "Iteration: 14290 loss: 0.0000000004 time: 0.1822352409362793\n",
      "Iteration: 14300 loss: 0.0000000004 time: 0.1767444610595703\n",
      "Iteration: 14310 loss: 0.0000000004 time: 0.17818713188171387\n",
      "Iteration: 14320 loss: 0.0000000004 time: 0.17229080200195312\n",
      "Iteration: 14330 loss: 0.0000000004 time: 0.17543530464172363\n",
      "Iteration: 14340 loss: 0.0000000007 time: 0.16974949836730957\n",
      "Iteration: 14350 loss: 0.0000000357 time: 0.1798257827758789\n",
      "Iteration: 14360 loss: 0.0000007607 time: 0.1701960563659668\n",
      "Iteration: 14370 loss: 0.0000001702 time: 0.17241907119750977\n",
      "Iteration: 14380 loss: 0.0000000223 time: 0.17290210723876953\n",
      "Iteration: 14390 loss: 0.0000000079 time: 0.17721939086914062\n",
      "Iteration: 14400 loss: 0.0000000184 time: 0.1759629249572754\n",
      "Iteration: 14410 loss: 0.0000000258 time: 0.18096065521240234\n",
      "Iteration: 14420 loss: 0.0000000953 time: 0.17160487174987793\n",
      "Iteration: 14430 loss: 0.0000002405 time: 0.17728614807128906\n",
      "Iteration: 14440 loss: 0.0000000172 time: 0.17654085159301758\n",
      "Iteration: 14450 loss: 0.0000000123 time: 0.18422579765319824\n",
      "Iteration: 14460 loss: 0.0000000136 time: 0.17619991302490234\n",
      "Iteration: 14470 loss: 0.0000000037 time: 0.1716296672821045\n",
      "Iteration: 14480 loss: 0.0000000004 time: 0.17088818550109863\n",
      "Iteration: 14490 loss: 0.0000000015 time: 0.16903185844421387\n",
      "Iteration: 14500 loss: 0.0000000175 time: 0.17900896072387695\n",
      "Iteration: 14510 loss: 0.0000004354 time: 0.17008495330810547\n",
      "Iteration: 14520 loss: 0.0000000253 time: 0.17445015907287598\n",
      "Iteration: 14530 loss: 0.0000000873 time: 0.17156004905700684\n",
      "Iteration: 14540 loss: 0.0000000483 time: 0.17284417152404785\n",
      "Iteration: 14550 loss: 0.0000000084 time: 0.17627477645874023\n",
      "Iteration: 14560 loss: 0.0000000008 time: 0.17972564697265625\n",
      "Iteration: 14570 loss: 0.0000000004 time: 0.17184805870056152\n",
      "Iteration: 14580 loss: 0.0000000004 time: 0.16968488693237305\n",
      "Iteration: 14590 loss: 0.0000000005 time: 0.17342281341552734\n",
      "Iteration: 14600 loss: 0.0000000005 time: 0.17908787727355957\n",
      "Iteration: 14610 loss: 0.0000000004 time: 0.18527626991271973\n",
      "Iteration: 14620 loss: 0.0000000004 time: 0.17244625091552734\n",
      "Iteration: 14630 loss: 0.0000000004 time: 0.17946767807006836\n",
      "Iteration: 14640 loss: 0.0000000004 time: 0.17567110061645508\n",
      "Iteration: 14650 loss: 0.0000000004 time: 0.16406869888305664\n",
      "Iteration: 14660 loss: 0.0000000010 time: 0.17202138900756836\n",
      "Iteration: 14670 loss: 0.0000000528 time: 0.17971181869506836\n",
      "Iteration: 14680 loss: 0.0000015225 time: 0.18684911727905273\n",
      "Iteration: 14690 loss: 0.0000000040 time: 0.1823265552520752\n",
      "Iteration: 14700 loss: 0.0000000859 time: 0.18159103393554688\n",
      "Iteration: 14710 loss: 0.0000000569 time: 0.18576693534851074\n",
      "Iteration: 14720 loss: 0.0000000086 time: 0.17389535903930664\n",
      "Iteration: 14730 loss: 0.0000000012 time: 0.17678236961364746\n",
      "Iteration: 14740 loss: 0.0000000031 time: 0.1861438751220703\n",
      "Iteration: 14750 loss: 0.0000000005 time: 0.17086124420166016\n",
      "Iteration: 14760 loss: 0.0000000007 time: 0.16675353050231934\n",
      "Iteration: 14770 loss: 0.0000000004 time: 0.18247199058532715\n",
      "Iteration: 14780 loss: 0.0000000004 time: 0.17597484588623047\n",
      "Iteration: 14790 loss: 0.0000000004 time: 0.18151569366455078\n",
      "Iteration: 14800 loss: 0.0000000010 time: 0.17404985427856445\n",
      "Iteration: 14810 loss: 0.0000000468 time: 0.16979455947875977\n",
      "Iteration: 14820 loss: 0.0000004847 time: 0.17482209205627441\n",
      "Iteration: 14830 loss: 0.0000000312 time: 0.17163777351379395\n",
      "Iteration: 14840 loss: 0.0000000284 time: 0.1754157543182373\n",
      "Iteration: 14850 loss: 0.0000000227 time: 0.16424894332885742\n",
      "Iteration: 14860 loss: 0.0000000009 time: 0.17850637435913086\n",
      "Iteration: 14870 loss: 0.0000000026 time: 0.1697711944580078\n",
      "Iteration: 14880 loss: 0.0000000007 time: 0.1798245906829834\n",
      "Iteration: 14890 loss: 0.0000000006 time: 0.1751718521118164\n",
      "Iteration: 14900 loss: 0.0000000004 time: 0.17583751678466797\n",
      "Iteration: 14910 loss: 0.0000000004 time: 0.1688833236694336\n",
      "Iteration: 14920 loss: 0.0000000003 time: 0.1829543113708496\n",
      "Iteration: 14930 loss: 0.0000000003 time: 0.18506360054016113\n",
      "Iteration: 14940 loss: 0.0000000003 time: 0.17513108253479004\n",
      "Iteration: 14950 loss: 0.0000000008 time: 0.1718752384185791\n",
      "Iteration: 14960 loss: 0.0000000329 time: 0.175062894821167\n",
      "Iteration: 14970 loss: 0.0000015180 time: 0.17718029022216797\n",
      "Iteration: 14980 loss: 0.0000001532 time: 0.16769933700561523\n",
      "Iteration: 14990 loss: 0.0000001685 time: 0.18089818954467773\n",
      "Iteration: 15000 loss: 0.0000000202 time: 0.17765021324157715\n",
      "-->mesh : \n",
      "     n_triangles :  256\n",
      "     n_vertices  :  145\n",
      "     n_edges     :  400\n",
      "     h_max           :  0.12500000000033273\n",
      "     h_min           :  0.0883883476480272\n",
      "-->test_fun      : \n",
      "     order       :  1\n",
      "     dof         :  113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 12:19:14.619365: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_2/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_2/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:19:14.621438: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_3/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_3/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:19:14.624480: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:19:14.627417: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_1/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_1/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:19:14.628994: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_6/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_6/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:19:14.631516: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_7/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_7/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:19:14.633070: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_8/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_8/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:19:14.634703: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_9/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_9/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:19:14.636631: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_4/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_4/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:19:14.638285: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/gradient_tape/strided_slice_5/StridedSliceGrad_grad/StridedSlice/gradient_tape/strided_slice_5/StridedSliceGrad/strides}}]]\n",
      "2023-12-26 12:19:18.438724: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_26' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_26}}]]\n",
      "2023-12-26 12:19:18.439045: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_41' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_41}}]]\n",
      "2023-12-26 12:19:18.439159: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_58' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_58}}]]\n",
      "2023-12-26 12:19:18.439222: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_74' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_74}}]]\n",
      "2023-12-26 12:19:18.439292: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_90' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_90}}]]\n",
      "2023-12-26 12:19:18.439357: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_106' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_106}}]]\n",
      "2023-12-26 12:19:18.439460: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_122' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_122}}]]\n",
      "2023-12-26 12:19:18.439561: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_138' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_138}}]]\n",
      "2023-12-26 12:19:18.439663: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_154' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_154}}]]\n",
      "2023-12-26 12:19:18.439728: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_170' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_170}}]]\n",
      "2023-12-26 12:19:18.439827: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_186' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_186}}]]\n",
      "2023-12-26 12:19:18.439927: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_202' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_202}}]]\n",
      "2023-12-26 12:19:18.439987: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_218' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_218}}]]\n",
      "2023-12-26 12:19:18.440081: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_234' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_234}}]]\n",
      "2023-12-26 12:19:18.440175: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_250' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_250}}]]\n",
      "2023-12-26 12:19:18.440235: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_266' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_266}}]]\n",
      "2023-12-26 12:19:18.440330: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_282' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_282}}]]\n",
      "2023-12-26 12:19:18.440428: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_298' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_298}}]]\n",
      "2023-12-26 12:19:18.440492: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_314' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_314}}]]\n",
      "2023-12-26 12:19:18.440589: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_330' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_330}}]]\n",
      "2023-12-26 12:19:18.440683: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_350' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_350}}]]\n",
      "2023-12-26 12:19:18.440778: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_379' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_379}}]]\n",
      "2023-12-26 12:19:18.440877: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_397' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_397}}]]\n",
      "2023-12-26 12:19:18.440979: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_415' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_415}}]]\n",
      "2023-12-26 12:19:18.441076: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_420' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_420}}]]\n",
      "2023-12-26 12:19:18.441173: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_436' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_436}}]]\n",
      "2023-12-26 12:19:18.441272: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_454' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_454}}]]\n",
      "2023-12-26 12:19:18.441370: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_482' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_482}}]]\n",
      "2023-12-26 12:19:18.441470: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_490' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_490}}]]\n",
      "2023-12-26 12:19:18.441570: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_519' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_519}}]]\n",
      "2023-12-26 12:19:18.441681: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_537' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_537}}]]\n",
      "2023-12-26 12:19:18.441781: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_555' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_555}}]]\n",
      "2023-12-26 12:19:18.441882: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_560' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_560}}]]\n",
      "2023-12-26 12:19:18.441989: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_576' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_576}}]]\n",
      "2023-12-26 12:19:18.442180: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_592' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_592}}]]\n",
      "2023-12-26 12:19:18.442338: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_608' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_608}}]]\n",
      "2023-12-26 12:19:18.442548: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_624' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_624}}]]\n",
      "2023-12-26 12:19:18.442716: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_640' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_640}}]]\n",
      "2023-12-26 12:19:18.442880: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_656' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_656}}]]\n",
      "2023-12-26 12:19:18.443062: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_672' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_672}}]]\n",
      "2023-12-26 12:19:18.443216: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_688' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_688}}]]\n",
      "2023-12-26 12:19:18.443375: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_704' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_704}}]]\n",
      "2023-12-26 12:19:18.443568: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_720' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_720}}]]\n",
      "2023-12-26 12:19:18.443791: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_736' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_736}}]]\n",
      "2023-12-26 12:19:18.443996: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_752' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_752}}]]\n",
      "2023-12-26 12:19:18.444170: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_768' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_768}}]]\n",
      "2023-12-26 12:19:18.444410: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_786' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_786}}]]\n",
      "2023-12-26 12:19:18.444577: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_814' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_814}}]]\n",
      "2023-12-26 12:19:18.444710: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_818' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_818}}]]\n",
      "2023-12-26 12:19:18.444784: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_834' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_834}}]]\n",
      "2023-12-26 12:19:18.444896: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_850' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_850}}]]\n",
      "2023-12-26 12:19:18.444970: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_866' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_866}}]]\n",
      "2023-12-26 12:19:18.445081: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_886' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_886}}]]\n",
      "2023-12-26 12:19:18.445193: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_915' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_915}}]]\n",
      "2023-12-26 12:19:18.445265: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_933' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_933}}]]\n",
      "2023-12-26 12:19:18.445375: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_951' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_951}}]]\n",
      "2023-12-26 12:19:18.445447: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_956' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_956}}]]\n",
      "2023-12-26 12:19:18.445557: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_972' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_972}}]]\n",
      "2023-12-26 12:19:18.445669: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_990' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_990}}]]\n",
      "2023-12-26 12:19:18.445741: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1018' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1018}}]]\n",
      "2023-12-26 12:19:18.445850: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1026' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1026}}]]\n",
      "2023-12-26 12:19:18.445962: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1055' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1055}}]]\n",
      "2023-12-26 12:19:18.446036: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1073' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1073}}]]\n",
      "2023-12-26 12:19:18.446148: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1091' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1091}}]]\n",
      "2023-12-26 12:19:18.446260: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1096' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1096}}]]\n",
      "2023-12-26 12:19:18.446333: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1112' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1112}}]]\n",
      "2023-12-26 12:19:18.446447: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1128' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1128}}]]\n",
      "2023-12-26 12:19:18.446520: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1144' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1144}}]]\n",
      "2023-12-26 12:19:18.446644: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1160' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1160}}]]\n",
      "2023-12-26 12:19:18.446720: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1176' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1176}}]]\n",
      "2023-12-26 12:19:18.446831: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1192' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1192}}]]\n",
      "2023-12-26 12:19:18.446905: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1208' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1208}}]]\n",
      "2023-12-26 12:19:18.447016: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1224' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1224}}]]\n",
      "2023-12-26 12:19:18.447092: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1240' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1240}}]]\n",
      "2023-12-26 12:19:18.447203: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1256' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1256}}]]\n",
      "2023-12-26 12:19:18.447277: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1272' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1272}}]]\n",
      "2023-12-26 12:19:18.447388: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1288' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1288}}]]\n",
      "2023-12-26 12:19:18.447499: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1304' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1304}}]]\n",
      "2023-12-26 12:19:18.447572: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1320' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1320}}]]\n",
      "2023-12-26 12:19:18.447682: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1336' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1336}}]]\n",
      "2023-12-26 12:19:18.447794: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1352' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1352}}]]\n",
      "2023-12-26 12:19:18.447906: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1368' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1368}}]]\n",
      "2023-12-26 12:19:18.448018: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1384' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1384}}]]\n",
      "2023-12-26 12:19:18.448128: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1400' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1400}}]]\n",
      "2023-12-26 12:19:18.448238: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1420' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1420}}]]\n",
      "2023-12-26 12:19:18.448347: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1449' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1449}}]]\n",
      "2023-12-26 12:19:18.448460: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1467' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1467}}]]\n",
      "2023-12-26 12:19:18.448573: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1485' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1485}}]]\n",
      "2023-12-26 12:19:18.448685: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1490' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1490}}]]\n",
      "2023-12-26 12:19:18.448806: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1506' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1506}}]]\n",
      "2023-12-26 12:19:18.448905: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1524' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1524}}]]\n",
      "2023-12-26 12:19:18.449005: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1552' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1552}}]]\n",
      "2023-12-26 12:19:18.449105: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1560' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1560}}]]\n",
      "2023-12-26 12:19:18.449202: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1589' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1589}}]]\n",
      "2023-12-26 12:19:18.449302: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1607' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1607}}]]\n",
      "2023-12-26 12:19:18.449404: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1625' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1625}}]]\n",
      "2023-12-26 12:19:18.449504: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1630' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1630}}]]\n",
      "2023-12-26 12:19:18.449604: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1646' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1646}}]]\n",
      "2023-12-26 12:19:18.449702: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1662' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1662}}]]\n",
      "2023-12-26 12:19:18.449800: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1678' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1678}}]]\n",
      "2023-12-26 12:19:18.449899: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1694' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1694}}]]\n",
      "2023-12-26 12:19:18.449998: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1710' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1710}}]]\n",
      "2023-12-26 12:19:18.450097: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1726' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1726}}]]\n",
      "2023-12-26 12:19:18.450196: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1742' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1742}}]]\n",
      "2023-12-26 12:19:18.450296: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1758' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1758}}]]\n",
      "2023-12-26 12:19:18.450396: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1774' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1774}}]]\n",
      "2023-12-26 12:19:18.450496: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1790' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1790}}]]\n",
      "2023-12-26 12:19:18.450596: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1806' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1806}}]]\n",
      "2023-12-26 12:19:18.450691: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1822' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1822}}]]\n",
      "2023-12-26 12:19:18.450786: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1838' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1838}}]]\n",
      "2023-12-26 12:19:18.450881: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1856' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1856}}]]\n",
      "2023-12-26 12:19:18.450977: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1884' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1884}}]]\n",
      "2023-12-26 12:19:18.451072: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1888' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1888}}]]\n",
      "2023-12-26 12:19:18.451169: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1904' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1904}}]]\n",
      "2023-12-26 12:19:18.451263: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1920' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1920}}]]\n",
      "2023-12-26 12:19:18.451356: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1936' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1936}}]]\n",
      "2023-12-26 12:19:18.451449: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1956' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1956}}]]\n",
      "2023-12-26 12:19:18.451549: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1985' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_1985}}]]\n",
      "2023-12-26 12:19:18.451681: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2003' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2003}}]]\n",
      "2023-12-26 12:19:18.451841: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2021' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2021}}]]\n",
      "2023-12-26 12:19:18.451942: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2026' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2026}}]]\n",
      "2023-12-26 12:19:18.452043: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2042' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2042}}]]\n",
      "2023-12-26 12:19:18.452137: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2060' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2060}}]]\n",
      "2023-12-26 12:19:18.452231: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2088' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2088}}]]\n",
      "2023-12-26 12:19:18.452325: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2096' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2096}}]]\n",
      "2023-12-26 12:19:18.452419: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2125' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2125}}]]\n",
      "2023-12-26 12:19:18.452514: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2143' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2143}}]]\n",
      "2023-12-26 12:19:18.452609: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2161' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2161}}]]\n",
      "2023-12-26 12:19:18.452703: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2166' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2166}}]]\n",
      "2023-12-26 12:19:18.452797: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2182' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2182}}]]\n",
      "2023-12-26 12:19:18.452891: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2198' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2198}}]]\n",
      "2023-12-26 12:19:18.452985: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2214' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2214}}]]\n",
      "2023-12-26 12:19:18.453079: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2230' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2230}}]]\n",
      "2023-12-26 12:19:18.453174: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2246' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2246}}]]\n",
      "2023-12-26 12:19:18.453268: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2262' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2262}}]]\n",
      "2023-12-26 12:19:18.453362: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2278' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2278}}]]\n",
      "2023-12-26 12:19:18.453456: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2294' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2294}}]]\n",
      "2023-12-26 12:19:18.453551: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2310' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2310}}]]\n",
      "2023-12-26 12:19:18.453815: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2326' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2326}}]]\n",
      "2023-12-26 12:19:18.454009: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2342' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2342}}]]\n",
      "2023-12-26 12:19:18.454167: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2358' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2358}}]]\n",
      "2023-12-26 12:19:18.454353: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2374' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2374}}]]\n",
      "2023-12-26 12:19:18.454527: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2390' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2390}}]]\n",
      "2023-12-26 12:19:18.454730: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2406' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2406}}]]\n",
      "2023-12-26 12:19:18.454881: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2422' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2422}}]]\n",
      "2023-12-26 12:19:18.455064: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2438' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2438}}]]\n",
      "2023-12-26 12:19:18.455211: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2454' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2454}}]]\n",
      "2023-12-26 12:19:18.455374: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2470' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2470}}]]\n",
      "2023-12-26 12:19:18.455480: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2490' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2490}}]]\n",
      "2023-12-26 12:19:18.455623: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2519' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2519}}]]\n",
      "2023-12-26 12:19:18.455826: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2537' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2537}}]]\n",
      "2023-12-26 12:19:18.455927: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2555' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2555}}]]\n",
      "2023-12-26 12:19:18.455990: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2560' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2560}}]]\n",
      "2023-12-26 12:19:18.456086: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2576' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2576}}]]\n",
      "2023-12-26 12:19:18.456174: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2594' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2594}}]]\n",
      "2023-12-26 12:19:18.456321: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2622' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2622}}]]\n",
      "2023-12-26 12:19:18.456510: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2630' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2630}}]]\n",
      "2023-12-26 12:19:18.456670: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2659' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2659}}]]\n",
      "2023-12-26 12:19:18.456838: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2677' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2677}}]]\n",
      "2023-12-26 12:19:18.456988: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2695' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2695}}]]\n",
      "2023-12-26 12:19:18.457177: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2700' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2700}}]]\n",
      "2023-12-26 12:19:18.457296: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2716' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2716}}]]\n",
      "2023-12-26 12:19:18.457394: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2732' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2732}}]]\n",
      "2023-12-26 12:19:18.457495: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2748' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2748}}]]\n",
      "2023-12-26 12:19:18.457620: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2764' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2764}}]]\n",
      "2023-12-26 12:19:18.457759: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2780' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2780}}]]\n",
      "2023-12-26 12:19:18.457926: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2796' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2796}}]]\n",
      "2023-12-26 12:19:18.458023: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2812' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2812}}]]\n",
      "2023-12-26 12:19:18.458130: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2828' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2828}}]]\n",
      "2023-12-26 12:19:18.458251: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2844' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2844}}]]\n",
      "2023-12-26 12:19:18.458424: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2860' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2860}}]]\n",
      "2023-12-26 12:19:18.458539: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2876' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2876}}]]\n",
      "2023-12-26 12:19:18.458699: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2892' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2892}}]]\n",
      "2023-12-26 12:19:18.458964: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2908' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2908}}]]\n",
      "2023-12-26 12:19:18.459168: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2926' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2926}}]]\n",
      "2023-12-26 12:19:18.459329: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2954' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2954}}]]\n",
      "2023-12-26 12:19:18.459526: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2958' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2958}}]]\n",
      "2023-12-26 12:19:18.459682: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2974' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2974}}]]\n",
      "2023-12-26 12:19:18.459880: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2990' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_2990}}]]\n",
      "2023-12-26 12:19:18.460065: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3006' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3006}}]]\n",
      "2023-12-26 12:19:18.460221: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3026' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3026}}]]\n",
      "2023-12-26 12:19:18.460393: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3055' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3055}}]]\n",
      "2023-12-26 12:19:18.460565: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3073' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3073}}]]\n",
      "2023-12-26 12:19:18.460815: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3091' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3091}}]]\n",
      "2023-12-26 12:19:18.460952: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3096' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3096}}]]\n",
      "2023-12-26 12:19:18.461115: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3112' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3112}}]]\n",
      "2023-12-26 12:19:18.461292: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3130' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3130}}]]\n",
      "2023-12-26 12:19:18.461433: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3158' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3158}}]]\n",
      "2023-12-26 12:19:18.461570: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3166' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3166}}]]\n",
      "2023-12-26 12:19:18.461741: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3195' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3195}}]]\n",
      "2023-12-26 12:19:18.461908: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3213' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3213}}]]\n",
      "2023-12-26 12:19:18.462008: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3231' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3231}}]]\n",
      "2023-12-26 12:19:18.462106: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3236' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3236}}]]\n",
      "2023-12-26 12:19:18.462172: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3252' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3252}}]]\n",
      "2023-12-26 12:19:18.462270: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3268' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3268}}]]\n",
      "2023-12-26 12:19:18.462378: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3284' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3284}}]]\n",
      "2023-12-26 12:19:18.462447: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3300' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3300}}]]\n",
      "2023-12-26 12:19:18.462592: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3316' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3316}}]]\n",
      "2023-12-26 12:19:18.462697: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3332' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3332}}]]\n",
      "2023-12-26 12:19:18.462801: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3348' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3348}}]]\n",
      "2023-12-26 12:19:18.462870: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3364' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3364}}]]\n",
      "2023-12-26 12:19:18.462974: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3380' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3380}}]]\n",
      "2023-12-26 12:19:18.463077: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3396' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3396}}]]\n",
      "2023-12-26 12:19:18.463180: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3412' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3412}}]]\n",
      "2023-12-26 12:19:18.463249: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3428' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3428}}]]\n",
      "2023-12-26 12:19:18.463350: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3444' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3444}}]]\n",
      "2023-12-26 12:19:18.463419: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3460' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3460}}]]\n",
      "2023-12-26 12:19:18.463520: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3476' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3476}}]]\n",
      "2023-12-26 12:19:18.463590: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3492' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3492}}]]\n",
      "2023-12-26 12:19:18.463692: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3508' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3508}}]]\n",
      "2023-12-26 12:19:18.463810: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3524' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3524}}]]\n",
      "2023-12-26 12:19:18.463953: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3540' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3540}}]]\n",
      "2023-12-26 12:19:18.464122: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3560' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3560}}]]\n",
      "2023-12-26 12:19:18.464299: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3589' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3589}}]]\n",
      "2023-12-26 12:19:18.464449: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3607' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3607}}]]\n",
      "2023-12-26 12:19:18.464632: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3625' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3625}}]]\n",
      "2023-12-26 12:19:18.464794: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3630' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3630}}]]\n",
      "2023-12-26 12:19:18.464971: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3646' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3646}}]]\n",
      "2023-12-26 12:19:18.465129: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3664' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3664}}]]\n",
      "2023-12-26 12:19:18.465288: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3692' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3692}}]]\n",
      "2023-12-26 12:19:18.465446: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3700' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3700}}]]\n",
      "2023-12-26 12:19:18.465623: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3729' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3729}}]]\n",
      "2023-12-26 12:19:18.465797: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3747' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3747}}]]\n",
      "2023-12-26 12:19:18.465939: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3765' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3765}}]]\n",
      "2023-12-26 12:19:18.466079: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3770' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3770}}]]\n",
      "2023-12-26 12:19:18.466220: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3786' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3786}}]]\n",
      "2023-12-26 12:19:18.466361: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3802' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3802}}]]\n",
      "2023-12-26 12:19:18.466499: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3818' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3818}}]]\n",
      "2023-12-26 12:19:18.466639: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3834' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3834}}]]\n",
      "2023-12-26 12:19:18.466778: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3850' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3850}}]]\n",
      "2023-12-26 12:19:18.466917: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3866' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3866}}]]\n",
      "2023-12-26 12:19:18.467058: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3882' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3882}}]]\n",
      "2023-12-26 12:19:18.467183: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3898' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3898}}]]\n",
      "2023-12-26 12:19:18.467337: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3914' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3914}}]]\n",
      "2023-12-26 12:19:18.467450: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3930' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3930}}]]\n",
      "2023-12-26 12:19:18.467580: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3946' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3946}}]]\n",
      "2023-12-26 12:19:18.467753: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3962' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3962}}]]\n",
      "2023-12-26 12:19:18.467916: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3978' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3978}}]]\n",
      "2023-12-26 12:19:18.468033: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3996' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3996}}]]\n",
      "2023-12-26 12:19:18.468182: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4024' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4024}}]]\n",
      "2023-12-26 12:19:18.468323: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4028' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4028}}]]\n",
      "2023-12-26 12:19:18.468441: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4044' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4044}}]]\n",
      "2023-12-26 12:19:18.468558: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4060' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4060}}]]\n",
      "2023-12-26 12:19:18.468704: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4076' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4076}}]]\n",
      "2023-12-26 12:19:18.468841: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4096' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4096}}]]\n",
      "2023-12-26 12:19:18.468998: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4125' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4125}}]]\n",
      "2023-12-26 12:19:18.469121: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4143' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4143}}]]\n",
      "2023-12-26 12:19:18.469294: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4161' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4161}}]]\n",
      "2023-12-26 12:19:18.469416: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4166' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4166}}]]\n",
      "2023-12-26 12:19:18.469597: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4182' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4182}}]]\n",
      "2023-12-26 12:19:18.469811: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4200' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4200}}]]\n",
      "2023-12-26 12:19:18.469980: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4228' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4228}}]]\n",
      "2023-12-26 12:19:18.470116: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4236' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4236}}]]\n",
      "2023-12-26 12:19:18.470284: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4264' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4264}}]]\n",
      "2023-12-26 12:19:18.470429: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4275' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4275}}]]\n",
      "2023-12-26 12:19:18.470604: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4282' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4282}}]]\n",
      "2023-12-26 12:19:18.470742: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4287' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4287}}]]\n",
      "2023-12-26 12:19:18.470913: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4290' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4290}}]]\n",
      "2023-12-26 12:19:18.471081: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4293' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4293}}]]\n",
      "2023-12-26 12:19:18.471242: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4296' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4296}}]]\n",
      "2023-12-26 12:19:18.471371: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4299' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4299}}]]\n",
      "2023-12-26 12:19:18.471580: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4302' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4302}}]]\n",
      "2023-12-26 12:19:18.471961: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4305' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4305}}]]\n",
      "2023-12-26 12:19:18.472152: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4308' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4308}}]]\n",
      "2023-12-26 12:19:18.472352: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4311' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4311}}]]\n",
      "2023-12-26 12:19:18.472546: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4314' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_4314}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 loss: 0.0005437941 time: 32.22886109352112\n",
      "Iteration: 10 loss: 0.0001489374 time: 0.3427770137786865\n",
      "Iteration: 20 loss: 0.0000997782 time: 0.33704257011413574\n",
      "Iteration: 30 loss: 0.0000636894 time: 0.3545658588409424\n",
      "Iteration: 40 loss: 0.0000502882 time: 0.33411097526550293\n",
      "Iteration: 50 loss: 0.0000482877 time: 0.3187592029571533\n",
      "Iteration: 60 loss: 0.0000463916 time: 0.32625389099121094\n",
      "Iteration: 70 loss: 0.0000446941 time: 0.3223001956939697\n",
      "Iteration: 80 loss: 0.0000437834 time: 0.3200392723083496\n",
      "Iteration: 90 loss: 0.0000428962 time: 0.32009291648864746\n",
      "Iteration: 100 loss: 0.0000419218 time: 0.3280346393585205\n",
      "Iteration: 110 loss: 0.0000408579 time: 0.33176279067993164\n",
      "Iteration: 120 loss: 0.0000396645 time: 0.31200361251831055\n",
      "Iteration: 130 loss: 0.0000383315 time: 0.3176259994506836\n",
      "Iteration: 140 loss: 0.0000368416 time: 0.32559704780578613\n",
      "Iteration: 150 loss: 0.0000352025 time: 0.32386159896850586\n",
      "Iteration: 160 loss: 0.0000334505 time: 0.310361385345459\n",
      "Iteration: 170 loss: 0.0000316645 time: 0.3261878490447998\n",
      "Iteration: 180 loss: 0.0000299693 time: 0.3221588134765625\n",
      "Iteration: 190 loss: 0.0000285093 time: 0.3245360851287842\n",
      "Iteration: 200 loss: 0.0000273853 time: 0.3160371780395508\n",
      "Iteration: 210 loss: 0.0000265917 time: 0.3124673366546631\n",
      "Iteration: 220 loss: 0.0000260206 time: 0.31671738624572754\n",
      "Iteration: 230 loss: 0.0000255460 time: 0.30993175506591797\n",
      "Iteration: 240 loss: 0.0000250968 time: 0.33118534088134766\n",
      "Iteration: 250 loss: 0.0000246548 time: 0.3146932125091553\n",
      "Iteration: 260 loss: 0.0000242204 time: 0.32269740104675293\n",
      "Iteration: 270 loss: 0.0000237930 time: 0.31650638580322266\n",
      "Iteration: 280 loss: 0.0000233700 time: 0.32302260398864746\n",
      "Iteration: 290 loss: 0.0000229485 time: 0.3166377544403076\n",
      "Iteration: 300 loss: 0.0000225264 time: 0.31455349922180176\n",
      "Iteration: 310 loss: 0.0000221020 time: 0.32685041427612305\n",
      "Iteration: 320 loss: 0.0000216741 time: 0.3210105895996094\n",
      "Iteration: 330 loss: 0.0000212417 time: 0.31412482261657715\n",
      "Iteration: 340 loss: 0.0000208043 time: 0.31060051918029785\n",
      "Iteration: 350 loss: 0.0000203615 time: 0.30713891983032227\n",
      "Iteration: 360 loss: 0.0000199127 time: 0.31418371200561523\n",
      "Iteration: 370 loss: 0.0000194570 time: 0.3169128894805908\n",
      "Iteration: 380 loss: 0.0000189926 time: 0.31951236724853516\n",
      "Iteration: 390 loss: 0.0000185158 time: 0.32014894485473633\n",
      "Iteration: 400 loss: 0.0000180206 time: 0.32006335258483887\n",
      "Iteration: 410 loss: 0.0000174976 time: 0.31521081924438477\n",
      "Iteration: 420 loss: 0.0000169331 time: 0.3138885498046875\n",
      "Iteration: 430 loss: 0.0000163084 time: 0.32152438163757324\n",
      "Iteration: 440 loss: 0.0000155999 time: 0.3417789936065674\n",
      "Iteration: 450 loss: 0.0000147789 time: 0.35014867782592773\n",
      "Iteration: 460 loss: 0.0000138162 time: 0.34480738639831543\n",
      "Iteration: 470 loss: 0.0000126892 time: 0.340651273727417\n",
      "Iteration: 480 loss: 0.0000113976 time: 0.34931421279907227\n",
      "Iteration: 490 loss: 0.0000099822 time: 0.34526753425598145\n",
      "Iteration: 500 loss: 0.0000085339 time: 0.3501698970794678\n",
      "Iteration: 510 loss: 0.0000071718 time: 0.35056567192077637\n",
      "Iteration: 520 loss: 0.0000059909 time: 0.34609484672546387\n",
      "Iteration: 530 loss: 0.0000050253 time: 0.3513178825378418\n",
      "Iteration: 540 loss: 0.0000042588 time: 0.34468817710876465\n",
      "Iteration: 550 loss: 0.0000036629 time: 0.34383511543273926\n",
      "Iteration: 560 loss: 0.0000032137 time: 0.3214411735534668\n",
      "Iteration: 570 loss: 0.0000028868 time: 0.3262465000152588\n",
      "Iteration: 580 loss: 0.0000026535 time: 0.30799174308776855\n",
      "Iteration: 590 loss: 0.0000024860 time: 0.3120384216308594\n",
      "Iteration: 600 loss: 0.0000023625 time: 0.3102271556854248\n",
      "Iteration: 610 loss: 0.0000022673 time: 0.31212759017944336\n",
      "Iteration: 620 loss: 0.0000021902 time: 0.31652331352233887\n",
      "Iteration: 630 loss: 0.0000021246 time: 0.30724573135375977\n",
      "Iteration: 640 loss: 0.0000020665 time: 0.32314014434814453\n",
      "Iteration: 650 loss: 0.0000020137 time: 0.3072514533996582\n",
      "Iteration: 660 loss: 0.0000019643 time: 0.3063499927520752\n",
      "Iteration: 670 loss: 0.0000019174 time: 0.3100752830505371\n",
      "Iteration: 680 loss: 0.0000018723 time: 0.3105299472808838\n",
      "Iteration: 690 loss: 0.0000018285 time: 0.3169138431549072\n",
      "Iteration: 700 loss: 0.0000017856 time: 0.3177499771118164\n",
      "Iteration: 710 loss: 0.0000017435 time: 0.3254396915435791\n",
      "Iteration: 720 loss: 0.0000017021 time: 0.31481337547302246\n",
      "Iteration: 730 loss: 0.0000016613 time: 0.31685757637023926\n",
      "Iteration: 740 loss: 0.0000016212 time: 0.31329822540283203\n",
      "Iteration: 750 loss: 0.0000015817 time: 0.3134140968322754\n",
      "Iteration: 760 loss: 0.0000015428 time: 0.30682849884033203\n",
      "Iteration: 770 loss: 0.0000015046 time: 0.31781816482543945\n",
      "Iteration: 780 loss: 0.0000014672 time: 0.3212625980377197\n",
      "Iteration: 790 loss: 0.0000014306 time: 0.3115088939666748\n",
      "Iteration: 800 loss: 0.0000013949 time: 0.31386470794677734\n",
      "Iteration: 810 loss: 0.0000013600 time: 0.314558744430542\n",
      "Iteration: 820 loss: 0.0000013260 time: 0.31285738945007324\n",
      "Iteration: 830 loss: 0.0000012930 time: 0.31137609481811523\n",
      "Iteration: 840 loss: 0.0000012609 time: 0.3158230781555176\n",
      "Iteration: 850 loss: 0.0000012298 time: 0.3159453868865967\n",
      "Iteration: 860 loss: 0.0000011997 time: 0.30977487564086914\n",
      "Iteration: 870 loss: 0.0000011707 time: 0.3137810230255127\n",
      "Iteration: 880 loss: 0.0000011426 time: 0.30597662925720215\n",
      "Iteration: 890 loss: 0.0000011156 time: 0.30770111083984375\n",
      "Iteration: 900 loss: 0.0000010896 time: 0.31360316276550293\n",
      "Iteration: 910 loss: 0.0000010645 time: 0.3281083106994629\n",
      "Iteration: 920 loss: 0.0000010404 time: 0.3132438659667969\n",
      "Iteration: 930 loss: 0.0000010173 time: 0.31630563735961914\n",
      "Iteration: 940 loss: 0.0000009951 time: 0.3147103786468506\n",
      "Iteration: 950 loss: 0.0000009738 time: 0.31119513511657715\n",
      "Iteration: 960 loss: 0.0000009533 time: 0.31731653213500977\n",
      "Iteration: 970 loss: 0.0000009336 time: 0.32228755950927734\n",
      "Iteration: 980 loss: 0.0000009148 time: 0.3388204574584961\n",
      "Iteration: 990 loss: 0.0000008966 time: 0.3107419013977051\n",
      "Iteration: 1000 loss: 0.0000008792 time: 0.32130980491638184\n",
      "Iteration: 1010 loss: 0.0000008624 time: 0.3115730285644531\n",
      "Iteration: 1020 loss: 0.0000008462 time: 0.3075594902038574\n",
      "Iteration: 1030 loss: 0.0000008305 time: 0.314333438873291\n",
      "Iteration: 1040 loss: 0.0000008153 time: 0.31046628952026367\n",
      "Iteration: 1050 loss: 0.0000008005 time: 0.3341207504272461\n",
      "Iteration: 1060 loss: 0.0000007862 time: 0.318098783493042\n",
      "Iteration: 1070 loss: 0.0000007722 time: 0.3148648738861084\n",
      "Iteration: 1080 loss: 0.0000007585 time: 0.31435728073120117\n",
      "Iteration: 1090 loss: 0.0000007450 time: 0.317000150680542\n",
      "Iteration: 1100 loss: 0.0000007318 time: 0.3112297058105469\n",
      "Iteration: 1110 loss: 0.0000007188 time: 0.310469388961792\n",
      "Iteration: 1120 loss: 0.0000007059 time: 0.32446742057800293\n",
      "Iteration: 1130 loss: 0.0000006932 time: 0.3158705234527588\n",
      "Iteration: 1140 loss: 0.0000006806 time: 0.306790828704834\n",
      "Iteration: 1150 loss: 0.0000006680 time: 0.3179032802581787\n",
      "Iteration: 1160 loss: 0.0000006555 time: 0.3075706958770752\n",
      "Iteration: 1170 loss: 0.0000006431 time: 0.3120090961456299\n",
      "Iteration: 1180 loss: 0.0000006307 time: 0.32636141777038574\n",
      "Iteration: 1190 loss: 0.0000006184 time: 0.32295703887939453\n",
      "Iteration: 1200 loss: 0.0000006060 time: 0.31703615188598633\n",
      "Iteration: 1210 loss: 0.0000005937 time: 0.3190193176269531\n",
      "Iteration: 1220 loss: 0.0000005814 time: 0.3079838752746582\n",
      "Iteration: 1230 loss: 0.0000005692 time: 0.3069953918457031\n",
      "Iteration: 1240 loss: 0.0000005569 time: 0.3175013065338135\n",
      "Iteration: 1250 loss: 0.0000005447 time: 0.3258681297302246\n",
      "Iteration: 1260 loss: 0.0000005325 time: 0.31468987464904785\n",
      "Iteration: 1270 loss: 0.0000005204 time: 0.3116145133972168\n",
      "Iteration: 1280 loss: 0.0000005084 time: 0.308077335357666\n",
      "Iteration: 1290 loss: 0.0000004964 time: 0.31319713592529297\n",
      "Iteration: 1300 loss: 0.0000004845 time: 0.31063246726989746\n",
      "Iteration: 1310 loss: 0.0000004727 time: 0.3399014472961426\n",
      "Iteration: 1320 loss: 0.0000004611 time: 0.27567553520202637\n",
      "Iteration: 1330 loss: 0.0000004496 time: 0.28133344650268555\n",
      "Iteration: 1340 loss: 0.0000004382 time: 0.3093149662017822\n",
      "Iteration: 1350 loss: 0.0000004270 time: 0.3058462142944336\n",
      "Iteration: 1360 loss: 0.0000004160 time: 0.31553173065185547\n",
      "Iteration: 1370 loss: 0.0000004052 time: 0.3070249557495117\n",
      "Iteration: 1380 loss: 0.0000003946 time: 0.31413936614990234\n",
      "Iteration: 1390 loss: 0.0000003842 time: 0.3349337577819824\n",
      "Iteration: 1400 loss: 0.0000003740 time: 0.3101804256439209\n",
      "Iteration: 1410 loss: 0.0000003641 time: 0.31882309913635254\n",
      "Iteration: 1420 loss: 0.0000003545 time: 0.3138918876647949\n",
      "Iteration: 1430 loss: 0.0000003450 time: 0.31273412704467773\n",
      "Iteration: 1440 loss: 0.0000003359 time: 0.3205268383026123\n",
      "Iteration: 1450 loss: 0.0000003270 time: 0.3163881301879883\n",
      "Iteration: 1460 loss: 0.0000003183 time: 0.3245840072631836\n",
      "Iteration: 1470 loss: 0.0000003100 time: 0.3124575614929199\n",
      "Iteration: 1480 loss: 0.0000003018 time: 0.31638240814208984\n",
      "Iteration: 1490 loss: 0.0000002939 time: 0.31830906867980957\n",
      "Iteration: 1500 loss: 0.0000002863 time: 0.3134462833404541\n",
      "Iteration: 1510 loss: 0.0000002789 time: 0.3091261386871338\n",
      "Iteration: 1520 loss: 0.0000002717 time: 0.3131439685821533\n",
      "Iteration: 1530 loss: 0.0000002648 time: 0.32501220703125\n",
      "Iteration: 1540 loss: 0.0000002580 time: 0.32094478607177734\n",
      "Iteration: 1550 loss: 0.0000002515 time: 0.314969539642334\n",
      "Iteration: 1560 loss: 0.0000002452 time: 0.3071889877319336\n",
      "Iteration: 1570 loss: 0.0000002390 time: 0.3182563781738281\n",
      "Iteration: 1580 loss: 0.0000002331 time: 0.3186919689178467\n",
      "Iteration: 1590 loss: 0.0000002273 time: 0.3185913562774658\n",
      "Iteration: 1600 loss: 0.0000002217 time: 0.3247051239013672\n",
      "Iteration: 1610 loss: 0.0000002162 time: 0.314542293548584\n",
      "Iteration: 1620 loss: 0.0000002109 time: 0.3171391487121582\n",
      "Iteration: 1630 loss: 0.0000002058 time: 0.31766748428344727\n",
      "Iteration: 1640 loss: 0.0000002008 time: 0.31772661209106445\n",
      "Iteration: 1650 loss: 0.0000001959 time: 0.31397080421447754\n",
      "Iteration: 1660 loss: 0.0000001912 time: 0.3197288513183594\n",
      "Iteration: 1670 loss: 0.0000001867 time: 0.32346415519714355\n",
      "Iteration: 1680 loss: 0.0000001822 time: 0.31415653228759766\n",
      "Iteration: 1690 loss: 0.0000001779 time: 0.3121309280395508\n",
      "Iteration: 1700 loss: 0.0000001738 time: 0.3216419219970703\n",
      "Iteration: 1710 loss: 0.0000001698 time: 0.3176085948944092\n",
      "Iteration: 1720 loss: 0.0000001659 time: 0.3119020462036133\n",
      "Iteration: 1730 loss: 0.0000001621 time: 0.3319694995880127\n",
      "Iteration: 1740 loss: 0.0000001585 time: 0.31606316566467285\n",
      "Iteration: 1750 loss: 0.0000001550 time: 0.3136570453643799\n",
      "Iteration: 1760 loss: 0.0000001516 time: 0.31702733039855957\n",
      "Iteration: 1770 loss: 0.0000001484 time: 0.3220705986022949\n",
      "Iteration: 1780 loss: 0.0000001453 time: 0.3128063678741455\n",
      "Iteration: 1790 loss: 0.0000001423 time: 0.31047892570495605\n",
      "Iteration: 1800 loss: 0.0000001394 time: 0.3337717056274414\n",
      "Iteration: 1810 loss: 0.0000001366 time: 0.3153400421142578\n",
      "Iteration: 1820 loss: 0.0000001339 time: 0.31290745735168457\n",
      "Iteration: 1830 loss: 0.0000001313 time: 0.31529688835144043\n",
      "Iteration: 1840 loss: 0.0000001289 time: 0.3203408718109131\n",
      "Iteration: 1850 loss: 0.0000001265 time: 0.320676326751709\n",
      "Iteration: 1860 loss: 0.0000001242 time: 0.315227746963501\n",
      "Iteration: 1870 loss: 0.0000001220 time: 0.3276820182800293\n",
      "Iteration: 1880 loss: 0.0000001199 time: 0.31028056144714355\n",
      "Iteration: 1890 loss: 0.0000001179 time: 0.32479238510131836\n",
      "Iteration: 1900 loss: 0.0000001160 time: 0.31705427169799805\n",
      "Iteration: 1910 loss: 0.0000001141 time: 0.30875301361083984\n",
      "Iteration: 1920 loss: 0.0000001123 time: 0.32505202293395996\n",
      "Iteration: 1930 loss: 0.0000001106 time: 0.32543325424194336\n",
      "Iteration: 1940 loss: 0.0000001089 time: 0.3210296630859375\n",
      "Iteration: 1950 loss: 0.0000001073 time: 0.3161301612854004\n",
      "Iteration: 1960 loss: 0.0000001058 time: 0.31667327880859375\n",
      "Iteration: 1970 loss: 0.0000001042 time: 0.30802440643310547\n",
      "Iteration: 1980 loss: 0.0000001028 time: 0.31035804748535156\n",
      "Iteration: 1990 loss: 0.0000001014 time: 0.3121647834777832\n",
      "Iteration: 2000 loss: 0.0000001000 time: 0.32134509086608887\n",
      "Iteration: 2010 loss: 0.0000000987 time: 0.3175797462463379\n",
      "Iteration: 2020 loss: 0.0000000974 time: 0.32253575325012207\n",
      "Iteration: 2030 loss: 0.0000000962 time: 0.3142545223236084\n",
      "Iteration: 2040 loss: 0.0000000949 time: 0.3208286762237549\n",
      "Iteration: 2050 loss: 0.0000000937 time: 0.30997228622436523\n",
      "Iteration: 2060 loss: 0.0000000926 time: 0.32077813148498535\n",
      "Iteration: 2070 loss: 0.0000000914 time: 0.32521629333496094\n",
      "Iteration: 2080 loss: 0.0000000903 time: 0.31598377227783203\n",
      "Iteration: 2090 loss: 0.0000000892 time: 0.30971670150756836\n",
      "Iteration: 2100 loss: 0.0000000882 time: 0.32123303413391113\n",
      "Iteration: 2110 loss: 0.0000000871 time: 0.31664323806762695\n",
      "Iteration: 2120 loss: 0.0000000861 time: 0.3206758499145508\n",
      "Iteration: 2130 loss: 0.0000000851 time: 0.30986571311950684\n",
      "Iteration: 2140 loss: 0.0000000841 time: 0.33229684829711914\n",
      "Iteration: 2150 loss: 0.0000000831 time: 0.31043100357055664\n",
      "Iteration: 2160 loss: 0.0000000821 time: 0.31163454055786133\n",
      "Iteration: 2170 loss: 0.0000000812 time: 0.31287360191345215\n",
      "Iteration: 2180 loss: 0.0000000802 time: 0.3173563480377197\n",
      "Iteration: 2190 loss: 0.0000000793 time: 0.30823349952697754\n",
      "Iteration: 2200 loss: 0.0000000784 time: 0.316622257232666\n",
      "Iteration: 2210 loss: 0.0000000775 time: 0.33136749267578125\n",
      "Iteration: 2220 loss: 0.0000000766 time: 0.3232307434082031\n",
      "Iteration: 2230 loss: 0.0000000757 time: 0.3162045478820801\n",
      "Iteration: 2240 loss: 0.0000000749 time: 0.3181142807006836\n",
      "Iteration: 2250 loss: 0.0000000740 time: 0.31573963165283203\n",
      "Iteration: 2260 loss: 0.0000000731 time: 0.31038761138916016\n",
      "Iteration: 2270 loss: 0.0000000723 time: 0.3366110324859619\n",
      "Iteration: 2280 loss: 0.0000000715 time: 0.32105422019958496\n",
      "Iteration: 2290 loss: 0.0000000706 time: 0.31092309951782227\n",
      "Iteration: 2300 loss: 0.0000000698 time: 0.3142831325531006\n",
      "Iteration: 2310 loss: 0.0000000690 time: 0.31819868087768555\n",
      "Iteration: 2320 loss: 0.0000000682 time: 0.33095502853393555\n",
      "Iteration: 2330 loss: 0.0000000674 time: 0.3219563961029053\n",
      "Iteration: 2340 loss: 0.0000000666 time: 0.32749176025390625\n",
      "Iteration: 2350 loss: 0.0000000658 time: 0.31166720390319824\n",
      "Iteration: 2360 loss: 0.0000000651 time: 0.3260316848754883\n",
      "Iteration: 2370 loss: 0.0000000643 time: 0.31717991828918457\n",
      "Iteration: 2380 loss: 0.0000000636 time: 0.3092920780181885\n",
      "Iteration: 2390 loss: 0.0000000628 time: 0.3155782222747803\n",
      "Iteration: 2400 loss: 0.0000000621 time: 0.3174400329589844\n",
      "Iteration: 2410 loss: 0.0000000613 time: 0.3276958465576172\n",
      "Iteration: 2420 loss: 0.0000000606 time: 0.31198859214782715\n",
      "Iteration: 2430 loss: 0.0000000599 time: 0.3174557685852051\n",
      "Iteration: 2440 loss: 0.0000000591 time: 0.31656384468078613\n",
      "Iteration: 2450 loss: 0.0000000584 time: 0.31526923179626465\n",
      "Iteration: 2460 loss: 0.0000000577 time: 0.32306909561157227\n",
      "Iteration: 2470 loss: 0.0000000570 time: 0.33698344230651855\n",
      "Iteration: 2480 loss: 0.0000000563 time: 0.31792688369750977\n",
      "Iteration: 2490 loss: 0.0000000557 time: 0.3251171112060547\n",
      "Iteration: 2500 loss: 0.0000000550 time: 0.32048559188842773\n",
      "Iteration: 2510 loss: 0.0000000543 time: 0.3145711421966553\n",
      "Iteration: 2520 loss: 0.0000000536 time: 0.3194911479949951\n",
      "Iteration: 2530 loss: 0.0000000530 time: 0.31781649589538574\n",
      "Iteration: 2540 loss: 0.0000000523 time: 0.32977986335754395\n",
      "Iteration: 2550 loss: 0.0000000517 time: 0.32764196395874023\n",
      "Iteration: 2560 loss: 0.0000000510 time: 0.3106050491333008\n",
      "Iteration: 2570 loss: 0.0000000504 time: 0.2863593101501465\n",
      "Iteration: 2580 loss: 0.0000000498 time: 0.2904207706451416\n",
      "Iteration: 2590 loss: 0.0000000491 time: 0.3028998374938965\n",
      "Iteration: 2600 loss: 0.0000000485 time: 0.314882755279541\n",
      "Iteration: 2610 loss: 0.0000000479 time: 0.32970237731933594\n",
      "Iteration: 2620 loss: 0.0000000473 time: 0.31907010078430176\n",
      "Iteration: 2630 loss: 0.0000000467 time: 0.317807674407959\n",
      "Iteration: 2640 loss: 0.0000000461 time: 0.31502389907836914\n",
      "Iteration: 2650 loss: 0.0000000455 time: 0.324038028717041\n",
      "Iteration: 2660 loss: 0.0000000450 time: 0.31877708435058594\n",
      "Iteration: 2670 loss: 0.0000000444 time: 0.3182516098022461\n",
      "Iteration: 2680 loss: 0.0000000438 time: 0.32609105110168457\n",
      "Iteration: 2690 loss: 0.0000000433 time: 0.315248966217041\n",
      "Iteration: 2700 loss: 0.0000000427 time: 0.3203771114349365\n",
      "Iteration: 2710 loss: 0.0000000422 time: 0.3222076892852783\n",
      "Iteration: 2720 loss: 0.0000000416 time: 0.3159959316253662\n",
      "Iteration: 2730 loss: 0.0000000411 time: 0.3164370059967041\n",
      "Iteration: 2740 loss: 0.0000000406 time: 0.32328224182128906\n",
      "Iteration: 2750 loss: 0.0000000400 time: 0.32656192779541016\n",
      "Iteration: 2760 loss: 0.0000000395 time: 0.3156471252441406\n",
      "Iteration: 2770 loss: 0.0000000390 time: 0.31964898109436035\n",
      "Iteration: 2780 loss: 0.0000000385 time: 0.3162357807159424\n",
      "Iteration: 2790 loss: 0.0000000380 time: 0.3308298587799072\n",
      "Iteration: 2800 loss: 0.0000000375 time: 0.3139050006866455\n",
      "Iteration: 2810 loss: 0.0000000370 time: 0.3190267086029053\n",
      "Iteration: 2820 loss: 0.0000000365 time: 0.3229660987854004\n",
      "Iteration: 2830 loss: 0.0000000361 time: 0.31502842903137207\n",
      "Iteration: 2840 loss: 0.0000000356 time: 0.32727932929992676\n",
      "Iteration: 2850 loss: 0.0000000351 time: 0.3094034194946289\n",
      "Iteration: 2860 loss: 0.0000000347 time: 0.31203651428222656\n",
      "Iteration: 2870 loss: 0.0000000342 time: 0.3329129219055176\n",
      "Iteration: 2880 loss: 0.0000000338 time: 0.31978321075439453\n",
      "Iteration: 2890 loss: 0.0000000334 time: 0.3244607448577881\n",
      "Iteration: 2900 loss: 0.0000000329 time: 0.3151860237121582\n",
      "Iteration: 2910 loss: 0.0000000325 time: 0.31311917304992676\n",
      "Iteration: 2920 loss: 0.0000000321 time: 0.3129997253417969\n",
      "Iteration: 2930 loss: 0.0000000317 time: 0.3144714832305908\n",
      "Iteration: 2940 loss: 0.0000000313 time: 0.3189697265625\n",
      "Iteration: 2950 loss: 0.0000000308 time: 0.32902073860168457\n",
      "Iteration: 2960 loss: 0.0000000305 time: 0.3149120807647705\n",
      "Iteration: 2970 loss: 0.0000000301 time: 0.328416109085083\n",
      "Iteration: 2980 loss: 0.0000000297 time: 0.3217155933380127\n",
      "Iteration: 2990 loss: 0.0000000293 time: 0.3163306713104248\n",
      "Iteration: 3000 loss: 0.0000000289 time: 0.31766247749328613\n",
      "Iteration: 3010 loss: 0.0000000285 time: 0.3204011917114258\n",
      "Iteration: 3020 loss: 0.0000000282 time: 0.33005809783935547\n",
      "Iteration: 3030 loss: 0.0000000278 time: 0.31775856018066406\n",
      "Iteration: 3040 loss: 0.0000000275 time: 0.3128523826599121\n",
      "Iteration: 3050 loss: 0.0000000271 time: 0.31482934951782227\n",
      "Iteration: 3060 loss: 0.0000000268 time: 0.31762099266052246\n",
      "Iteration: 3070 loss: 0.0000000264 time: 0.31499576568603516\n",
      "Iteration: 3080 loss: 0.0000000261 time: 0.3362696170806885\n",
      "Iteration: 3090 loss: 0.0000000258 time: 0.331432580947876\n",
      "Iteration: 3100 loss: 0.0000000255 time: 0.31110262870788574\n",
      "Iteration: 3110 loss: 0.0000000251 time: 0.31656742095947266\n",
      "Iteration: 3120 loss: 0.0000000248 time: 0.31655001640319824\n",
      "Iteration: 3130 loss: 0.0000000245 time: 0.3193840980529785\n",
      "Iteration: 3140 loss: 0.0000000242 time: 0.3155345916748047\n",
      "Iteration: 3150 loss: 0.0000000239 time: 0.32991814613342285\n",
      "Iteration: 3160 loss: 0.0000000236 time: 0.31536197662353516\n",
      "Iteration: 3170 loss: 0.0000000233 time: 0.31871509552001953\n",
      "Iteration: 3180 loss: 0.0000000230 time: 0.31409549713134766\n",
      "Iteration: 3190 loss: 0.0000000228 time: 0.316800594329834\n",
      "Iteration: 3200 loss: 0.0000000225 time: 0.308793306350708\n",
      "Iteration: 3210 loss: 0.0000000222 time: 0.31427931785583496\n",
      "Iteration: 3220 loss: 0.0000000220 time: 0.3352646827697754\n",
      "Iteration: 3230 loss: 0.0000000217 time: 0.3164091110229492\n",
      "Iteration: 3240 loss: 0.0000000214 time: 0.32515859603881836\n",
      "Iteration: 3250 loss: 0.0000000212 time: 0.31819987297058105\n",
      "Iteration: 3260 loss: 0.0000000209 time: 0.31966423988342285\n",
      "Iteration: 3270 loss: 0.0000000207 time: 0.31656932830810547\n",
      "Iteration: 3280 loss: 0.0000000204 time: 0.31943702697753906\n",
      "Iteration: 3290 loss: 0.0000000202 time: 0.3357372283935547\n",
      "Iteration: 3300 loss: 0.0000000200 time: 0.32005953788757324\n",
      "Iteration: 3310 loss: 0.0000000197 time: 0.3154795169830322\n",
      "Iteration: 3320 loss: 0.0000000195 time: 0.3134167194366455\n",
      "Iteration: 3330 loss: 0.0000000193 time: 0.3230443000793457\n",
      "Iteration: 3340 loss: 0.0000000191 time: 0.3254258632659912\n",
      "Iteration: 3350 loss: 0.0000000189 time: 0.32620716094970703\n",
      "Iteration: 3360 loss: 0.0000000186 time: 0.32347655296325684\n",
      "Iteration: 3370 loss: 0.0000000184 time: 0.31612586975097656\n",
      "Iteration: 3380 loss: 0.0000000182 time: 0.3217959403991699\n",
      "Iteration: 3390 loss: 0.0000000180 time: 0.32012200355529785\n",
      "Iteration: 3400 loss: 0.0000000178 time: 0.3101685047149658\n",
      "Iteration: 3410 loss: 0.0000000176 time: 0.3241291046142578\n",
      "Iteration: 3420 loss: 0.0000000174 time: 0.3323087692260742\n",
      "Iteration: 3430 loss: 0.0000000172 time: 0.31583666801452637\n",
      "Iteration: 3440 loss: 0.0000000171 time: 0.3119642734527588\n",
      "Iteration: 3450 loss: 0.0000000169 time: 0.31508898735046387\n",
      "Iteration: 3460 loss: 0.0000000167 time: 0.3171963691711426\n",
      "Iteration: 3470 loss: 0.0000000165 time: 0.3187119960784912\n",
      "Iteration: 3480 loss: 0.0000000163 time: 0.3137216567993164\n",
      "Iteration: 3490 loss: 0.0000000162 time: 0.32831549644470215\n",
      "Iteration: 3500 loss: 0.0000000160 time: 0.32184791564941406\n",
      "Iteration: 3510 loss: 0.0000000158 time: 0.3099846839904785\n",
      "Iteration: 3520 loss: 0.0000000157 time: 0.3153986930847168\n",
      "Iteration: 3530 loss: 0.0000000155 time: 0.322063684463501\n",
      "Iteration: 3540 loss: 0.0000000154 time: 0.3209989070892334\n",
      "Iteration: 3550 loss: 0.0000000152 time: 0.31667137145996094\n",
      "Iteration: 3560 loss: 0.0000000150 time: 0.3267979621887207\n",
      "Iteration: 3570 loss: 0.0000000149 time: 0.3191678524017334\n",
      "Iteration: 3580 loss: 0.0000000147 time: 0.3195619583129883\n",
      "Iteration: 3590 loss: 0.0000000146 time: 0.31551408767700195\n",
      "Iteration: 3600 loss: 0.0000000145 time: 0.31701087951660156\n",
      "Iteration: 3610 loss: 0.0000000143 time: 0.32427406311035156\n",
      "Iteration: 3620 loss: 0.0000000142 time: 0.3332505226135254\n",
      "Iteration: 3630 loss: 0.0000000140 time: 0.3168067932128906\n",
      "Iteration: 3640 loss: 0.0000000139 time: 0.311525821685791\n",
      "Iteration: 3650 loss: 0.0000000138 time: 0.3267371654510498\n",
      "Iteration: 3660 loss: 0.0000000136 time: 0.31978487968444824\n",
      "Iteration: 3670 loss: 0.0000000135 time: 0.3131892681121826\n",
      "Iteration: 3680 loss: 0.0000000134 time: 0.3219742774963379\n",
      "Iteration: 3690 loss: 0.0000000133 time: 0.32799839973449707\n",
      "Iteration: 3700 loss: 0.0000000131 time: 0.3188915252685547\n",
      "Iteration: 3710 loss: 0.0000000130 time: 0.3182656764984131\n",
      "Iteration: 3720 loss: 0.0000000129 time: 0.3132154941558838\n",
      "Iteration: 3730 loss: 0.0000000128 time: 0.3226161003112793\n",
      "Iteration: 3740 loss: 0.0000000127 time: 0.317554235458374\n",
      "Iteration: 3750 loss: 0.0000000125 time: 0.31438589096069336\n",
      "Iteration: 3760 loss: 0.0000000124 time: 0.3336808681488037\n",
      "Iteration: 3770 loss: 0.0000000123 time: 0.3127906322479248\n",
      "Iteration: 3780 loss: 0.0000000122 time: 0.3187110424041748\n",
      "Iteration: 3790 loss: 0.0000000121 time: 0.32328152656555176\n",
      "Iteration: 3800 loss: 0.0000000120 time: 0.32045888900756836\n",
      "Iteration: 3810 loss: 0.0000000119 time: 0.31314969062805176\n",
      "Iteration: 3820 loss: 0.0000000118 time: 0.3270726203918457\n",
      "Iteration: 3830 loss: 0.0000000117 time: 0.3243286609649658\n",
      "Iteration: 3840 loss: 0.0000000116 time: 0.3206667900085449\n",
      "Iteration: 3850 loss: 0.0000000115 time: 0.32268810272216797\n",
      "Iteration: 3860 loss: 0.0000000114 time: 0.31438469886779785\n",
      "Iteration: 3870 loss: 0.0000000113 time: 0.33592677116394043\n",
      "Iteration: 3880 loss: 0.0000000112 time: 0.2881152629852295\n",
      "Iteration: 3890 loss: 0.0000000111 time: 0.32723283767700195\n",
      "Iteration: 3900 loss: 0.0000000110 time: 0.3222630023956299\n",
      "Iteration: 3910 loss: 0.0000000110 time: 0.31894946098327637\n",
      "Iteration: 3920 loss: 0.0000000109 time: 0.31745004653930664\n",
      "Iteration: 3930 loss: 0.0000000108 time: 0.32242774963378906\n",
      "Iteration: 3940 loss: 0.0000000107 time: 0.3190426826477051\n",
      "Iteration: 3950 loss: 0.0000000106 time: 0.3197154998779297\n",
      "Iteration: 3960 loss: 0.0000000105 time: 0.3271980285644531\n",
      "Iteration: 3970 loss: 0.0000000104 time: 0.32627367973327637\n",
      "Iteration: 3980 loss: 0.0000000104 time: 0.3229506015777588\n",
      "Iteration: 3990 loss: 0.0000000103 time: 0.32010340690612793\n",
      "Iteration: 4000 loss: 0.0000000102 time: 0.3109557628631592\n",
      "Iteration: 4010 loss: 0.0000000101 time: 0.32320117950439453\n",
      "Iteration: 4020 loss: 0.0000000101 time: 0.31923890113830566\n",
      "Iteration: 4030 loss: 0.0000000100 time: 0.3353145122528076\n",
      "Iteration: 4040 loss: 0.0000000099 time: 0.3211195468902588\n",
      "Iteration: 4050 loss: 0.0000000098 time: 0.3154106140136719\n",
      "Iteration: 4060 loss: 0.0000000098 time: 0.3199145793914795\n",
      "Iteration: 4070 loss: 0.0000000097 time: 0.32857728004455566\n",
      "Iteration: 4080 loss: 0.0000000096 time: 0.31586599349975586\n",
      "Iteration: 4090 loss: 0.0000000096 time: 0.33226990699768066\n",
      "Iteration: 4100 loss: 0.0000000095 time: 0.3218262195587158\n",
      "Iteration: 4110 loss: 0.0000000094 time: 0.32572460174560547\n",
      "Iteration: 4120 loss: 0.0000000094 time: 0.32132935523986816\n",
      "Iteration: 4130 loss: 0.0000000093 time: 0.3150503635406494\n",
      "Iteration: 4140 loss: 0.0000000092 time: 0.32349181175231934\n",
      "Iteration: 4150 loss: 0.0000000092 time: 0.3230469226837158\n",
      "Iteration: 4160 loss: 0.0000000091 time: 0.33629798889160156\n",
      "Iteration: 4170 loss: 0.0000000090 time: 0.33146190643310547\n",
      "Iteration: 4180 loss: 0.0000000090 time: 0.31833863258361816\n",
      "Iteration: 4190 loss: 0.0000000089 time: 0.31931447982788086\n",
      "Iteration: 4200 loss: 0.0000000089 time: 0.32106661796569824\n",
      "Iteration: 4210 loss: 0.0000000088 time: 0.3249812126159668\n",
      "Iteration: 4220 loss: 0.0000000087 time: 0.32254552841186523\n",
      "Iteration: 4230 loss: 0.0000000087 time: 0.3390166759490967\n",
      "Iteration: 4240 loss: 0.0000000086 time: 0.3158395290374756\n",
      "Iteration: 4250 loss: 0.0000000086 time: 0.3197591304779053\n",
      "Iteration: 4260 loss: 0.0000000085 time: 0.32030725479125977\n",
      "Iteration: 4270 loss: 0.0000000085 time: 0.3217928409576416\n",
      "Iteration: 4280 loss: 0.0000000084 time: 0.3225064277648926\n",
      "Iteration: 4290 loss: 0.0000000084 time: 0.3281683921813965\n",
      "Iteration: 4300 loss: 0.0000000083 time: 0.3199138641357422\n",
      "Iteration: 4310 loss: 0.0000000083 time: 0.32375192642211914\n",
      "Iteration: 4320 loss: 0.0000000082 time: 0.3177039623260498\n",
      "Iteration: 4330 loss: 0.0000000082 time: 0.3230624198913574\n",
      "Iteration: 4340 loss: 0.0000000081 time: 0.3275015354156494\n",
      "Iteration: 4350 loss: 0.0000000081 time: 0.34023332595825195\n",
      "Iteration: 4360 loss: 0.0000000080 time: 0.3608436584472656\n",
      "Iteration: 4370 loss: 0.0000000080 time: 0.3454258441925049\n",
      "Iteration: 4380 loss: 0.0000000079 time: 0.35062456130981445\n",
      "Iteration: 4390 loss: 0.0000000079 time: 0.3484940528869629\n",
      "Iteration: 4400 loss: 0.0000000078 time: 0.37085700035095215\n",
      "Iteration: 4410 loss: 0.0000000078 time: 0.3451669216156006\n",
      "Iteration: 4420 loss: 0.0000000077 time: 0.35770344734191895\n",
      "Iteration: 4430 loss: 0.0000000077 time: 0.34707140922546387\n",
      "Iteration: 4440 loss: 0.0000000077 time: 0.344646692276001\n",
      "Iteration: 4450 loss: 0.0000000076 time: 0.34940481185913086\n",
      "Iteration: 4460 loss: 0.0000000076 time: 0.3460867404937744\n",
      "Iteration: 4470 loss: 0.0000000075 time: 0.3400399684906006\n",
      "Iteration: 4480 loss: 0.0000000075 time: 0.34727001190185547\n",
      "Iteration: 4490 loss: 0.0000000074 time: 0.34135961532592773\n",
      "Iteration: 4500 loss: 0.0000000074 time: 0.3323955535888672\n",
      "Iteration: 4510 loss: 0.0000000074 time: 0.33281970024108887\n",
      "Iteration: 4520 loss: 0.0000000073 time: 0.3445000648498535\n",
      "Iteration: 4530 loss: 0.0000000073 time: 0.32785582542419434\n",
      "Iteration: 4540 loss: 0.0000000072 time: 0.3405494689941406\n",
      "Iteration: 4550 loss: 0.0000000072 time: 0.3200376033782959\n",
      "Iteration: 4560 loss: 0.0000000072 time: 0.3128199577331543\n",
      "Iteration: 4570 loss: 0.0000000071 time: 0.31760239601135254\n",
      "Iteration: 4580 loss: 0.0000000071 time: 0.3199014663696289\n",
      "Iteration: 4590 loss: 0.0000000071 time: 0.3175215721130371\n",
      "Iteration: 4600 loss: 0.0000000070 time: 0.32297682762145996\n",
      "Iteration: 4610 loss: 0.0000000070 time: 0.3276815414428711\n",
      "Iteration: 4620 loss: 0.0000000070 time: 0.32918524742126465\n",
      "Iteration: 4630 loss: 0.0000000069 time: 0.3162879943847656\n",
      "Iteration: 4640 loss: 0.0000000069 time: 0.3194394111633301\n",
      "Iteration: 4650 loss: 0.0000000069 time: 0.32128119468688965\n",
      "Iteration: 4660 loss: 0.0000000068 time: 0.32117509841918945\n",
      "Iteration: 4670 loss: 0.0000000068 time: 0.32109785079956055\n",
      "Iteration: 4680 loss: 0.0000000068 time: 0.35062503814697266\n",
      "Iteration: 4690 loss: 0.0000000067 time: 0.318648099899292\n",
      "Iteration: 4700 loss: 0.0000000067 time: 0.324753999710083\n",
      "Iteration: 4710 loss: 0.0000000067 time: 0.32483744621276855\n",
      "Iteration: 4720 loss: 0.0000000066 time: 0.32941174507141113\n",
      "Iteration: 4730 loss: 0.0000000066 time: 0.3253483772277832\n",
      "Iteration: 4740 loss: 0.0000000066 time: 0.32225656509399414\n",
      "Iteration: 4750 loss: 0.0000000065 time: 0.3393404483795166\n",
      "Iteration: 4760 loss: 0.0000000065 time: 0.3252882957458496\n",
      "Iteration: 4770 loss: 0.0000000065 time: 0.3239102363586426\n",
      "Iteration: 4780 loss: 0.0000000064 time: 0.32566356658935547\n",
      "Iteration: 4790 loss: 0.0000000064 time: 0.32488131523132324\n",
      "Iteration: 4800 loss: 0.0000000064 time: 0.31770944595336914\n",
      "Iteration: 4810 loss: 0.0000000064 time: 0.33361220359802246\n",
      "Iteration: 4820 loss: 0.0000000063 time: 0.32479286193847656\n",
      "Iteration: 4830 loss: 0.0000000063 time: 0.32184553146362305\n",
      "Iteration: 4840 loss: 0.0000000063 time: 0.3256208896636963\n",
      "Iteration: 4850 loss: 0.0000000062 time: 0.3233633041381836\n",
      "Iteration: 4860 loss: 0.0000000062 time: 0.32349181175231934\n",
      "Iteration: 4870 loss: 0.0000000062 time: 0.3190324306488037\n",
      "Iteration: 4880 loss: 0.0000000062 time: 0.32724905014038086\n",
      "Iteration: 4890 loss: 0.0000000061 time: 0.323436975479126\n",
      "Iteration: 4900 loss: 0.0000000061 time: 0.328967809677124\n",
      "Iteration: 4910 loss: 0.0000000061 time: 0.31238627433776855\n",
      "Iteration: 4920 loss: 0.0000000061 time: 0.32488417625427246\n",
      "Iteration: 4930 loss: 0.0000000060 time: 0.3310394287109375\n",
      "Iteration: 4940 loss: 0.0000000060 time: 0.31851911544799805\n",
      "Iteration: 4950 loss: 0.0000000060 time: 0.3321874141693115\n",
      "Iteration: 4960 loss: 0.0000000060 time: 0.32071614265441895\n",
      "Iteration: 4970 loss: 0.0000000059 time: 0.3225281238555908\n",
      "Iteration: 4980 loss: 0.0000000059 time: 0.31742429733276367\n",
      "Iteration: 4990 loss: 0.0000000059 time: 0.3229494094848633\n",
      "Iteration: 5000 loss: 0.0000000059 time: 0.32116055488586426\n",
      "Iteration: 5010 loss: 0.0000000058 time: 0.3331258296966553\n",
      "Iteration: 5020 loss: 0.0000000060 time: 0.32123470306396484\n",
      "Iteration: 5030 loss: 0.0000000580 time: 0.3252427577972412\n",
      "Iteration: 5040 loss: 0.0000000071 time: 0.31622838973999023\n",
      "Iteration: 5050 loss: 0.0000000061 time: 0.3173809051513672\n",
      "Iteration: 5060 loss: 0.0000000060 time: 0.32097840309143066\n",
      "Iteration: 5070 loss: 0.0000000059 time: 0.32566118240356445\n",
      "Iteration: 5080 loss: 0.0000000058 time: 0.3325948715209961\n",
      "Iteration: 5090 loss: 0.0000000057 time: 0.31967806816101074\n",
      "Iteration: 5100 loss: 0.0000000057 time: 0.32245802879333496\n",
      "Iteration: 5110 loss: 0.0000000056 time: 0.3171839714050293\n",
      "Iteration: 5120 loss: 0.0000000056 time: 0.3237488269805908\n",
      "Iteration: 5130 loss: 0.0000000056 time: 0.31981921195983887\n",
      "Iteration: 5140 loss: 0.0000000056 time: 0.3254969120025635\n",
      "Iteration: 5150 loss: 0.0000000056 time: 0.34306859970092773\n",
      "Iteration: 5160 loss: 0.0000000055 time: 0.31594157218933105\n",
      "Iteration: 5170 loss: 0.0000000055 time: 0.32215237617492676\n",
      "Iteration: 5180 loss: 0.0000000055 time: 0.3208334445953369\n",
      "Iteration: 5190 loss: 0.0000000055 time: 0.3268704414367676\n",
      "Iteration: 5200 loss: 0.0000000055 time: 0.32134509086608887\n",
      "Iteration: 5210 loss: 0.0000000054 time: 0.33469414710998535\n",
      "Iteration: 5220 loss: 0.0000000054 time: 0.32186079025268555\n",
      "Iteration: 5230 loss: 0.0000000054 time: 0.32178258895874023\n",
      "Iteration: 5240 loss: 0.0000000054 time: 0.3222951889038086\n",
      "Iteration: 5250 loss: 0.0000000054 time: 0.3228473663330078\n",
      "Iteration: 5260 loss: 0.0000000054 time: 0.3185577392578125\n",
      "Iteration: 5270 loss: 0.0000000053 time: 0.32198333740234375\n",
      "Iteration: 5280 loss: 0.0000000053 time: 0.32805728912353516\n",
      "Iteration: 5290 loss: 0.0000000053 time: 0.32268190383911133\n",
      "Iteration: 5300 loss: 0.0000000053 time: 0.3190758228302002\n",
      "Iteration: 5310 loss: 0.0000000053 time: 0.3191823959350586\n",
      "Iteration: 5320 loss: 0.0000000052 time: 0.32172584533691406\n",
      "Iteration: 5330 loss: 0.0000000052 time: 0.3301231861114502\n",
      "Iteration: 5340 loss: 0.0000000052 time: 0.3238708972930908\n",
      "Iteration: 5350 loss: 0.0000000052 time: 0.3304331302642822\n",
      "Iteration: 5360 loss: 0.0000000052 time: 0.3155384063720703\n",
      "Iteration: 5370 loss: 0.0000000052 time: 0.3243675231933594\n",
      "Iteration: 5380 loss: 0.0000000051 time: 0.3201925754547119\n",
      "Iteration: 5390 loss: 0.0000000051 time: 0.3180572986602783\n",
      "Iteration: 5400 loss: 0.0000000051 time: 0.3276834487915039\n",
      "Iteration: 5410 loss: 0.0000000051 time: 0.3335907459259033\n",
      "Iteration: 5420 loss: 0.0000000051 time: 0.3221907615661621\n",
      "Iteration: 5430 loss: 0.0000000051 time: 0.32505106925964355\n",
      "Iteration: 5440 loss: 0.0000000051 time: 0.32810163497924805\n",
      "Iteration: 5450 loss: 0.0000000050 time: 0.3244493007659912\n",
      "Iteration: 5460 loss: 0.0000000050 time: 0.3236539363861084\n",
      "Iteration: 5470 loss: 0.0000000050 time: 0.3294258117675781\n",
      "Iteration: 5480 loss: 0.0000000050 time: 0.3300209045410156\n",
      "Iteration: 5490 loss: 0.0000000050 time: 0.33052682876586914\n",
      "Iteration: 5500 loss: 0.0000000050 time: 0.325667142868042\n",
      "Iteration: 5510 loss: 0.0000000049 time: 0.32547879219055176\n",
      "Iteration: 5520 loss: 0.0000000049 time: 0.31932997703552246\n",
      "Iteration: 5530 loss: 0.0000000050 time: 0.3238668441772461\n",
      "Iteration: 5540 loss: 0.0000000166 time: 0.324779748916626\n",
      "Iteration: 5550 loss: 0.0000000224 time: 0.32887721061706543\n",
      "Iteration: 5560 loss: 0.0000000057 time: 0.321000337600708\n",
      "Iteration: 5570 loss: 0.0000000049 time: 0.3181784152984619\n",
      "Iteration: 5580 loss: 0.0000000049 time: 0.3213484287261963\n",
      "Iteration: 5590 loss: 0.0000000048 time: 0.32286524772644043\n",
      "Iteration: 5600 loss: 0.0000000048 time: 0.3190324306488037\n",
      "Iteration: 5610 loss: 0.0000000048 time: 0.336991548538208\n",
      "Iteration: 5620 loss: 0.0000000048 time: 0.3232858180999756\n",
      "Iteration: 5630 loss: 0.0000000048 time: 0.31756114959716797\n",
      "Iteration: 5640 loss: 0.0000000048 time: 0.327669620513916\n",
      "Iteration: 5650 loss: 0.0000000048 time: 0.31723618507385254\n",
      "Iteration: 5660 loss: 0.0000000047 time: 0.3315010070800781\n",
      "Iteration: 5670 loss: 0.0000000047 time: 0.32502317428588867\n",
      "Iteration: 5680 loss: 0.0000000047 time: 0.34209132194519043\n",
      "Iteration: 5690 loss: 0.0000000047 time: 0.31849074363708496\n",
      "Iteration: 5700 loss: 0.0000000047 time: 0.32717370986938477\n",
      "Iteration: 5710 loss: 0.0000000047 time: 0.3253605365753174\n",
      "Iteration: 5720 loss: 0.0000000047 time: 0.31938695907592773\n",
      "Iteration: 5730 loss: 0.0000000047 time: 0.3213496208190918\n",
      "Iteration: 5740 loss: 0.0000000046 time: 0.32732176780700684\n",
      "Iteration: 5750 loss: 0.0000000046 time: 0.33121538162231445\n",
      "Iteration: 5760 loss: 0.0000000046 time: 0.31893348693847656\n",
      "Iteration: 5770 loss: 0.0000000046 time: 0.32732605934143066\n",
      "Iteration: 5780 loss: 0.0000000046 time: 0.323244571685791\n",
      "Iteration: 5790 loss: 0.0000000046 time: 0.31514525413513184\n",
      "Iteration: 5800 loss: 0.0000000046 time: 0.3206970691680908\n",
      "Iteration: 5810 loss: 0.0000000046 time: 0.3335845470428467\n",
      "Iteration: 5820 loss: 0.0000000046 time: 0.31889796257019043\n",
      "Iteration: 5830 loss: 0.0000000045 time: 0.31984615325927734\n",
      "Iteration: 5840 loss: 0.0000000045 time: 0.32226109504699707\n",
      "Iteration: 5850 loss: 0.0000000045 time: 0.3183553218841553\n",
      "Iteration: 5860 loss: 0.0000000045 time: 0.32379579544067383\n",
      "Iteration: 5870 loss: 0.0000000045 time: 0.31745290756225586\n",
      "Iteration: 5880 loss: 0.0000000045 time: 0.3314781188964844\n",
      "Iteration: 5890 loss: 0.0000000045 time: 0.3231651782989502\n",
      "Iteration: 5900 loss: 0.0000000045 time: 0.32013893127441406\n",
      "Iteration: 5910 loss: 0.0000000045 time: 0.32039594650268555\n",
      "Iteration: 5920 loss: 0.0000000044 time: 0.3196883201599121\n",
      "Iteration: 5930 loss: 0.0000000044 time: 0.3243372440338135\n",
      "Iteration: 5940 loss: 0.0000000044 time: 0.32939767837524414\n",
      "Iteration: 5950 loss: 0.0000000044 time: 0.34455227851867676\n",
      "Iteration: 5960 loss: 0.0000000044 time: 0.3172624111175537\n",
      "Iteration: 5970 loss: 0.0000000044 time: 0.31738710403442383\n",
      "Iteration: 5980 loss: 0.0000000044 time: 0.3211381435394287\n",
      "Iteration: 5990 loss: 0.0000000044 time: 0.3221752643585205\n",
      "Iteration: 6000 loss: 0.0000000044 time: 0.3205757141113281\n",
      "Iteration: 6010 loss: 0.0000000044 time: 0.3394317626953125\n",
      "Iteration: 6020 loss: 0.0000000203 time: 0.31850457191467285\n",
      "Iteration: 6030 loss: 0.0000000267 time: 0.31972789764404297\n",
      "Iteration: 6040 loss: 0.0000000060 time: 0.3173072338104248\n",
      "Iteration: 6050 loss: 0.0000000045 time: 0.32059764862060547\n",
      "Iteration: 6060 loss: 0.0000000043 time: 0.3183274269104004\n",
      "Iteration: 6070 loss: 0.0000000043 time: 0.31883978843688965\n",
      "Iteration: 6080 loss: 0.0000000043 time: 0.3383796215057373\n",
      "Iteration: 6090 loss: 0.0000000043 time: 0.31635260581970215\n",
      "Iteration: 6100 loss: 0.0000000043 time: 0.31766366958618164\n",
      "Iteration: 6110 loss: 0.0000000043 time: 0.3226199150085449\n",
      "Iteration: 6120 loss: 0.0000000042 time: 0.3194146156311035\n",
      "Iteration: 6130 loss: 0.0000000042 time: 0.31995201110839844\n",
      "Iteration: 6140 loss: 0.0000000042 time: 0.3413686752319336\n",
      "Iteration: 6150 loss: 0.0000000042 time: 0.3268129825592041\n",
      "Iteration: 6160 loss: 0.0000000042 time: 0.3236830234527588\n",
      "Iteration: 6170 loss: 0.0000000042 time: 0.3251359462738037\n",
      "Iteration: 6180 loss: 0.0000000042 time: 0.31995129585266113\n",
      "Iteration: 6190 loss: 0.0000000042 time: 0.3269188404083252\n",
      "Iteration: 6200 loss: 0.0000000042 time: 0.3217658996582031\n",
      "Iteration: 6210 loss: 0.0000000042 time: 0.33625078201293945\n",
      "Iteration: 6220 loss: 0.0000000042 time: 0.32402515411376953\n",
      "Iteration: 6230 loss: 0.0000000041 time: 0.3235044479370117\n",
      "Iteration: 6240 loss: 0.0000000041 time: 0.3268756866455078\n",
      "Iteration: 6250 loss: 0.0000000041 time: 0.31989145278930664\n",
      "Iteration: 6260 loss: 0.0000000041 time: 0.31905031204223633\n",
      "Iteration: 6270 loss: 0.0000000041 time: 0.3278048038482666\n",
      "Iteration: 6280 loss: 0.0000000041 time: 0.33214378356933594\n",
      "Iteration: 6290 loss: 0.0000000041 time: 0.3186185359954834\n",
      "Iteration: 6300 loss: 0.0000000041 time: 0.31760334968566895\n",
      "Iteration: 6310 loss: 0.0000000041 time: 0.31923890113830566\n",
      "Iteration: 6320 loss: 0.0000000041 time: 0.3196229934692383\n",
      "Iteration: 6330 loss: 0.0000000041 time: 0.3240935802459717\n",
      "Iteration: 6340 loss: 0.0000000041 time: 0.3235764503479004\n",
      "Iteration: 6350 loss: 0.0000000040 time: 0.33275938034057617\n",
      "Iteration: 6360 loss: 0.0000000040 time: 0.3211989402770996\n",
      "Iteration: 6370 loss: 0.0000000040 time: 0.3179619312286377\n",
      "Iteration: 6380 loss: 0.0000000040 time: 0.3147611618041992\n",
      "Iteration: 6390 loss: 0.0000000050 time: 0.3332698345184326\n",
      "Iteration: 6400 loss: 0.0000000268 time: 0.32260847091674805\n",
      "Iteration: 6410 loss: 0.0000000042 time: 0.3361639976501465\n",
      "Iteration: 6420 loss: 0.0000000057 time: 0.3207361698150635\n",
      "Iteration: 6430 loss: 0.0000000048 time: 0.31698083877563477\n",
      "Iteration: 6440 loss: 0.0000000040 time: 0.3269782066345215\n",
      "Iteration: 6450 loss: 0.0000000041 time: 0.32581281661987305\n",
      "Iteration: 6460 loss: 0.0000000040 time: 0.3334953784942627\n",
      "Iteration: 6470 loss: 0.0000000040 time: 0.29358506202697754\n",
      "Iteration: 6480 loss: 0.0000000040 time: 0.3201417922973633\n",
      "Iteration: 6490 loss: 0.0000000039 time: 0.30668187141418457\n",
      "Iteration: 6500 loss: 0.0000000039 time: 0.31891489028930664\n",
      "Iteration: 6510 loss: 0.0000000039 time: 0.3206167221069336\n",
      "Iteration: 6520 loss: 0.0000000042 time: 0.3156700134277344\n",
      "Iteration: 6530 loss: 0.0000000112 time: 0.316601037979126\n",
      "Iteration: 6540 loss: 0.0000000098 time: 0.32901859283447266\n",
      "Iteration: 6550 loss: 0.0000000042 time: 0.3333778381347656\n",
      "Iteration: 6560 loss: 0.0000000042 time: 0.3275470733642578\n",
      "Iteration: 6570 loss: 0.0000000044 time: 0.32018446922302246\n",
      "Iteration: 6580 loss: 0.0000000041 time: 0.32321786880493164\n",
      "Iteration: 6590 loss: 0.0000000040 time: 0.3128793239593506\n",
      "Iteration: 6600 loss: 0.0000000039 time: 0.3220083713531494\n",
      "Iteration: 6610 loss: 0.0000000039 time: 0.3336355686187744\n",
      "Iteration: 6620 loss: 0.0000000039 time: 0.3214743137359619\n",
      "Iteration: 6630 loss: 0.0000000038 time: 0.32088160514831543\n",
      "Iteration: 6640 loss: 0.0000000038 time: 0.32353687286376953\n",
      "Iteration: 6650 loss: 0.0000000038 time: 0.3160111904144287\n",
      "Iteration: 6660 loss: 0.0000000038 time: 0.31984925270080566\n",
      "Iteration: 6670 loss: 0.0000000038 time: 0.3183152675628662\n",
      "Iteration: 6680 loss: 0.0000000038 time: 0.3290085792541504\n",
      "Iteration: 6690 loss: 0.0000000038 time: 0.323991060256958\n",
      "Iteration: 6700 loss: 0.0000000038 time: 0.321758508682251\n",
      "Iteration: 6710 loss: 0.0000000038 time: 0.32139134407043457\n",
      "Iteration: 6720 loss: 0.0000000038 time: 0.3209419250488281\n",
      "Iteration: 6730 loss: 0.0000000038 time: 0.31735920906066895\n",
      "Iteration: 6740 loss: 0.0000000038 time: 0.31671142578125\n",
      "Iteration: 6750 loss: 0.0000000049 time: 0.33478593826293945\n",
      "Iteration: 6760 loss: 0.0000000384 time: 0.32988405227661133\n",
      "Iteration: 6770 loss: 0.0000000173 time: 0.3235650062561035\n",
      "Iteration: 6780 loss: 0.0000000050 time: 0.3212578296661377\n",
      "Iteration: 6790 loss: 0.0000000046 time: 0.32011985778808594\n",
      "Iteration: 6800 loss: 0.0000000042 time: 0.32636594772338867\n",
      "Iteration: 6810 loss: 0.0000000037 time: 0.33138179779052734\n",
      "Iteration: 6820 loss: 0.0000000038 time: 0.3196537494659424\n",
      "Iteration: 6830 loss: 0.0000000037 time: 0.33471179008483887\n",
      "Iteration: 6840 loss: 0.0000000037 time: 0.32226991653442383\n",
      "Iteration: 6850 loss: 0.0000000037 time: 0.31860804557800293\n",
      "Iteration: 6860 loss: 0.0000000037 time: 0.32170748710632324\n",
      "Iteration: 6870 loss: 0.0000000037 time: 0.3145267963409424\n",
      "Iteration: 6880 loss: 0.0000000037 time: 0.33944129943847656\n",
      "Iteration: 6890 loss: 0.0000000037 time: 0.3251521587371826\n",
      "Iteration: 6900 loss: 0.0000000037 time: 0.3260221481323242\n",
      "Iteration: 6910 loss: 0.0000000037 time: 0.32466912269592285\n",
      "Iteration: 6920 loss: 0.0000000037 time: 0.3252577781677246\n",
      "Iteration: 6930 loss: 0.0000000037 time: 0.32430028915405273\n",
      "Iteration: 6940 loss: 0.0000000036 time: 0.32080078125\n",
      "Iteration: 6950 loss: 0.0000000037 time: 0.33335423469543457\n",
      "Iteration: 6960 loss: 0.0000000044 time: 0.31912875175476074\n",
      "Iteration: 6970 loss: 0.0000000204 time: 0.3269658088684082\n",
      "Iteration: 6980 loss: 0.0000000097 time: 0.3203122615814209\n",
      "Iteration: 6990 loss: 0.0000000047 time: 0.32494521141052246\n",
      "Iteration: 7000 loss: 0.0000000037 time: 0.3246760368347168\n",
      "Iteration: 7010 loss: 0.0000000038 time: 0.33715271949768066\n",
      "Iteration: 7020 loss: 0.0000000037 time: 0.3266446590423584\n",
      "Iteration: 7030 loss: 0.0000000036 time: 0.32625842094421387\n",
      "Iteration: 7040 loss: 0.0000000037 time: 0.3197915554046631\n",
      "Iteration: 7050 loss: 0.0000000047 time: 0.3218822479248047\n",
      "Iteration: 7060 loss: 0.0000000164 time: 0.3190443515777588\n",
      "Iteration: 7070 loss: 0.0000000036 time: 0.3274674415588379\n",
      "Iteration: 7080 loss: 0.0000000037 time: 0.333881139755249\n",
      "Iteration: 7090 loss: 0.0000000037 time: 0.3219566345214844\n",
      "Iteration: 7100 loss: 0.0000000037 time: 0.32092952728271484\n",
      "Iteration: 7110 loss: 0.0000000037 time: 0.3285250663757324\n",
      "Iteration: 7120 loss: 0.0000000036 time: 0.3285641670227051\n",
      "Iteration: 7130 loss: 0.0000000035 time: 0.319317102432251\n",
      "Iteration: 7140 loss: 0.0000000035 time: 0.3299376964569092\n",
      "Iteration: 7150 loss: 0.0000000035 time: 0.3261451721191406\n",
      "Iteration: 7160 loss: 0.0000000035 time: 0.3205416202545166\n",
      "Iteration: 7170 loss: 0.0000000035 time: 0.32567310333251953\n",
      "Iteration: 7180 loss: 0.0000000035 time: 0.32072925567626953\n",
      "Iteration: 7190 loss: 0.0000000035 time: 0.32068753242492676\n",
      "Iteration: 7200 loss: 0.0000000035 time: 0.3233351707458496\n",
      "Iteration: 7210 loss: 0.0000000035 time: 0.3302128314971924\n",
      "Iteration: 7220 loss: 0.0000000035 time: 0.32453298568725586\n",
      "Iteration: 7230 loss: 0.0000000035 time: 0.33183860778808594\n",
      "Iteration: 7240 loss: 0.0000000080 time: 0.31787967681884766\n",
      "Iteration: 7250 loss: 0.0000000294 time: 0.3234426975250244\n",
      "Iteration: 7260 loss: 0.0000000057 time: 0.32308316230773926\n",
      "Iteration: 7270 loss: 0.0000000065 time: 0.31992435455322266\n",
      "Iteration: 7280 loss: 0.0000000044 time: 0.3346831798553467\n",
      "Iteration: 7290 loss: 0.0000000036 time: 0.3201630115509033\n",
      "Iteration: 7300 loss: 0.0000000036 time: 0.3319864273071289\n",
      "Iteration: 7310 loss: 0.0000000035 time: 0.3286004066467285\n",
      "Iteration: 7320 loss: 0.0000000035 time: 0.31810593605041504\n",
      "Iteration: 7330 loss: 0.0000000034 time: 0.31972599029541016\n",
      "Iteration: 7340 loss: 0.0000000034 time: 0.32485318183898926\n",
      "Iteration: 7350 loss: 0.0000000034 time: 0.3346865177154541\n",
      "Iteration: 7360 loss: 0.0000000034 time: 0.3220958709716797\n",
      "Iteration: 7370 loss: 0.0000000034 time: 0.3177807331085205\n",
      "Iteration: 7380 loss: 0.0000000034 time: 0.3260340690612793\n",
      "Iteration: 7390 loss: 0.0000000034 time: 0.3246140480041504\n",
      "Iteration: 7400 loss: 0.0000000034 time: 0.3175222873687744\n",
      "Iteration: 7410 loss: 0.0000000034 time: 0.33594655990600586\n",
      "Iteration: 7420 loss: 0.0000000034 time: 0.325817346572876\n",
      "Iteration: 7430 loss: 0.0000000036 time: 0.31696009635925293\n",
      "Iteration: 7440 loss: 0.0000000116 time: 0.32303857803344727\n",
      "Iteration: 7450 loss: 0.0000000036 time: 0.3243870735168457\n",
      "Iteration: 7460 loss: 0.0000000055 time: 0.31706976890563965\n",
      "Iteration: 7470 loss: 0.0000000043 time: 0.32102298736572266\n",
      "Iteration: 7480 loss: 0.0000000034 time: 0.3305933475494385\n",
      "Iteration: 7490 loss: 0.0000000034 time: 0.31940579414367676\n",
      "Iteration: 7500 loss: 0.0000000034 time: 0.32344889640808105\n",
      "Iteration: 7510 loss: 0.0000000034 time: 0.3245053291320801\n",
      "Iteration: 7520 loss: 0.0000000034 time: 0.32171010971069336\n",
      "Iteration: 7530 loss: 0.0000000033 time: 0.33038949966430664\n",
      "Iteration: 7540 loss: 0.0000000033 time: 0.34103965759277344\n",
      "Iteration: 7550 loss: 0.0000000033 time: 0.3197619915008545\n",
      "Iteration: 7560 loss: 0.0000000033 time: 0.32451295852661133\n",
      "Iteration: 7570 loss: 0.0000000033 time: 0.3217785358428955\n",
      "Iteration: 7580 loss: 0.0000000033 time: 0.32493090629577637\n",
      "Iteration: 7590 loss: 0.0000000033 time: 0.32305049896240234\n",
      "Iteration: 7600 loss: 0.0000000033 time: 0.3150336742401123\n",
      "Iteration: 7610 loss: 0.0000000041 time: 0.3297410011291504\n",
      "Iteration: 7620 loss: 0.0000000384 time: 0.3294854164123535\n",
      "Iteration: 7630 loss: 0.0000000191 time: 0.3217732906341553\n",
      "Iteration: 7640 loss: 0.0000000044 time: 0.31944966316223145\n",
      "Iteration: 7650 loss: 0.0000000049 time: 0.32942771911621094\n",
      "Iteration: 7660 loss: 0.0000000035 time: 0.3498077392578125\n",
      "Iteration: 7670 loss: 0.0000000034 time: 0.36939215660095215\n",
      "Iteration: 7680 loss: 0.0000000034 time: 0.37293553352355957\n",
      "Iteration: 7690 loss: 0.0000000033 time: 0.31314849853515625\n",
      "Iteration: 7700 loss: 0.0000000033 time: 0.2889080047607422\n",
      "Iteration: 7710 loss: 0.0000000033 time: 0.29273080825805664\n",
      "Iteration: 7720 loss: 0.0000000033 time: 0.31358933448791504\n",
      "Iteration: 7730 loss: 0.0000000033 time: 0.3417332172393799\n",
      "Iteration: 7740 loss: 0.0000000033 time: 0.3516104221343994\n",
      "Iteration: 7750 loss: 0.0000000033 time: 0.3216094970703125\n",
      "Iteration: 7760 loss: 0.0000000032 time: 0.31807565689086914\n",
      "Iteration: 7770 loss: 0.0000000032 time: 0.32602763175964355\n",
      "Iteration: 7780 loss: 0.0000000032 time: 0.30631256103515625\n",
      "Iteration: 7790 loss: 0.0000000032 time: 0.3209223747253418\n",
      "Iteration: 7800 loss: 0.0000000032 time: 0.2770051956176758\n",
      "Iteration: 7810 loss: 0.0000000032 time: 0.28317689895629883\n",
      "Iteration: 7820 loss: 0.0000000034 time: 0.292360782623291\n",
      "Iteration: 7830 loss: 0.0000000110 time: 0.30234527587890625\n",
      "Iteration: 7840 loss: 0.0000000042 time: 0.3564126491546631\n",
      "Iteration: 7850 loss: 0.0000000059 time: 0.33097290992736816\n",
      "Iteration: 7860 loss: 0.0000000043 time: 0.3372616767883301\n",
      "Iteration: 7870 loss: 0.0000000032 time: 0.33638644218444824\n",
      "Iteration: 7880 loss: 0.0000000034 time: 0.3328399658203125\n",
      "Iteration: 7890 loss: 0.0000000032 time: 0.3184623718261719\n",
      "Iteration: 7900 loss: 0.0000000032 time: 0.32043004035949707\n",
      "Iteration: 7910 loss: 0.0000000032 time: 0.31508779525756836\n",
      "Iteration: 7920 loss: 0.0000000032 time: 0.3158538341522217\n",
      "Iteration: 7930 loss: 0.0000000032 time: 0.313647985458374\n",
      "Iteration: 7940 loss: 0.0000000034 time: 0.32438015937805176\n",
      "Iteration: 7950 loss: 0.0000000074 time: 0.32735586166381836\n",
      "Iteration: 7960 loss: 0.0000000193 time: 0.3132352828979492\n",
      "Iteration: 7970 loss: 0.0000000086 time: 0.3277413845062256\n",
      "Iteration: 7980 loss: 0.0000000050 time: 0.3272414207458496\n",
      "Iteration: 7990 loss: 0.0000000038 time: 0.3160562515258789\n",
      "Iteration: 8000 loss: 0.0000000034 time: 0.3252263069152832\n",
      "Iteration: 8010 loss: 0.0000000032 time: 0.33162522315979004\n",
      "Iteration: 8020 loss: 0.0000000032 time: 0.32590603828430176\n",
      "Iteration: 8030 loss: 0.0000000031 time: 0.33104777336120605\n",
      "Iteration: 8040 loss: 0.0000000031 time: 0.3092343807220459\n",
      "Iteration: 8050 loss: 0.0000000031 time: 0.32756495475769043\n",
      "Iteration: 8060 loss: 0.0000000031 time: 0.3119027614593506\n",
      "Iteration: 8070 loss: 0.0000000031 time: 0.3045227527618408\n",
      "Iteration: 8080 loss: 0.0000000031 time: 0.3389873504638672\n",
      "Iteration: 8090 loss: 0.0000000031 time: 0.3399665355682373\n",
      "Iteration: 8100 loss: 0.0000000031 time: 0.3438096046447754\n",
      "Iteration: 8110 loss: 0.0000000031 time: 0.32143378257751465\n",
      "Iteration: 8120 loss: 0.0000000031 time: 0.30652952194213867\n",
      "Iteration: 8130 loss: 0.0000000037 time: 0.3220961093902588\n",
      "Iteration: 8140 loss: 0.0000000227 time: 0.32091307640075684\n",
      "Iteration: 8150 loss: 0.0000000055 time: 0.33690953254699707\n",
      "Iteration: 8160 loss: 0.0000000077 time: 0.3094820976257324\n",
      "Iteration: 8170 loss: 0.0000000046 time: 0.32533812522888184\n",
      "Iteration: 8180 loss: 0.0000000031 time: 0.3342013359069824\n",
      "Iteration: 8190 loss: 0.0000000032 time: 0.33646678924560547\n",
      "Iteration: 8200 loss: 0.0000000032 time: 0.3271169662475586\n",
      "Iteration: 8210 loss: 0.0000000031 time: 0.3342289924621582\n",
      "Iteration: 8220 loss: 0.0000000031 time: 0.31359171867370605\n",
      "Iteration: 8230 loss: 0.0000000031 time: 0.30925869941711426\n",
      "Iteration: 8240 loss: 0.0000000031 time: 0.31410646438598633\n",
      "Iteration: 8250 loss: 0.0000000031 time: 0.30568981170654297\n",
      "Iteration: 8260 loss: 0.0000000031 time: 0.3151214122772217\n",
      "Iteration: 8270 loss: 0.0000000032 time: 0.3107750415802002\n",
      "Iteration: 8280 loss: 0.0000000057 time: 0.3301887512207031\n",
      "Iteration: 8290 loss: 0.0000000135 time: 0.30881476402282715\n",
      "Iteration: 8300 loss: 0.0000000055 time: 0.31032395362854004\n",
      "Iteration: 8310 loss: 0.0000000032 time: 0.3077507019042969\n",
      "Iteration: 8320 loss: 0.0000000031 time: 0.31963181495666504\n",
      "Iteration: 8330 loss: 0.0000000031 time: 0.3239095211029053\n",
      "Iteration: 8340 loss: 0.0000000031 time: 0.31535935401916504\n",
      "Iteration: 8350 loss: 0.0000000031 time: 0.341259241104126\n",
      "Iteration: 8360 loss: 0.0000000030 time: 0.3173704147338867\n",
      "Iteration: 8370 loss: 0.0000000030 time: 0.32123422622680664\n",
      "Iteration: 8380 loss: 0.0000000030 time: 0.3104543685913086\n",
      "Iteration: 8390 loss: 0.0000000030 time: 0.30846691131591797\n",
      "Iteration: 8400 loss: 0.0000000030 time: 0.31331920623779297\n",
      "Iteration: 8410 loss: 0.0000000033 time: 0.3092200756072998\n",
      "Iteration: 8420 loss: 0.0000000116 time: 0.32344484329223633\n",
      "Iteration: 8430 loss: 0.0000000083 time: 0.3268742561340332\n",
      "Iteration: 8440 loss: 0.0000000035 time: 0.3326878547668457\n",
      "Iteration: 8450 loss: 0.0000000035 time: 0.32013845443725586\n",
      "Iteration: 8460 loss: 0.0000000036 time: 0.33150339126586914\n",
      "Iteration: 8470 loss: 0.0000000033 time: 0.33115434646606445\n",
      "Iteration: 8480 loss: 0.0000000031 time: 0.33898305892944336\n",
      "Iteration: 8490 loss: 0.0000000030 time: 0.3130810260772705\n",
      "Iteration: 8500 loss: 0.0000000030 time: 0.33118152618408203\n",
      "Iteration: 8510 loss: 0.0000000030 time: 0.31702494621276855\n",
      "Iteration: 8520 loss: 0.0000000030 time: 0.33904051780700684\n",
      "Iteration: 8530 loss: 0.0000000030 time: 0.33289051055908203\n",
      "Iteration: 8540 loss: 0.0000000030 time: 0.3182845115661621\n",
      "Iteration: 8550 loss: 0.0000000030 time: 0.36264729499816895\n",
      "Iteration: 8560 loss: 0.0000000030 time: 0.3304002285003662\n",
      "Iteration: 8570 loss: 0.0000000030 time: 0.31369566917419434\n",
      "Iteration: 8580 loss: 0.0000000030 time: 0.32430171966552734\n",
      "Iteration: 8590 loss: 0.0000000033 time: 0.3098573684692383\n",
      "Iteration: 8600 loss: 0.0000000150 time: 0.314422607421875\n",
      "Iteration: 8610 loss: 0.0000000070 time: 0.32101917266845703\n",
      "Iteration: 8620 loss: 0.0000000060 time: 0.31441211700439453\n",
      "Iteration: 8630 loss: 0.0000000037 time: 0.3182685375213623\n",
      "Iteration: 8640 loss: 0.0000000030 time: 0.3347761631011963\n",
      "Iteration: 8650 loss: 0.0000000031 time: 0.3428781032562256\n",
      "Iteration: 8660 loss: 0.0000000030 time: 0.337984561920166\n",
      "Iteration: 8670 loss: 0.0000000030 time: 0.3076608180999756\n",
      "Iteration: 8680 loss: 0.0000000031 time: 0.33038973808288574\n",
      "Iteration: 8690 loss: 0.0000000046 time: 0.3271982669830322\n",
      "Iteration: 8700 loss: 0.0000000161 time: 0.31233859062194824\n",
      "Iteration: 8710 loss: 0.0000000032 time: 0.32607245445251465\n",
      "Iteration: 8720 loss: 0.0000000030 time: 0.3110635280609131\n",
      "Iteration: 8730 loss: 0.0000000032 time: 0.3103468418121338\n",
      "Iteration: 8740 loss: 0.0000000032 time: 0.31011271476745605\n",
      "Iteration: 8750 loss: 0.0000000030 time: 0.3231208324432373\n",
      "Iteration: 8760 loss: 0.0000000029 time: 0.32027435302734375\n",
      "Iteration: 8770 loss: 0.0000000029 time: 0.3098256587982178\n",
      "Iteration: 8780 loss: 0.0000000029 time: 0.3092379570007324\n",
      "Iteration: 8790 loss: 0.0000000029 time: 0.3239591121673584\n",
      "Iteration: 8800 loss: 0.0000000029 time: 0.32427191734313965\n",
      "Iteration: 8810 loss: 0.0000000029 time: 0.3223879337310791\n",
      "Iteration: 8820 loss: 0.0000000029 time: 0.3469386100769043\n",
      "Iteration: 8830 loss: 0.0000000030 time: 0.36368608474731445\n",
      "Iteration: 8840 loss: 0.0000000046 time: 0.30438756942749023\n",
      "Iteration: 8850 loss: 0.0000000259 time: 0.3026890754699707\n",
      "Iteration: 8860 loss: 0.0000000085 time: 0.30535364151000977\n",
      "Iteration: 8870 loss: 0.0000000056 time: 0.28174805641174316\n",
      "Iteration: 8880 loss: 0.0000000040 time: 0.27644920349121094\n",
      "Iteration: 8890 loss: 0.0000000032 time: 0.2916114330291748\n",
      "Iteration: 8900 loss: 0.0000000030 time: 0.30765390396118164\n",
      "Iteration: 8910 loss: 0.0000000029 time: 0.31384897232055664\n",
      "Iteration: 8920 loss: 0.0000000029 time: 0.31275200843811035\n",
      "Iteration: 8930 loss: 0.0000000029 time: 0.31458306312561035\n",
      "Iteration: 8940 loss: 0.0000000029 time: 0.33083009719848633\n",
      "Iteration: 8950 loss: 0.0000000028 time: 0.35021305084228516\n",
      "Iteration: 8960 loss: 0.0000000028 time: 0.3199481964111328\n",
      "Iteration: 8970 loss: 0.0000000029 time: 0.3137507438659668\n",
      "Iteration: 8980 loss: 0.0000000035 time: 0.31728148460388184\n",
      "Iteration: 8990 loss: 0.0000000132 time: 0.31156229972839355\n",
      "Iteration: 9000 loss: 0.0000000039 time: 0.3287043571472168\n",
      "Iteration: 9010 loss: 0.0000000034 time: 0.3147470951080322\n",
      "Iteration: 9020 loss: 0.0000000033 time: 0.3326420783996582\n",
      "Iteration: 9030 loss: 0.0000000030 time: 0.3395054340362549\n",
      "Iteration: 9040 loss: 0.0000000029 time: 0.33098435401916504\n",
      "Iteration: 9050 loss: 0.0000000028 time: 0.32959938049316406\n",
      "Iteration: 9060 loss: 0.0000000028 time: 0.3285350799560547\n",
      "Iteration: 9070 loss: 0.0000000029 time: 0.2848939895629883\n",
      "Iteration: 9080 loss: 0.0000000045 time: 0.2984657287597656\n",
      "Iteration: 9090 loss: 0.0000000287 time: 0.31146764755249023\n",
      "Iteration: 9100 loss: 0.0000000104 time: 0.31212854385375977\n",
      "Iteration: 9110 loss: 0.0000000059 time: 0.31875085830688477\n",
      "Iteration: 9120 loss: 0.0000000035 time: 0.3105928897857666\n",
      "Iteration: 9130 loss: 0.0000000029 time: 0.31732773780822754\n",
      "Iteration: 9140 loss: 0.0000000028 time: 0.31821775436401367\n",
      "Iteration: 9150 loss: 0.0000000028 time: 0.32558608055114746\n",
      "Iteration: 9160 loss: 0.0000000028 time: 0.3262302875518799\n",
      "Iteration: 9170 loss: 0.0000000028 time: 0.315288782119751\n",
      "Iteration: 9180 loss: 0.0000000028 time: 0.3212459087371826\n",
      "Iteration: 9190 loss: 0.0000000028 time: 0.308103084564209\n",
      "Iteration: 9200 loss: 0.0000000028 time: 0.3315615653991699\n",
      "Iteration: 9210 loss: 0.0000000028 time: 0.36225318908691406\n",
      "Iteration: 9220 loss: 0.0000000028 time: 0.348552942276001\n",
      "Iteration: 9230 loss: 0.0000000028 time: 0.327772855758667\n",
      "Iteration: 9240 loss: 0.0000000028 time: 0.3245217800140381\n",
      "Iteration: 9250 loss: 0.0000000028 time: 0.3146343231201172\n",
      "Iteration: 9260 loss: 0.0000000028 time: 0.3295004367828369\n",
      "Iteration: 9270 loss: 0.0000000028 time: 0.3532230854034424\n",
      "Iteration: 9280 loss: 0.0000000035 time: 0.34688448905944824\n",
      "Iteration: 9290 loss: 0.0000000282 time: 0.35376858711242676\n",
      "Iteration: 9300 loss: 0.0000000166 time: 0.31462597846984863\n",
      "Iteration: 9310 loss: 0.0000000040 time: 0.33436012268066406\n",
      "Iteration: 9320 loss: 0.0000000041 time: 0.3316378593444824\n",
      "Iteration: 9330 loss: 0.0000000028 time: 0.3515055179595947\n",
      "Iteration: 9340 loss: 0.0000000030 time: 0.32663798332214355\n",
      "Iteration: 9350 loss: 0.0000000027 time: 0.3521444797515869\n",
      "Iteration: 9360 loss: 0.0000000028 time: 0.3521304130554199\n",
      "Iteration: 9370 loss: 0.0000000027 time: 0.34940648078918457\n",
      "Iteration: 9380 loss: 0.0000000027 time: 0.32427549362182617\n",
      "Iteration: 9390 loss: 0.0000000027 time: 0.36696648597717285\n",
      "Iteration: 9400 loss: 0.0000000027 time: 0.3032646179199219\n",
      "Iteration: 9410 loss: 0.0000000029 time: 0.30803370475769043\n",
      "Iteration: 9420 loss: 0.0000000066 time: 0.29205846786499023\n",
      "Iteration: 9430 loss: 0.0000000203 time: 0.31121253967285156\n",
      "Iteration: 9440 loss: 0.0000000084 time: 0.3147275447845459\n",
      "Iteration: 9450 loss: 0.0000000045 time: 0.33971071243286133\n",
      "Iteration: 9460 loss: 0.0000000032 time: 0.32558298110961914\n",
      "Iteration: 9470 loss: 0.0000000029 time: 0.31461048126220703\n",
      "Iteration: 9480 loss: 0.0000000028 time: 0.33173131942749023\n",
      "Iteration: 9490 loss: 0.0000000027 time: 0.31281590461730957\n",
      "Iteration: 9500 loss: 0.0000000027 time: 0.3062558174133301\n",
      "Iteration: 9510 loss: 0.0000000027 time: 0.3224301338195801\n",
      "Iteration: 9520 loss: 0.0000000027 time: 0.3122105598449707\n",
      "Iteration: 9530 loss: 0.0000000027 time: 0.3167259693145752\n",
      "Iteration: 9540 loss: 0.0000000027 time: 0.3213539123535156\n",
      "Iteration: 9550 loss: 0.0000000027 time: 0.32822656631469727\n",
      "Iteration: 9560 loss: 0.0000000027 time: 0.3157389163970947\n",
      "Iteration: 9570 loss: 0.0000000027 time: 0.324049711227417\n",
      "Iteration: 9580 loss: 0.0000000027 time: 0.31766581535339355\n",
      "Iteration: 9590 loss: 0.0000000027 time: 0.3368966579437256\n",
      "Iteration: 9600 loss: 0.0000000027 time: 0.320345401763916\n",
      "Iteration: 9610 loss: 0.0000000027 time: 0.33858156204223633\n",
      "Iteration: 9620 loss: 0.0000000036 time: 0.33907628059387207\n",
      "Iteration: 9630 loss: 0.0000000364 time: 0.32066893577575684\n",
      "Iteration: 9640 loss: 0.0000000172 time: 0.3575890064239502\n",
      "Iteration: 9650 loss: 0.0000000047 time: 0.316436767578125\n",
      "Iteration: 9660 loss: 0.0000000035 time: 0.2965693473815918\n",
      "Iteration: 9670 loss: 0.0000000032 time: 0.295912504196167\n",
      "Iteration: 9680 loss: 0.0000000027 time: 0.3390781879425049\n",
      "Iteration: 9690 loss: 0.0000000027 time: 0.3455686569213867\n",
      "Iteration: 9700 loss: 0.0000000027 time: 0.32123374938964844\n",
      "Iteration: 9710 loss: 0.0000000027 time: 0.32151198387145996\n",
      "Iteration: 9720 loss: 0.0000000027 time: 0.3160431385040283\n",
      "Iteration: 9730 loss: 0.0000000027 time: 0.3142988681793213\n",
      "Iteration: 9740 loss: 0.0000000026 time: 0.3259153366088867\n",
      "Iteration: 9750 loss: 0.0000000026 time: 0.32796764373779297\n",
      "Iteration: 9760 loss: 0.0000000026 time: 0.3211324214935303\n",
      "Iteration: 9770 loss: 0.0000000026 time: 0.3232114315032959\n",
      "Iteration: 9780 loss: 0.0000000028 time: 0.3167860507965088\n",
      "Iteration: 9790 loss: 0.0000000073 time: 0.3172309398651123\n",
      "Iteration: 9800 loss: 0.0000000065 time: 0.3174605369567871\n",
      "Iteration: 9810 loss: 0.0000000028 time: 0.32067012786865234\n",
      "Iteration: 9820 loss: 0.0000000031 time: 0.3238341808319092\n",
      "Iteration: 9830 loss: 0.0000000031 time: 0.3154330253601074\n",
      "Iteration: 9840 loss: 0.0000000028 time: 0.31843113899230957\n",
      "Iteration: 9850 loss: 0.0000000026 time: 0.3198361396789551\n",
      "Iteration: 9860 loss: 0.0000000026 time: 0.3558964729309082\n",
      "Iteration: 9870 loss: 0.0000000026 time: 0.3561403751373291\n",
      "Iteration: 9880 loss: 0.0000000026 time: 0.3226656913757324\n",
      "Iteration: 9890 loss: 0.0000000026 time: 0.3169121742248535\n",
      "Iteration: 9900 loss: 0.0000000026 time: 0.32120633125305176\n",
      "Iteration: 9910 loss: 0.0000000026 time: 0.3153347969055176\n",
      "Iteration: 9920 loss: 0.0000000026 time: 0.33023810386657715\n",
      "Iteration: 9930 loss: 0.0000000026 time: 0.3068099021911621\n",
      "Iteration: 9940 loss: 0.0000000026 time: 0.28966212272644043\n",
      "Iteration: 9950 loss: 0.0000000027 time: 0.31849193572998047\n",
      "Iteration: 9960 loss: 0.0000000104 time: 0.30180811882019043\n",
      "Iteration: 9970 loss: 0.0000000094 time: 0.3242366313934326\n",
      "Iteration: 9980 loss: 0.0000000064 time: 0.3094618320465088\n",
      "Iteration: 9990 loss: 0.0000000050 time: 0.2858719825744629\n",
      "Iteration: 10000 loss: 0.0000000029 time: 0.2869589328765869\n",
      "Iteration: 10010 loss: 0.0000000029 time: 0.2949092388153076\n",
      "Iteration: 10020 loss: 0.0000000026 time: 0.3162109851837158\n",
      "Iteration: 10030 loss: 0.0000000026 time: 0.30504441261291504\n",
      "Iteration: 10040 loss: 0.0000000026 time: 0.28389930725097656\n",
      "Iteration: 10050 loss: 0.0000000026 time: 0.3045644760131836\n",
      "Iteration: 10060 loss: 0.0000000026 time: 0.3308858871459961\n",
      "Iteration: 10070 loss: 0.0000000026 time: 0.3409156799316406\n",
      "Iteration: 10080 loss: 0.0000000026 time: 0.3492605686187744\n",
      "Iteration: 10090 loss: 0.0000000026 time: 0.3148317337036133\n",
      "Iteration: 10100 loss: 0.0000000026 time: 0.3187859058380127\n",
      "Iteration: 10110 loss: 0.0000000026 time: 0.3628561496734619\n",
      "Iteration: 10120 loss: 0.0000000026 time: 0.3496263027191162\n",
      "Iteration: 10130 loss: 0.0000000026 time: 0.30692481994628906\n",
      "Iteration: 10140 loss: 0.0000000026 time: 0.3231039047241211\n",
      "Iteration: 10150 loss: 0.0000000027 time: 0.30985355377197266\n",
      "Iteration: 10160 loss: 0.0000000067 time: 0.3118133544921875\n",
      "Iteration: 10170 loss: 0.0000000085 time: 0.29143452644348145\n",
      "Iteration: 10180 loss: 0.0000000028 time: 0.31075382232666016\n",
      "Iteration: 10190 loss: 0.0000000036 time: 0.31677961349487305\n",
      "Iteration: 10200 loss: 0.0000000030 time: 0.31778454780578613\n",
      "Iteration: 10210 loss: 0.0000000026 time: 0.3012886047363281\n",
      "Iteration: 10220 loss: 0.0000000026 time: 0.353374719619751\n",
      "Iteration: 10230 loss: 0.0000000026 time: 0.34998106956481934\n",
      "Iteration: 10240 loss: 0.0000000025 time: 0.32654428482055664\n",
      "Iteration: 10250 loss: 0.0000000025 time: 0.3201477527618408\n",
      "Iteration: 10260 loss: 0.0000000025 time: 0.3229856491088867\n",
      "Iteration: 10270 loss: 0.0000000025 time: 0.3633310794830322\n",
      "Iteration: 10280 loss: 0.0000000026 time: 0.33306264877319336\n",
      "Iteration: 10290 loss: 0.0000000030 time: 0.35279011726379395\n",
      "Iteration: 10300 loss: 0.0000000140 time: 0.33690643310546875\n",
      "Iteration: 10310 loss: 0.0000000034 time: 0.36637091636657715\n",
      "Iteration: 10320 loss: 0.0000000027 time: 0.3064723014831543\n",
      "Iteration: 10330 loss: 0.0000000029 time: 0.2962605953216553\n",
      "Iteration: 10340 loss: 0.0000000028 time: 0.35942912101745605\n",
      "Iteration: 10350 loss: 0.0000000027 time: 0.31159090995788574\n",
      "Iteration: 10360 loss: 0.0000000026 time: 0.3097085952758789\n",
      "Iteration: 10370 loss: 0.0000000025 time: 0.28949713706970215\n",
      "Iteration: 10380 loss: 0.0000000025 time: 0.31757521629333496\n",
      "Iteration: 10390 loss: 0.0000000025 time: 0.32161426544189453\n",
      "Iteration: 10400 loss: 0.0000000025 time: 0.31697583198547363\n",
      "Iteration: 10410 loss: 0.0000000025 time: 0.32165980339050293\n",
      "Iteration: 10420 loss: 0.0000000025 time: 0.31351423263549805\n",
      "Iteration: 10430 loss: 0.0000000025 time: 0.3552072048187256\n",
      "Iteration: 10440 loss: 0.0000000025 time: 0.3301398754119873\n",
      "Iteration: 10450 loss: 0.0000000025 time: 0.32309579849243164\n",
      "Iteration: 10460 loss: 0.0000000025 time: 0.34087133407592773\n",
      "Iteration: 10470 loss: 0.0000000025 time: 0.3119337558746338\n",
      "Iteration: 10480 loss: 0.0000000025 time: 0.29360151290893555\n",
      "Iteration: 10490 loss: 0.0000000027 time: 0.32103800773620605\n",
      "Iteration: 10500 loss: 0.0000000102 time: 0.3440585136413574\n",
      "Iteration: 10510 loss: 0.0000000126 time: 0.34691882133483887\n",
      "Iteration: 10520 loss: 0.0000000054 time: 0.34584808349609375\n",
      "Iteration: 10530 loss: 0.0000000044 time: 0.3205695152282715\n",
      "Iteration: 10540 loss: 0.0000000025 time: 0.31403017044067383\n",
      "Iteration: 10550 loss: 0.0000000028 time: 0.3235766887664795\n",
      "Iteration: 10560 loss: 0.0000000025 time: 0.2966768741607666\n",
      "Iteration: 10570 loss: 0.0000000025 time: 0.31276535987854004\n",
      "Iteration: 10580 loss: 0.0000000025 time: 0.3224148750305176\n",
      "Iteration: 10590 loss: 0.0000000025 time: 0.3224809169769287\n",
      "Iteration: 10600 loss: 0.0000000025 time: 0.30443620681762695\n",
      "Iteration: 10610 loss: 0.0000000025 time: 0.34540486335754395\n",
      "Iteration: 10620 loss: 0.0000000025 time: 0.41113924980163574\n",
      "Iteration: 10630 loss: 0.0000000025 time: 0.3015778064727783\n",
      "Iteration: 10640 loss: 0.0000000027 time: 0.3220641613006592\n",
      "Iteration: 10650 loss: 0.0000000070 time: 0.3143737316131592\n",
      "Iteration: 10660 loss: 0.0000000167 time: 0.3379499912261963\n",
      "Iteration: 10670 loss: 0.0000000069 time: 0.3072192668914795\n",
      "Iteration: 10680 loss: 0.0000000035 time: 0.3067173957824707\n",
      "Iteration: 10690 loss: 0.0000000027 time: 0.36490750312805176\n",
      "Iteration: 10700 loss: 0.0000000025 time: 0.32214903831481934\n",
      "Iteration: 10710 loss: 0.0000000025 time: 0.29475855827331543\n",
      "Iteration: 10720 loss: 0.0000000025 time: 0.3105494976043701\n",
      "Iteration: 10730 loss: 0.0000000025 time: 0.2736492156982422\n",
      "Iteration: 10740 loss: 0.0000000024 time: 0.3373231887817383\n",
      "Iteration: 10750 loss: 0.0000000024 time: 0.31734156608581543\n",
      "Iteration: 10760 loss: 0.0000000024 time: 0.3340632915496826\n",
      "Iteration: 10770 loss: 0.0000000024 time: 0.33165645599365234\n",
      "Iteration: 10780 loss: 0.0000000024 time: 0.31186723709106445\n",
      "Iteration: 10790 loss: 0.0000000024 time: 0.28151822090148926\n",
      "Iteration: 10800 loss: 0.0000000024 time: 0.2897968292236328\n",
      "Iteration: 10810 loss: 0.0000000024 time: 0.2891843318939209\n",
      "Iteration: 10820 loss: 0.0000000024 time: 0.3071258068084717\n",
      "Iteration: 10830 loss: 0.0000000024 time: 0.33475804328918457\n",
      "Iteration: 10840 loss: 0.0000000026 time: 0.3449738025665283\n",
      "Iteration: 10850 loss: 0.0000000092 time: 0.3313565254211426\n",
      "Iteration: 10860 loss: 0.0000000142 time: 0.361295223236084\n",
      "Iteration: 10870 loss: 0.0000000025 time: 0.3025329113006592\n",
      "Iteration: 10880 loss: 0.0000000039 time: 0.3009195327758789\n",
      "Iteration: 10890 loss: 0.0000000033 time: 0.35703063011169434\n",
      "Iteration: 10900 loss: 0.0000000025 time: 0.32715344429016113\n",
      "Iteration: 10910 loss: 0.0000000024 time: 0.3142852783203125\n",
      "Iteration: 10920 loss: 0.0000000024 time: 0.31188368797302246\n",
      "Iteration: 10930 loss: 0.0000000024 time: 0.3097052574157715\n",
      "Iteration: 10940 loss: 0.0000000024 time: 0.320385217666626\n",
      "Iteration: 10950 loss: 0.0000000024 time: 0.28812718391418457\n",
      "Iteration: 10960 loss: 0.0000000024 time: 0.36245155334472656\n",
      "Iteration: 10970 loss: 0.0000000025 time: 0.32413434982299805\n",
      "Iteration: 10980 loss: 0.0000000053 time: 0.3250620365142822\n",
      "Iteration: 10990 loss: 0.0000000104 time: 0.3270876407623291\n",
      "Iteration: 11000 loss: 0.0000000050 time: 0.3370819091796875\n",
      "Iteration: 11010 loss: 0.0000000031 time: 0.30010104179382324\n",
      "Iteration: 11020 loss: 0.0000000025 time: 0.3083333969116211\n",
      "Iteration: 11030 loss: 0.0000000024 time: 0.31999707221984863\n",
      "Iteration: 11040 loss: 0.0000000024 time: 0.30586791038513184\n",
      "Iteration: 11050 loss: 0.0000000024 time: 0.3000481128692627\n",
      "Iteration: 11060 loss: 0.0000000024 time: 0.31945133209228516\n",
      "Iteration: 11070 loss: 0.0000000024 time: 0.327883243560791\n",
      "Iteration: 11080 loss: 0.0000000024 time: 0.28382325172424316\n",
      "Iteration: 11090 loss: 0.0000000024 time: 0.28462886810302734\n",
      "Iteration: 11100 loss: 0.0000000024 time: 0.3154008388519287\n",
      "Iteration: 11110 loss: 0.0000000024 time: 0.3267347812652588\n",
      "Iteration: 11120 loss: 0.0000000024 time: 0.35701823234558105\n",
      "Iteration: 11130 loss: 0.0000000026 time: 0.3228275775909424\n",
      "Iteration: 11140 loss: 0.0000000105 time: 0.3216259479522705\n",
      "Iteration: 11150 loss: 0.0000000086 time: 0.3599693775177002\n",
      "Iteration: 11160 loss: 0.0000000029 time: 0.37746310234069824\n",
      "Iteration: 11170 loss: 0.0000000046 time: 0.3531684875488281\n",
      "Iteration: 11180 loss: 0.0000000029 time: 0.3509073257446289\n",
      "Iteration: 11190 loss: 0.0000000024 time: 0.30966997146606445\n",
      "Iteration: 11200 loss: 0.0000000024 time: 0.31005382537841797\n",
      "Iteration: 11210 loss: 0.0000000024 time: 0.2975471019744873\n",
      "Iteration: 11220 loss: 0.0000000024 time: 0.30562496185302734\n",
      "Iteration: 11230 loss: 0.0000000023 time: 0.3411242961883545\n",
      "Iteration: 11240 loss: 0.0000000023 time: 0.3056931495666504\n",
      "Iteration: 11250 loss: 0.0000000023 time: 0.3044567108154297\n",
      "Iteration: 11260 loss: 0.0000000023 time: 0.2866344451904297\n",
      "Iteration: 11270 loss: 0.0000000024 time: 0.3196423053741455\n",
      "Iteration: 11280 loss: 0.0000000026 time: 0.33986687660217285\n",
      "Iteration: 11290 loss: 0.0000000086 time: 0.33591556549072266\n",
      "Iteration: 11300 loss: 0.0000000034 time: 0.3138587474822998\n",
      "Iteration: 11310 loss: 0.0000000026 time: 0.28025317192077637\n",
      "Iteration: 11320 loss: 0.0000000026 time: 0.29459142684936523\n",
      "Iteration: 11330 loss: 0.0000000025 time: 0.3446080684661865\n",
      "Iteration: 11340 loss: 0.0000000024 time: 0.36257314682006836\n",
      "Iteration: 11350 loss: 0.0000000024 time: 0.28084850311279297\n",
      "Iteration: 11360 loss: 0.0000000023 time: 0.3054637908935547\n",
      "Iteration: 11370 loss: 0.0000000023 time: 0.304412841796875\n",
      "Iteration: 11380 loss: 0.0000000023 time: 0.31934380531311035\n",
      "Iteration: 11390 loss: 0.0000000023 time: 0.31241917610168457\n",
      "Iteration: 11400 loss: 0.0000000023 time: 0.3142409324645996\n",
      "Iteration: 11410 loss: 0.0000000023 time: 0.3162069320678711\n",
      "Iteration: 11420 loss: 0.0000000024 time: 0.34571266174316406\n",
      "Iteration: 11430 loss: 0.0000000053 time: 0.35197925567626953\n",
      "Iteration: 11440 loss: 0.0000000279 time: 0.29800868034362793\n",
      "Iteration: 11450 loss: 0.0000000097 time: 0.30809497833251953\n",
      "Iteration: 11460 loss: 0.0000000034 time: 0.3050351142883301\n",
      "Iteration: 11470 loss: 0.0000000023 time: 0.29159092903137207\n",
      "Iteration: 11480 loss: 0.0000000024 time: 0.295015811920166\n",
      "Iteration: 11490 loss: 0.0000000024 time: 0.30111026763916016\n",
      "Iteration: 11500 loss: 0.0000000023 time: 0.3375718593597412\n",
      "Iteration: 11510 loss: 0.0000000023 time: 0.3099503517150879\n",
      "Iteration: 11520 loss: 0.0000000023 time: 0.30112695693969727\n",
      "Iteration: 11530 loss: 0.0000000023 time: 0.2931058406829834\n",
      "Iteration: 11540 loss: 0.0000000023 time: 0.31406307220458984\n",
      "Iteration: 11550 loss: 0.0000000023 time: 0.31577610969543457\n",
      "Iteration: 11560 loss: 0.0000000023 time: 0.298966646194458\n",
      "Iteration: 11570 loss: 0.0000000023 time: 0.3226172924041748\n",
      "Iteration: 11580 loss: 0.0000000023 time: 0.3083524703979492\n",
      "Iteration: 11590 loss: 0.0000000025 time: 0.29894590377807617\n",
      "Iteration: 11600 loss: 0.0000000073 time: 0.34232664108276367\n",
      "Iteration: 11610 loss: 0.0000000056 time: 0.29920506477355957\n",
      "Iteration: 11620 loss: 0.0000000024 time: 0.30610036849975586\n",
      "Iteration: 11630 loss: 0.0000000025 time: 0.3092336654663086\n",
      "Iteration: 11640 loss: 0.0000000026 time: 0.30713343620300293\n",
      "Iteration: 11650 loss: 0.0000000024 time: 0.31928348541259766\n",
      "Iteration: 11660 loss: 0.0000000023 time: 0.3465311527252197\n",
      "Iteration: 11670 loss: 0.0000000023 time: 0.31816649436950684\n",
      "Iteration: 11680 loss: 0.0000000025 time: 0.33402204513549805\n",
      "Iteration: 11690 loss: 0.0000000050 time: 0.3285994529724121\n",
      "Iteration: 11700 loss: 0.0000000207 time: 0.3327302932739258\n",
      "Iteration: 11710 loss: 0.0000000074 time: 0.36126160621643066\n",
      "Iteration: 11720 loss: 0.0000000041 time: 0.3317737579345703\n",
      "Iteration: 11730 loss: 0.0000000029 time: 0.3080623149871826\n",
      "Iteration: 11740 loss: 0.0000000025 time: 0.30123329162597656\n",
      "Iteration: 11750 loss: 0.0000000023 time: 0.30775976181030273\n",
      "Iteration: 11760 loss: 0.0000000023 time: 0.32480931282043457\n",
      "Iteration: 11770 loss: 0.0000000023 time: 0.3129267692565918\n",
      "Iteration: 11780 loss: 0.0000000023 time: 0.3030409812927246\n",
      "Iteration: 11790 loss: 0.0000000023 time: 0.3081638813018799\n",
      "Iteration: 11800 loss: 0.0000000023 time: 0.3487861156463623\n",
      "Iteration: 11810 loss: 0.0000000023 time: 0.3328986167907715\n",
      "Iteration: 11820 loss: 0.0000000022 time: 0.3034181594848633\n",
      "Iteration: 11830 loss: 0.0000000022 time: 0.33739662170410156\n",
      "Iteration: 11840 loss: 0.0000000022 time: 0.35496068000793457\n",
      "Iteration: 11850 loss: 0.0000000023 time: 0.3182239532470703\n",
      "Iteration: 11860 loss: 0.0000000024 time: 0.3404567241668701\n",
      "Iteration: 11870 loss: 0.0000000062 time: 0.3008866310119629\n",
      "Iteration: 11880 loss: 0.0000000248 time: 0.2820158004760742\n",
      "Iteration: 11890 loss: 0.0000000082 time: 0.3622105121612549\n",
      "Iteration: 11900 loss: 0.0000000029 time: 0.34920573234558105\n",
      "Iteration: 11910 loss: 0.0000000022 time: 0.40406036376953125\n",
      "Iteration: 11920 loss: 0.0000000023 time: 0.2920811176300049\n",
      "Iteration: 11930 loss: 0.0000000023 time: 0.30939769744873047\n",
      "Iteration: 11940 loss: 0.0000000023 time: 0.2970564365386963\n",
      "Iteration: 11950 loss: 0.0000000022 time: 0.2924637794494629\n",
      "Iteration: 11960 loss: 0.0000000022 time: 0.3369486331939697\n",
      "Iteration: 11970 loss: 0.0000000022 time: 0.2945098876953125\n",
      "Iteration: 11980 loss: 0.0000000022 time: 0.28995823860168457\n",
      "Iteration: 11990 loss: 0.0000000025 time: 0.32248640060424805\n",
      "Iteration: 12000 loss: 0.0000000090 time: 0.35831570625305176\n",
      "Iteration: 12010 loss: 0.0000000028 time: 0.3272225856781006\n",
      "Iteration: 12020 loss: 0.0000000024 time: 0.3231372833251953\n",
      "Iteration: 12030 loss: 0.0000000024 time: 0.31581807136535645\n",
      "Iteration: 12040 loss: 0.0000000024 time: 0.3800349235534668\n",
      "Iteration: 12050 loss: 0.0000000023 time: 0.3405463695526123\n",
      "Iteration: 12060 loss: 0.0000000022 time: 0.33614349365234375\n",
      "Iteration: 12070 loss: 0.0000000022 time: 0.3286402225494385\n",
      "Iteration: 12080 loss: 0.0000000022 time: 0.38333868980407715\n",
      "Iteration: 12090 loss: 0.0000000022 time: 0.35735130310058594\n",
      "Iteration: 12100 loss: 0.0000000022 time: 0.3654749393463135\n",
      "Iteration: 12110 loss: 0.0000000022 time: 0.34659481048583984\n",
      "Iteration: 12120 loss: 0.0000000023 time: 0.3434274196624756\n",
      "Iteration: 12130 loss: 0.0000000037 time: 0.3392634391784668\n",
      "Iteration: 12140 loss: 0.0000000269 time: 0.33454322814941406\n",
      "Iteration: 12150 loss: 0.0000000094 time: 0.3753805160522461\n",
      "Iteration: 12160 loss: 0.0000000054 time: 0.33695244789123535\n",
      "Iteration: 12170 loss: 0.0000000030 time: 0.3425755500793457\n",
      "Iteration: 12180 loss: 0.0000000023 time: 0.29752135276794434\n",
      "Iteration: 12190 loss: 0.0000000022 time: 0.3471031188964844\n",
      "Iteration: 12200 loss: 0.0000000022 time: 0.3651399612426758\n",
      "Iteration: 12210 loss: 0.0000000022 time: 0.35751938819885254\n",
      "Iteration: 12220 loss: 0.0000000022 time: 0.2980477809906006\n",
      "Iteration: 12230 loss: 0.0000000022 time: 0.36023736000061035\n",
      "Iteration: 12240 loss: 0.0000000022 time: 0.310197114944458\n",
      "Iteration: 12250 loss: 0.0000000022 time: 0.2972078323364258\n",
      "Iteration: 12260 loss: 0.0000000022 time: 0.2983427047729492\n",
      "Iteration: 12270 loss: 0.0000000022 time: 0.3667449951171875\n",
      "Iteration: 12280 loss: 0.0000000022 time: 0.32158327102661133\n",
      "Iteration: 12290 loss: 0.0000000023 time: 0.30686092376708984\n",
      "Iteration: 12300 loss: 0.0000000048 time: 0.3288414478302002\n",
      "Iteration: 12310 loss: 0.0000000146 time: 0.36196398735046387\n",
      "Iteration: 12320 loss: 0.0000000044 time: 0.32242703437805176\n",
      "Iteration: 12330 loss: 0.0000000023 time: 0.3261070251464844\n",
      "Iteration: 12340 loss: 0.0000000024 time: 0.29016900062561035\n",
      "Iteration: 12350 loss: 0.0000000024 time: 0.3040328025817871\n",
      "Iteration: 12360 loss: 0.0000000023 time: 0.31586122512817383\n",
      "Iteration: 12370 loss: 0.0000000022 time: 0.3366405963897705\n",
      "Iteration: 12380 loss: 0.0000000024 time: 0.3290543556213379\n",
      "Iteration: 12390 loss: 0.0000000048 time: 0.3287670612335205\n",
      "Iteration: 12400 loss: 0.0000000149 time: 0.33564209938049316\n",
      "Iteration: 12410 loss: 0.0000000038 time: 0.33882999420166016\n",
      "Iteration: 12420 loss: 0.0000000022 time: 0.342327356338501\n",
      "Iteration: 12430 loss: 0.0000000022 time: 0.3413240909576416\n",
      "Iteration: 12440 loss: 0.0000000023 time: 0.3513946533203125\n",
      "Iteration: 12450 loss: 0.0000000022 time: 0.34114766120910645\n",
      "Iteration: 12460 loss: 0.0000000022 time: 0.34395742416381836\n",
      "Iteration: 12470 loss: 0.0000000022 time: 0.3304252624511719\n",
      "Iteration: 12480 loss: 0.0000000021 time: 0.32572221755981445\n",
      "Iteration: 12490 loss: 0.0000000021 time: 0.37159156799316406\n",
      "Iteration: 12500 loss: 0.0000000021 time: 0.3478083610534668\n",
      "Iteration: 12510 loss: 0.0000000022 time: 0.3329286575317383\n",
      "Iteration: 12520 loss: 0.0000000022 time: 0.3637261390686035\n",
      "Iteration: 12530 loss: 0.0000000033 time: 0.34881591796875\n",
      "Iteration: 12540 loss: 0.0000000180 time: 0.32381510734558105\n",
      "Iteration: 12550 loss: 0.0000000029 time: 0.30745625495910645\n",
      "Iteration: 12560 loss: 0.0000000025 time: 0.32282543182373047\n",
      "Iteration: 12570 loss: 0.0000000023 time: 0.2982790470123291\n",
      "Iteration: 12580 loss: 0.0000000022 time: 0.31351447105407715\n",
      "Iteration: 12590 loss: 0.0000000021 time: 0.3427300453186035\n",
      "Iteration: 12600 loss: 0.0000000021 time: 0.30142855644226074\n",
      "Iteration: 12610 loss: 0.0000000021 time: 0.29313135147094727\n",
      "Iteration: 12620 loss: 0.0000000021 time: 0.29350924491882324\n",
      "Iteration: 12630 loss: 0.0000000021 time: 0.356048583984375\n",
      "Iteration: 12640 loss: 0.0000000021 time: 0.3933990001678467\n",
      "Iteration: 12650 loss: 0.0000000022 time: 0.342149019241333\n",
      "Iteration: 12660 loss: 0.0000000032 time: 0.3861684799194336\n",
      "Iteration: 12670 loss: 0.0000000143 time: 0.41466736793518066\n",
      "Iteration: 12680 loss: 0.0000000052 time: 0.3494863510131836\n",
      "Iteration: 12690 loss: 0.0000000034 time: 0.3581366539001465\n",
      "Iteration: 12700 loss: 0.0000000027 time: 0.3479471206665039\n",
      "Iteration: 12710 loss: 0.0000000023 time: 0.3481271266937256\n",
      "Iteration: 12720 loss: 0.0000000022 time: 0.34416985511779785\n",
      "Iteration: 12730 loss: 0.0000000022 time: 0.363875150680542\n",
      "Iteration: 12740 loss: 0.0000000024 time: 0.35006189346313477\n",
      "Iteration: 12750 loss: 0.0000000061 time: 0.3479123115539551\n",
      "Iteration: 12760 loss: 0.0000000167 time: 0.3381786346435547\n",
      "Iteration: 12770 loss: 0.0000000071 time: 0.37381815910339355\n",
      "Iteration: 12780 loss: 0.0000000038 time: 0.32693934440612793\n",
      "Iteration: 12790 loss: 0.0000000027 time: 0.30551600456237793\n",
      "Iteration: 12800 loss: 0.0000000023 time: 0.2943761348724365\n",
      "Iteration: 12810 loss: 0.0000000022 time: 0.3213462829589844\n",
      "Iteration: 12820 loss: 0.0000000021 time: 0.3428926467895508\n",
      "Iteration: 12830 loss: 0.0000000021 time: 0.3100922107696533\n",
      "Iteration: 12840 loss: 0.0000000021 time: 0.3651463985443115\n",
      "Iteration: 12850 loss: 0.0000000021 time: 0.3290581703186035\n",
      "Iteration: 12860 loss: 0.0000000021 time: 0.2919762134552002\n",
      "Iteration: 12870 loss: 0.0000000021 time: 0.30085229873657227\n",
      "Iteration: 12880 loss: 0.0000000021 time: 0.33627820014953613\n",
      "Iteration: 12890 loss: 0.0000000021 time: 0.3326115608215332\n",
      "Iteration: 12900 loss: 0.0000000021 time: 0.3501451015472412\n",
      "Iteration: 12910 loss: 0.0000000021 time: 0.31909751892089844\n",
      "Iteration: 12920 loss: 0.0000000027 time: 0.2925994396209717\n",
      "Iteration: 12930 loss: 0.0000000181 time: 0.2909882068634033\n",
      "Iteration: 12940 loss: 0.0000000037 time: 0.31102633476257324\n",
      "Iteration: 12950 loss: 0.0000000048 time: 0.2983121871948242\n",
      "Iteration: 12960 loss: 0.0000000037 time: 0.29613566398620605\n",
      "Iteration: 12970 loss: 0.0000000024 time: 0.3217141628265381\n",
      "Iteration: 12980 loss: 0.0000000021 time: 0.3497641086578369\n",
      "Iteration: 12990 loss: 0.0000000021 time: 0.3750948905944824\n",
      "Iteration: 13000 loss: 0.0000000021 time: 0.33594512939453125\n",
      "Iteration: 13010 loss: 0.0000000021 time: 0.3481600284576416\n",
      "Iteration: 13020 loss: 0.0000000021 time: 0.3682398796081543\n",
      "Iteration: 13030 loss: 0.0000000021 time: 0.3348348140716553\n",
      "Iteration: 13040 loss: 0.0000000021 time: 0.37087559700012207\n",
      "Iteration: 13050 loss: 0.0000000021 time: 0.32913732528686523\n",
      "Iteration: 13060 loss: 0.0000000023 time: 0.29799413681030273\n",
      "Iteration: 13070 loss: 0.0000000083 time: 0.30016613006591797\n",
      "Iteration: 13080 loss: 0.0000000077 time: 0.3097858428955078\n",
      "Iteration: 13090 loss: 0.0000000033 time: 0.29743337631225586\n",
      "Iteration: 13100 loss: 0.0000000025 time: 0.34038686752319336\n",
      "Iteration: 13110 loss: 0.0000000024 time: 0.33545708656311035\n",
      "Iteration: 13120 loss: 0.0000000022 time: 0.30492234230041504\n",
      "Iteration: 13130 loss: 0.0000000021 time: 0.3262031078338623\n",
      "Iteration: 13140 loss: 0.0000000021 time: 0.36144065856933594\n",
      "Iteration: 13150 loss: 0.0000000021 time: 0.3355679512023926\n",
      "Iteration: 13160 loss: 0.0000000021 time: 0.3296957015991211\n",
      "Iteration: 13170 loss: 0.0000000021 time: 0.3246729373931885\n",
      "Iteration: 13180 loss: 0.0000000021 time: 0.29382801055908203\n",
      "Iteration: 13190 loss: 0.0000000021 time: 0.28844404220581055\n",
      "Iteration: 13200 loss: 0.0000000021 time: 0.27263784408569336\n",
      "Iteration: 13210 loss: 0.0000000020 time: 0.28798532485961914\n",
      "Iteration: 13220 loss: 0.0000000020 time: 0.2973968982696533\n",
      "Iteration: 13230 loss: 0.0000000021 time: 0.3245854377746582\n",
      "Iteration: 13240 loss: 0.0000000022 time: 0.3289353847503662\n",
      "Iteration: 13250 loss: 0.0000000070 time: 0.32887959480285645\n",
      "Iteration: 13260 loss: 0.0000000208 time: 0.3498694896697998\n",
      "Iteration: 13270 loss: 0.0000000044 time: 0.33092331886291504\n",
      "Iteration: 13280 loss: 0.0000000022 time: 0.3276331424713135\n",
      "Iteration: 13290 loss: 0.0000000028 time: 0.33432435989379883\n",
      "Iteration: 13300 loss: 0.0000000024 time: 0.3210017681121826\n",
      "Iteration: 13310 loss: 0.0000000021 time: 0.3219153881072998\n",
      "Iteration: 13320 loss: 0.0000000020 time: 0.3233339786529541\n",
      "Iteration: 13330 loss: 0.0000000020 time: 0.32344698905944824\n",
      "Iteration: 13340 loss: 0.0000000020 time: 0.3190298080444336\n",
      "Iteration: 13350 loss: 0.0000000020 time: 0.3644530773162842\n",
      "Iteration: 13360 loss: 0.0000000020 time: 0.377551794052124\n",
      "Iteration: 13370 loss: 0.0000000020 time: 0.3519461154937744\n",
      "Iteration: 13380 loss: 0.0000000020 time: 0.3476583957672119\n",
      "Iteration: 13390 loss: 0.0000000020 time: 0.3211708068847656\n",
      "Iteration: 13400 loss: 0.0000000020 time: 0.30820369720458984\n",
      "Iteration: 13410 loss: 0.0000000020 time: 0.3150622844696045\n",
      "Iteration: 13420 loss: 0.0000000020 time: 0.3447868824005127\n",
      "Iteration: 13430 loss: 0.0000000023 time: 0.322831392288208\n",
      "Iteration: 13440 loss: 0.0000000175 time: 0.30089712142944336\n",
      "Iteration: 13450 loss: 0.0000000090 time: 0.29396700859069824\n",
      "Iteration: 13460 loss: 0.0000000051 time: 0.33888769149780273\n",
      "Iteration: 13470 loss: 0.0000000025 time: 0.3139915466308594\n",
      "Iteration: 13480 loss: 0.0000000024 time: 0.32256126403808594\n",
      "Iteration: 13490 loss: 0.0000000021 time: 0.3150036334991455\n",
      "Iteration: 13500 loss: 0.0000000021 time: 0.32625341415405273\n",
      "Iteration: 13510 loss: 0.0000000020 time: 0.3426699638366699\n",
      "Iteration: 13520 loss: 0.0000000020 time: 0.3199644088745117\n",
      "Iteration: 13530 loss: 0.0000000020 time: 0.3033890724182129\n",
      "Iteration: 13540 loss: 0.0000000020 time: 0.32924699783325195\n",
      "Iteration: 13550 loss: 0.0000000020 time: 0.3263578414916992\n",
      "Iteration: 13560 loss: 0.0000000021 time: 0.296877384185791\n",
      "Iteration: 13570 loss: 0.0000000035 time: 0.3518102169036865\n",
      "Iteration: 13580 loss: 0.0000000221 time: 0.31551027297973633\n",
      "Iteration: 13590 loss: 0.0000000058 time: 0.28154993057250977\n",
      "Iteration: 13600 loss: 0.0000000039 time: 0.324568510055542\n",
      "Iteration: 13610 loss: 0.0000000029 time: 0.3088266849517822\n",
      "Iteration: 13620 loss: 0.0000000024 time: 0.3105430603027344\n",
      "Iteration: 13630 loss: 0.0000000021 time: 0.2820463180541992\n",
      "Iteration: 13640 loss: 0.0000000020 time: 0.29395079612731934\n",
      "Iteration: 13650 loss: 0.0000000020 time: 0.32923293113708496\n",
      "Iteration: 13660 loss: 0.0000000020 time: 0.3418912887573242\n",
      "Iteration: 13670 loss: 0.0000000020 time: 0.30692219734191895\n",
      "Iteration: 13680 loss: 0.0000000020 time: 0.3414311408996582\n",
      "Iteration: 13690 loss: 0.0000000020 time: 0.2857551574707031\n",
      "Iteration: 13700 loss: 0.0000000020 time: 0.2842838764190674\n",
      "Iteration: 13710 loss: 0.0000000020 time: 0.29883885383605957\n",
      "Iteration: 13720 loss: 0.0000000020 time: 0.3742058277130127\n",
      "Iteration: 13730 loss: 0.0000000020 time: 0.3302652835845947\n",
      "Iteration: 13740 loss: 0.0000000020 time: 0.3315110206604004\n",
      "Iteration: 13750 loss: 0.0000000020 time: 0.3416481018066406\n",
      "Iteration: 13760 loss: 0.0000000020 time: 0.37250256538391113\n",
      "Iteration: 13770 loss: 0.0000000035 time: 0.3458847999572754\n",
      "Iteration: 13780 loss: 0.0000000324 time: 0.33762049674987793\n",
      "Iteration: 13790 loss: 0.0000000126 time: 0.35788536071777344\n",
      "Iteration: 13800 loss: 0.0000000045 time: 0.34812068939208984\n",
      "Iteration: 13810 loss: 0.0000000020 time: 0.31710362434387207\n",
      "Iteration: 13820 loss: 0.0000000022 time: 0.3123331069946289\n",
      "Iteration: 13830 loss: 0.0000000021 time: 0.32394957542419434\n",
      "Iteration: 13840 loss: 0.0000000020 time: 0.3013014793395996\n",
      "Iteration: 13850 loss: 0.0000000020 time: 0.32483649253845215\n",
      "Iteration: 13860 loss: 0.0000000020 time: 0.32977795600891113\n",
      "Iteration: 13870 loss: 0.0000000020 time: 0.31179332733154297\n",
      "Iteration: 13880 loss: 0.0000000020 time: 0.37187695503234863\n",
      "Iteration: 13890 loss: 0.0000000020 time: 0.34084129333496094\n",
      "Iteration: 13900 loss: 0.0000000020 time: 0.32610249519348145\n",
      "Iteration: 13910 loss: 0.0000000020 time: 0.32503795623779297\n",
      "Iteration: 13920 loss: 0.0000000020 time: 0.3012571334838867\n",
      "Iteration: 13930 loss: 0.0000000020 time: 0.31609344482421875\n",
      "Iteration: 13940 loss: 0.0000000020 time: 0.31343817710876465\n",
      "Iteration: 13950 loss: 0.0000000051 time: 0.33578038215637207\n",
      "Iteration: 13960 loss: 0.0000000104 time: 0.3374514579772949\n",
      "Iteration: 13970 loss: 0.0000000020 time: 0.33342719078063965\n",
      "Iteration: 13980 loss: 0.0000000031 time: 0.33620762825012207\n",
      "Iteration: 13990 loss: 0.0000000024 time: 0.34372377395629883\n",
      "Iteration: 14000 loss: 0.0000000020 time: 0.36982274055480957\n",
      "Iteration: 14010 loss: 0.0000000020 time: 0.3472561836242676\n",
      "Iteration: 14020 loss: 0.0000000020 time: 0.35462403297424316\n",
      "Iteration: 14030 loss: 0.0000000020 time: 0.3420407772064209\n",
      "Iteration: 14040 loss: 0.0000000019 time: 0.35985255241394043\n",
      "Iteration: 14050 loss: 0.0000000020 time: 0.37366771697998047\n",
      "Iteration: 14060 loss: 0.0000000022 time: 0.32887697219848633\n",
      "Iteration: 14070 loss: 0.0000000071 time: 0.3796963691711426\n",
      "Iteration: 14080 loss: 0.0000000154 time: 0.35771942138671875\n",
      "Iteration: 14090 loss: 0.0000000049 time: 0.34330296516418457\n",
      "Iteration: 14100 loss: 0.0000000022 time: 0.3578333854675293\n",
      "Iteration: 14110 loss: 0.0000000020 time: 0.3597886562347412\n",
      "Iteration: 14120 loss: 0.0000000020 time: 0.3136882781982422\n",
      "Iteration: 14130 loss: 0.0000000020 time: 0.31122779846191406\n",
      "Iteration: 14140 loss: 0.0000000019 time: 0.30251049995422363\n",
      "Iteration: 14150 loss: 0.0000000019 time: 0.28987812995910645\n",
      "Iteration: 14160 loss: 0.0000000019 time: 0.3059983253479004\n",
      "Iteration: 14170 loss: 0.0000000019 time: 0.3271019458770752\n",
      "Iteration: 14180 loss: 0.0000000019 time: 0.3277568817138672\n",
      "Iteration: 14190 loss: 0.0000000019 time: 0.3402884006500244\n",
      "Iteration: 14200 loss: 0.0000000019 time: 0.33367395401000977\n",
      "Iteration: 14210 loss: 0.0000000019 time: 0.362821102142334\n",
      "Iteration: 14220 loss: 0.0000000019 time: 0.36374354362487793\n",
      "Iteration: 14230 loss: 0.0000000019 time: 0.3238203525543213\n",
      "Iteration: 14240 loss: 0.0000000019 time: 0.3602573871612549\n",
      "Iteration: 14250 loss: 0.0000000019 time: 0.33727145195007324\n",
      "Iteration: 14260 loss: 0.0000000019 time: 0.31181979179382324\n",
      "Iteration: 14270 loss: 0.0000000019 time: 0.2998368740081787\n",
      "Iteration: 14280 loss: 0.0000000020 time: 0.31867146492004395\n",
      "Iteration: 14290 loss: 0.0000000072 time: 0.3282310962677002\n",
      "Iteration: 14300 loss: 0.0000000198 time: 0.2890636920928955\n",
      "Iteration: 14310 loss: 0.0000000022 time: 0.286480188369751\n",
      "Iteration: 14320 loss: 0.0000000039 time: 0.3399951457977295\n",
      "Iteration: 14330 loss: 0.0000000027 time: 0.3618435859680176\n",
      "Iteration: 14340 loss: 0.0000000019 time: 0.29836511611938477\n",
      "Iteration: 14350 loss: 0.0000000020 time: 0.2981126308441162\n",
      "Iteration: 14360 loss: 0.0000000020 time: 0.30934786796569824\n",
      "Iteration: 14370 loss: 0.0000000019 time: 0.3027205467224121\n",
      "Iteration: 14380 loss: 0.0000000019 time: 0.29304027557373047\n",
      "Iteration: 14390 loss: 0.0000000019 time: 0.31688404083251953\n",
      "Iteration: 14400 loss: 0.0000000019 time: 0.3318192958831787\n",
      "Iteration: 14410 loss: 0.0000000019 time: 0.324601411819458\n",
      "Iteration: 14420 loss: 0.0000000019 time: 0.3202970027923584\n",
      "Iteration: 14430 loss: 0.0000000019 time: 0.3228428363800049\n",
      "Iteration: 14440 loss: 0.0000000031 time: 0.3182389736175537\n",
      "Iteration: 14450 loss: 0.0000000171 time: 0.31401968002319336\n",
      "Iteration: 14460 loss: 0.0000000066 time: 0.31813478469848633\n",
      "Iteration: 14470 loss: 0.0000000031 time: 0.33207178115844727\n",
      "Iteration: 14480 loss: 0.0000000020 time: 0.31879186630249023\n",
      "Iteration: 14490 loss: 0.0000000019 time: 0.31884121894836426\n",
      "Iteration: 14500 loss: 0.0000000019 time: 0.32073330879211426\n",
      "Iteration: 14510 loss: 0.0000000019 time: 0.35054874420166016\n",
      "Iteration: 14520 loss: 0.0000000019 time: 0.364330530166626\n",
      "Iteration: 14530 loss: 0.0000000019 time: 0.30655336380004883\n",
      "Iteration: 14540 loss: 0.0000000019 time: 0.2879626750946045\n",
      "Iteration: 14550 loss: 0.0000000019 time: 0.3069639205932617\n",
      "Iteration: 14560 loss: 0.0000000019 time: 0.29953885078430176\n",
      "Iteration: 14570 loss: 0.0000000019 time: 0.28804755210876465\n",
      "Iteration: 14580 loss: 0.0000000019 time: 0.310009241104126\n",
      "Iteration: 14590 loss: 0.0000000019 time: 0.30005693435668945\n",
      "Iteration: 14600 loss: 0.0000000031 time: 0.28548169136047363\n",
      "Iteration: 14610 loss: 0.0000000276 time: 0.3081231117248535\n",
      "Iteration: 14620 loss: 0.0000000100 time: 0.2989058494567871\n",
      "Iteration: 14630 loss: 0.0000000054 time: 0.31597471237182617\n",
      "Iteration: 14640 loss: 0.0000000022 time: 0.2903616428375244\n",
      "Iteration: 14650 loss: 0.0000000019 time: 0.3011929988861084\n",
      "Iteration: 14660 loss: 0.0000000020 time: 0.3297004699707031\n",
      "Iteration: 14670 loss: 0.0000000019 time: 0.3571748733520508\n",
      "Iteration: 14680 loss: 0.0000000019 time: 0.35465216636657715\n",
      "Iteration: 14690 loss: 0.0000000019 time: 0.3315138816833496\n",
      "Iteration: 14700 loss: 0.0000000019 time: 0.319854736328125\n",
      "Iteration: 14710 loss: 0.0000000019 time: 0.31624913215637207\n",
      "Iteration: 14720 loss: 0.0000000019 time: 0.3233795166015625\n",
      "Iteration: 14730 loss: 0.0000000019 time: 0.323016881942749\n",
      "Iteration: 14740 loss: 0.0000000019 time: 0.33613133430480957\n",
      "Iteration: 14750 loss: 0.0000000019 time: 0.32455968856811523\n",
      "Iteration: 14760 loss: 0.0000000019 time: 0.32122039794921875\n",
      "Iteration: 14770 loss: 0.0000000030 time: 0.3239307403564453\n",
      "Iteration: 14780 loss: 0.0000000171 time: 0.31167054176330566\n",
      "Iteration: 14790 loss: 0.0000000066 time: 0.32056736946105957\n",
      "Iteration: 14800 loss: 0.0000000035 time: 0.32022786140441895\n",
      "Iteration: 14810 loss: 0.0000000021 time: 0.3406977653503418\n",
      "Iteration: 14820 loss: 0.0000000019 time: 0.31914377212524414\n",
      "Iteration: 14830 loss: 0.0000000019 time: 0.3123776912689209\n",
      "Iteration: 14840 loss: 0.0000000019 time: 0.3144097328186035\n",
      "Iteration: 14850 loss: 0.0000000019 time: 0.3440744876861572\n",
      "Iteration: 14860 loss: 0.0000000019 time: 0.34340906143188477\n",
      "Iteration: 14870 loss: 0.0000000022 time: 0.3485262393951416\n",
      "Iteration: 14880 loss: 0.0000000067 time: 0.3318495750427246\n",
      "Iteration: 14890 loss: 0.0000000123 time: 0.34563493728637695\n",
      "Iteration: 14900 loss: 0.0000000059 time: 0.29029345512390137\n",
      "Iteration: 14910 loss: 0.0000000033 time: 0.3247642517089844\n",
      "Iteration: 14920 loss: 0.0000000023 time: 0.307636022567749\n",
      "Iteration: 14930 loss: 0.0000000020 time: 0.32236695289611816\n",
      "Iteration: 14940 loss: 0.0000000019 time: 0.3939077854156494\n",
      "Iteration: 14950 loss: 0.0000000019 time: 0.3321225643157959\n",
      "Iteration: 14960 loss: 0.0000000019 time: 0.37306976318359375\n",
      "Iteration: 14970 loss: 0.0000000018 time: 0.3235790729522705\n",
      "Iteration: 14980 loss: 0.0000000018 time: 0.28487515449523926\n",
      "Iteration: 14990 loss: 0.0000000018 time: 0.30279040336608887\n",
      "Iteration: 15000 loss: 0.0000000018 time: 0.30343103408813477\n"
     ]
    }
   ],
   "source": [
    "geo_file = './unit_square_triangles'\n",
    "worker = gw.gmsh_worker(geo_file)\n",
    "worker.generate_parallel_chain(False, False, False)\n",
    "\n",
    "# seed for reproducibility\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=1000)\n",
    "\n",
    "# mesh init\n",
    "domain = ((0, 0), (1, 0), (1, 1), (0, 1))\n",
    "\n",
    "# order of test function \n",
    "N_test = 1\n",
    "params = {'scheme': 'VPINNs','N_test':N_test}\n",
    "\n",
    "xy, w = get_quad_rule(30)\n",
    "\n",
    "H1 = []\n",
    "L2 = []\n",
    "semi = []\n",
    "h = []\n",
    "\n",
    "for refinement in [0,1,2,3]:\n",
    "    model = restart_model()\n",
    "    mesh, _ = ml.take_parallel_mesh_chain(worker.chain[refinement],worker.chain[-1], 'DDDD')\n",
    "    mesh = mesh.convert_to_dict()\n",
    "    vp=VPINN(pb,params,mesh,False,model)\n",
    "    history=vp.train(15000, 0.0005)\n",
    "    # ml.compare(worker.chain[refinement],worker.chain[refinement])\n",
    "\n",
    "\n",
    "    L2_error_=L2_error(xy,w)\n",
    "\n",
    "\n",
    "    semi_H1_err = semi_H1_error(xy,w)\n",
    "    #H1_err = np.sqrt(L2_error**2 + semi_H1_err**2)\n",
    "\n",
    "    #H1.append(H1_err)\n",
    "    semi.append(semi_H1_err)\n",
    "    L2.append(L2_error_)\n",
    "    a, b = mesh['h_max'], mesh['h_min']\n",
    "    h.append(0.5*a + 0.5*b)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.array(h)\n",
    "\n",
    "#H1 = np.array(H1)\n",
    "L2 = np.array(L2)\n",
    "semi = np.array(semi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semi rate: 2.864745033082956\n",
      "L2 rate: 3.5142746404802128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2a5ceca7c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slope_H1, intercept_H1 = np.polyfit(h, H1, 1)\n",
    "# slope_L2, intercept_L2 = np.polyfit(h, L2, 1)\n",
    "\n",
    "coeffs = np.polyfit(np.log10(h), np.log10(semi), 1)\n",
    "polynomial = np.poly1d(coeffs)\n",
    "log10_H1_fit = polynomial(np.log10(h))\n",
    "\n",
    "print(f\"semi rate: {coeffs[0]}\")\n",
    "\n",
    "coeffs = np.polyfit(np.log10(h), np.log10(L2), 1)\n",
    "polynomial = np.poly1d(coeffs)\n",
    "log10_L2_fit = polynomial(np.log10(h))\n",
    "\n",
    "print(f\"L2 rate: {coeffs[0]}\")\n",
    "\n",
    "#plt.loglog(h, H1, marker='o', label='H1')\n",
    "plt.loglog(h, L2, marker='o', label='L2')\n",
    "plt.loglog(h, semi, marker='o', label='semi')\n",
    "#plt.loglog(h, 10**log10_H1_fit)\n",
    "# plt.loglog(h, 10**log10_L2_fit)\n",
    "plt.loglog(h, h, linestyle=':', label='h1')\n",
    "plt.loglog(h, h*h, linestyle=':', label='h2')\n",
    "plt.loglog(h, h*h*h, linestyle=':', label='h3')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
